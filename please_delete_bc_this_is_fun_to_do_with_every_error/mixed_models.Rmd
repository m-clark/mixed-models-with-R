---
title: "Mixed Models in R"
author:  |
  <div class="title"><span style="font-size:125%; font-variant:small-caps; ">Michael Clark</span><br><br>
  <span class="" style="font-size:75%">http://m-clark.github.io/workshops/mixed_models_r/</span><br><br>
  <img src="img/signature-acronym.png" style="width:30%; padding:10px 0;"> <br>
  <img src="img/ARC-acronym-signature.png" style="width:21%; padding:10px 0;"> </div>
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output:
  bookdown::gitbook:
    css: [css/standard_html.css, css/mytufte.css]
    hightlight: pygments
    number_sections: false
    # split_by: section
    config:
      toc:
        collapse: subsection
        scroll_highlight: yes
        before: null
        after: null
      toolbar:
        position: fixed
      edit : null
      download: null
      search: yes
      # fontsettings:
      #   theme: white
      #   family: sans
      #   size: 2
      sharing:
        facebook: yes
        twitter: yes
        google: no
        weibo: no
        instapper: no
        vk: no
        all: ['facebook', 'google', 'twitter', 'weibo', 'instapaper']
always_allow_html: yes
font-import: http://fonts.googleapis.com/css?family=Roboto|Open+Sans
font-family: 'Roboto'
documentclass: book
bibliography: refs.bib
biblio-style: apalike
link-citations: yes
description: "Introduction to Mixed Models in R"
cover-image: img/nineteeneightyR.png
url: 'https\://m-clark.github.io/Workshops/'  # evidently the \: is required or you'll get text in the title/toc area
github-repo:  m-clark/

---




```{r chunk_setup, include=FALSE, eval=TRUE}
knitr::opts_chunk$set(echo = T, message=F, warning=F, comment=NA, autodep=F, 
                      eval=T, cache.rebuild=T, cache=T, R.options=list(width=120), 
                      fig.width=8, fig.align = 'center')
```

```{r load_common_packages, echo=FALSE, cache=FALSE, eval=TRUE}
library(lazerhawk); library(htmltools); library(forcats); library(lme4)
library(broom); library(pander); library(tidyverse); library(plotly); library(haven)
```

# 
```{r cover_image, fig.align='center', echo=FALSE, include=identical(knitr:::pandoc_to(), 'html')}
knitr::include_graphics('img/nineteeneightyR.png', dpi = NA)
```


<!--chapter:end:index.Rmd-->

# Introduction

## Overview

Mixed models are an extremely useful modeling tool for situations in which there is some dependency among observations in the data, where the correlation typically arises from the observations being clustered in some way. For example, it is quite common to have data in which we have repeated measurements for the units of observation, or in which the units of observation are otherwise clustered (e.g. students within school, cities within geographic region).   While there are different ways to approach such a situation, mixed models are a very common and powerful tool to do so.  In addition, they have ties to other statistical approaches that further expand their applicability.

### Goals

The goal of this workshop is primarily to provide a sense of when one would use mixed models and a variety of standard techniques.  Additionally, we'll have exercises to practice.  


### Prerequisites

The document is for the most part very applied in nature, and only assumes a basic understanding of standard regression models. Use of R for regression modeling is also assumed.  Demonstrations will be done almost entirely with the <span class="pack">lme4</span> package.

Note the following color coding used in this document:

- <span class="emph">emphasis</span>
- <span class="pack">package</span>
- <span class="func">function</span>
- <span class="objclass">object/class</span>
- [link]()


## Initial Steps

0. Download the zip file at http://m-clark.github.io/workshops/mixed_models_r/mem_workshop.zip. Be mindful of where you put it.
1. Unzip it. Be mindful of where you put the resulting folder.
2. Open RStudio.
3. File/Open Project and click on the blue icon in the folder you just created.
4. File/Open Click on the ReadMe file and do what it says.



<!--chapter:end:00_Introduction.Rmd-->

# Mixed Models

While they might be new to you, mixed models have been around a *long* time. For example, standard ANOVA methods can be seen as special cases of a mixed model.  More recently, mixed models have a variety of applications and extensions, allowing them to encompass a diverse range of data situations.  They can be seen as a first step in expanding one's toolset beyond the generalized linear model.

## Terminology

For the uninitiated, the terminology surrounding mixed models, especially across disciplines, can be a bit confusing. Some terms you might come across regarding these types of models include:

- Variance components
- Random intercepts and slopes
- Random effects
- Random coefficients
- Varying coefficients
- Intercepts- and/or slopes-as-outcomes
- Hierarchical linear models
- Multilevel models (implies multiple levels of hierarchically clustered data)
- Growth curve models (possibly Latent GCM)
- Mixed effects models

All describe types of mixed models.  Some might be more historical, others are more often seen in a specific discipline, others might refer to a certain data structure, and still others are special cases. <span class='emph'>Mixed effects</span>, or simply mixed, models generally refer to a mixture of fixed and random effects.  For the models in general, I prefer the terms 'mixed models' or 'random effects models' because they are simple, no specific structure is implied, and the latter can also apply to extensions that many would not think of when other terms are used[^richlypar].  Regarding the mixed effects, <span class='emph'>fixed effects</span> is perhaps a poor but nonetheless stubborn term for the typical main effects one would see in a linear regression model, i.e. the non-random part of a mixed model, and in some contexts they are referred to as the *population average* effect.  Though you will hear many definitions, random effects are simply those specific to an observational unit, however defined.  The approach outlined in this document largely pertains to the case where the observational unit is the level of some grouping factor, but this is only .


## Kinds of clustering

Data might have one or multiple sources of clustering, and that clustering may be hierarchical, such that clusters are nested within other clusters. An example would be scholastic aptitude tests given multiple times to students (repeated observations nested within students, students nested within schools, schools nested within districts). In other cases, there is no nesting structure. An example would be a reaction time experiment where participants perform the same set of tasks.  While observations are nested within individual, observations are also clustered according to task type.  Some use the terms <span class="emph">nested</span> and <span class="emph">crossed</span> to distinguish between these scenarios.  In addition clustering may be balanced or not. We might expect more balance in studies of an experimental nature, but definitely not in other cases, e.g. where the cluster is something like geographical unit and the observations are people.

In what follows we'll see mixed effect models in all these data situations. In general, our approach will be the same, as such clustering is really more a property of the data than the model.  However, it's important to get a sense of the flexibility of mixed models to handle a variety of data situations.



## Random Intercepts model

For the following we'll demonstrate the simplest[^vcmodel] and most common case of a mixed model, that in which we have a single random effect added to the standard regression situation.  For reasons that will hopefully become clear later, this is commonly called a random intercepts model. We will see an extension of it later.


## Example: student GPA

For the following we'll assess factors predicting college grade point average (GPA).  Each of the 200 students is assessed for six occasions (each semester for the first three years), so we have observations nested within students. We have other variables such as job status, sex, and high school GPA.  Some will be in both labeled and numeric form. See the [appendix][Appendix] for details.

```{r gpa_setup, echo=FALSE, eval=FALSE}
# MC Note: either the job label is incorrect or this variable makes no sense. 
# The label is 0,1:3, 4 or more hours (pt jobs for less than 4 hours? per day?).
# However only values of 1 (rare to non-existent some years) 2 or 3.  How the
# hell do you 'simulate' a factor that only has 3 of 5 levels and one category
# that makes up 97% of the data? Avoid or change.

gpa = read_spss('data/raw_data/joop_hox_data2/5 Longitudinal/gpa2long.sav') %>% 
  mutate(highgpa=as.numeric(highgpa),
         occasion = as.numeric(occas) %>%  # to get rid of stupid labels
  as_factor

glimpse(gpa)
save(gpa, file='data/gpa.RData')
```

```{r show_gpa_data, echo=FALSE}
load('data/gpa.RData')
DT::datatable(gpa, 
              options=list(dom='tp', 
                           scrollX=T,  
                           autoWidth=T,
                           columnDefs = list(list(width = '150px', targets = 1),
                                             list(width = '100px', targets = 3))), 
              rownames=F)
```

<br>
<br>

## The standard regression model

Now for the underlying model. We can show it in a couple different ways. First we start with just a standard regression to get our bearings.


$$\mathscr{gpa} = b_{\mathrm{intercept}} + b_{\mathrm{occ}}\cdot \mathscr{occasion} + \epsilon$$

We have coefficients for the intercept and the effect of time.  The error $\epsilon$ is assumed to be normally distributed with mean 0 and some standard deviation $\sigma$.

$$\epsilon \sim \mathscr{N}(0, \sigma)$$

An alternate way to write the model which puts emphasis on the underlying data generating process for $y$ is

$$\mathscr{gpa} \sim \mathscr{N}(\mu, \sigma)$$
$$\mu = b_{\mathrm{intercept}} + b_{\mathrm{occ}}\cdot \mathscr{occasion}$$

## The mixed model

##### Initial depiction

Now we show one way of showing it as a mixed model that includes a unique effect for each student. Consider the following model for a single student[^notation]. This depiction shows that the student-specific effect can be seen as an additional source of variance.


$$\mathscr{gpa} = b_{\mathrm{intercept}} + b_{\mathrm{occ}}\cdot \mathscr{occasion} + (\mathrm{effect}_{\mathscr{student}} + \epsilon)$$
If we rearrange it, we can instead focus on model coefficients.

$$\mathscr{gpa} = (b_{\mathrm{intercept}} + \mathrm{effect}_{\mathscr{student}}) + b_{\mathrm{occ}}\cdot \mathscr{occasion} +  \epsilon$$

In this way, we'll have student-specific intercepts, as each person will have their own unique effect added to the overall intercept, resulting in a different intercept for each person. As such this is often called a random intercepts model.

In either case, we (usually) assume the following for the student effects.  


$$\mathrm{effect}_{\mathrm{student}} \sim \mathscr{N}(0, \tau)$$


Thus the student effects are random, and specifically are normally distributed with mean of zero and some estimated standard deviation. In other words, conceptually the only difference between the mixed model and a standard regression is the student effect, which is *on average* no effect, but specifically varies from student to student with some standard deviation ($\tau$).



##### As a multi-level model

This next depiction is commonly seen in the multilevel modeling literature.  It is shown as a two part model, one at the observation level and one at the student level.  

$$\mathrm{gpa} = b_{\mathrm{int\_student}} + b_{\mathrm{occ}}\cdot \mathrm{occasion} + \epsilon$$

$$b_{\mathrm{int\_student}} = b_{\mathrm{intercept}} + \mathrm{effect}_{\mathrm{student}}$$

However, after 'plugging in' the second level part to the first, it is identical to the previous.

Note how we don't have a student-specific effect for occasion.  In this context, occasion is said to be a fixed effect only, and there is no random component. This definitely does not have to be the case though, as we'll see later.

## Application

#### Initial visualization

It always helps to look before we leap, so let's do so.  Here we plot GPA vs. occasion (i.e. semester) to get a sense of the variability in starting points and trends.

```{r spaghetti, echo=FALSE}
set.seed(1234)
gpa_lm = lm(gpa ~ occasion, data=gpa)
# sample_students = gpa %>% filter(student %in% sample(1:200, 10))
# occasion_sample = gpa$occasion[gpa$student %in% sample_students$student]
# gpa_sample = gpa$gpa[gpa$student %in% sample_students$student]
gpa %>% 
  modelr::add_predictions(gpa_lm, var='all') %>% 
  mutate(select = factor(student %in% sample(1:200, 10))) %>% 
  group_by(student, select) %>% 
  plot_ly %>% 
  add_lines(x=~occasion, y=~gpa, opacity=.35, color=~select, colors=c(palettes$orange$complementary, palettes$orange$orange), showlegend=F) %>%
  add_lines(x=~occasion, y=~all, color=I(palettes$stan_red$stan_red), opacity=.70) %>% 
  theme_plotly()
```

<br>

All student paths are shown in blue, with a sample of 10 shown in orange. The overall trend as estimated by the regression we'll do later is shown in red. Two things stand out.  One is that students have a lot of variability in starting out. Secondly, while the general trend in GPA is upward over time as we'd expect, individual students may vary in that trajectory.

#### Standard regression

So let's get started. First, we'll look at the regression and only the time trend.  Note that I present a cleaner version of the summarized objects for the purposes of this document.

```{r gpa_lm, echo=1:3, eval=-3}
load('data/gpa.RData')
gpa_lm = lm(gpa ~ occasion, data=gpa)
summary(gpa_lm)
pander(summary(gpa_lm), round=3)
gpa_lm_by_group = gpa %>% 
  split(.$student) %>% 
  map(~lm(gpa ~ occasion, data=.x)) %>% 
  map(coef) %>% 
  do.call(rbind, .) # some day bind_rows will work as advertised
coef_lm = coef(gpa_lm)
```

The above tells us that as we move from semester to semester, we can expect GPA to increase by about `r round(coef_lm[2], 2)` points.  This would be fine except that we are ignoring the clustering.  A side effect of doing so is that our standard errors are incorrect, and thus claims about statistical significance based on them would be off.  More importantly however is that we simply don't get to explore the student effect, which would be of interest by itself.

#### Regression by cluster

An alternative approach would be to run separate regressions for every student.  However, there are many drawbacks to this- it's not easily summarized when there are many groups, typically there would be very little data within each cluster to do so, and the models are over-contextualized, meaning they ignore what students have in common.  We'll compare this result to the mixed model later.

#### Mixed model

Next we run a mixed model that will allow for a student specific effect.  Such a model is easily conducted in R, specifically with the package <span class="pack">lme4</span>.  In the following, the code will look just like what you used for regression with <span class="func">lm</span>, but with an additional component specifying the group effect.  The `(1|student)` means that we are allowing the intercept, represented by `1`, to vary by student. With the mixed model, we get the same results as the regression, but with more to talk about.


```{r gpa_mixed, eval=-3}
library(lme4)
gpa_mixed = lmer(gpa ~ occasion + (1|student), data=gpa)
summary(gpa_mixed)
```

```{r gpa_mixed_pretty, echo=FALSE}
vcovs = tidy(VarCorr(gpa_mixed)) %>% 
  select(vcov)  # for icc later
pander(tidy(gpa_mixed, 'fixed') %>% 
         mutate_if(is.numeric, arm::fround, digits=3))
tidy(VarCorr(gpa_mixed)) %>% 
  select(-var1, -var2) %>% 
  rename(variance=vcov, sd=sdcor) %>%  
  mutate_if(is.numeric, arm::fround, digits=3) %>% 
  pander()
```


First we see that the coefficients for the intercept and time are the same[^lmlmercoef], as would be their interpretation.  The standard errors, on the other hand are different here, though in the end our conclusion as far as statistical significance goes would be the same. However, the <span class="pack">lme4</span> does not provide p-values.  There are [several reasons](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#why-doesnt-lme4-display-denominator-degrees-of-freedomp-values-what-other-options-do-i-have) for this, namely that with mixed models we are essentially dealing with different sample sizes, the $N_c$ within cluster, which may vary from cluster to cluster (and even be a single observation!), and N total observations, which puts us in kind of a fuzzy situation with regard to reference distributions, denominator degrees of freedom and how to approximate a 'best' solution. Other programs provide p-values automatically as if there is no issue, and without telling you *which* approach they use to calculate them (there are several).  Furthermore, those approximations may be very poor in some scenarios, or make assumptions that may not be appropriate for the situation[^fuzzyp].

However, it's more straightforward to get confidence intervals, and we can do so with <span class="pack">lme4</span> as follows[^confint].

```{r gpa_mixed_ci, eval=FALSE}
confint(gpa_mixed)
```

```{r gpa_mixed_ci_pretty, echo=FALSE}
# tidy(gpa_mixed, conf.int=T, conf.method='boot') %>% 
#   mutate_if(is.numeric, arm::fround, digits=2) %>% 
#   mutate_all(function(x) ifelse(stringr::str_trim(x)=='NA', '', x)) %>%
#   pander(justify='lrrrrrc', split.cells=Inf, split.tables=Inf)  

confint(gpa_mixed) %>% 
  data.frame(rn = rownames(.)) %>% 
  mutate(rn = c('student', 'residual', 'Intercept', 'occasion')) %>% 
  select(rn, X2.5.., X97.5..) %>% 
  rename(' '=rn,
         `2.5%` = X2.5..,
         `97.5%` = X97.5..) %>% 
  pander(justify='lrr', round=3)
```


One thing that's new compared to the standard regression output is the estimated variance/standard deviation of the student effect ($\tau$ in our formula depiction from before).  This tells us how much, on average, GPA bounces around as we move from student to student. In other words, even after making a prediction based on time point, each student has their own unique deviation, and that value is the estimated average deviation.  Note that scores move due to the student more than double what they move based on a semester change.

What's more, we can actually get estimates of the student effects.  I show two ways for the first five students, as random effect and as random intercept (i.e. effect + intercept).

```{r randeffs, eval=FALSE}
ranef(gpa_mixed)$student %>% head(5)
```
```{r randeffs_pretty, echo=FALSE}
ranef(gpa_mixed)$student %>% head(5) %>% arm::fround(digits=3) %>% pander(justify='r')
```

```{r randints, eval=FALSE}
coef(gpa_mixed)$student %>% head(5)
```

```{r randints_pretty, echo=FALSE}
coef(gpa_mixed)$student %>% head(5) %>% arm::fround(digits=3) %>% pander(justify='rr')
```


Note that we did not allow occasion to vary, so it is a constant, i.e. fixed, effect for all students. 


Another way to interpret the variance output is via the <span class="emph">intraclass correlation</span>, which tells us how much of the variance is due to the clustering.  In this case it's just the student variance out of the total, or `r round(vcovs[1,1], 3)` / `r round(sum(vcovs), 3)` =  `r round(vcovs[1,1]/sum(vcovs), 3)*100`%.

## Cluster level covariate

Note our depiction of a mixed model as a multilevel model.

$$\mathrm{gpa} = b_{\mathrm{int\_student}} + b_{\mathrm{occ}}\cdot \mathrm{occasion} + \epsilon$$

$$b_{\mathrm{int\_student}} = b_{\mathrm{intercept}} + \mathrm{effect}_{\mathrm{student}}$$
If we add student a student level covariate, e.g sex, to the model, we then have the following.

$$b_{\mathrm{int\_student}} = b_{\mathrm{intercept}} + b_{sex}\cdot \mathrm{sex} +  \mathrm{effect}_{\mathrm{student}}$$

Which, after plugging in, we still have the same model as before, just with an additional predictor.

$$\mathrm{gpa} = b_{\mathrm{intercept}} + b_{\mathrm{occ}}\cdot \mathrm{occasion}+ b_{sex}\cdot \mathrm{sex} + (\mathrm{effect}_{\mathscr{student}} + \epsilon)$$

Thus, adding cluster level covariates doesn't have any unusual effect on how we think about the model[^mlevel]. We simply add them to our set of predictor variables. Note also, that we can create cluster level covariates as means or some other summary of the observation level variables.  This is especially common when the clusters represent geographical units and observations are people.

## Summary

Mixed models allow for us to take into account clustering in the data.  If this were all it was used for, we would have more accurate inference relative to what would be had if we ignored the structure in the data.  However, we get much more.  We better understand the sources of variability in the target variable.  We also get group specific estimates of the parameters in the model, allowing us to understand exactly how the groups differ from one another.  Furthermore, this in turn allows for group specific prediction, and much more accurate prediction.  In short, there is much to be gained by mixed models, even in the simplest of settings.


## Exercises


### Sleep

For this exercise, we'll use the sleep study data from the <span class="pack">lme4</span> package.  The following describes it.

> The average reaction time per day for subjects in a sleep deprivation study. On day 0 the subjects had their normal amount of sleep. Starting that night they were restricted to 3 hours of sleep per night. The observations represent the average reaction time (in milliseconds) on a series of tests given each day to each subject.

After loading the package, the data can be loaded as follows.  I show the first few observations.

```{r sleepstudy, echo=-3}
library(lme4)
data("sleepstudy")
pander(head(sleepstudy))
```

First run a regression with Reaction as the target variable and Days as the predictor. Then Run a mixed model with a random intercept for Subject.

Interpret the variance components and fixed effects.



### Cluster level covariate

Rerun the mixed model with the [GPA data][Mixed model] adding the cluster level covariate of `sex`, or high school GPA (`highgpa`), or both.  Interpret all aspects of the results.

```{r gpa_cluster, echo=F, eval=FALSE}
gpa_mixed_cluster_level = lmer(gpa ~ occasion + sex + highgpa + (1|student), gpa)
summary(gpa_mixed_cluster_level)
```

What happened to the student variance after adding cluster level covariates to the model?



### Simulation

The following represents a simple way to simulate a random intercepts model.  Note each object what each object is, and make sure the code make sense to you.  Then run it.

```{r simMixed, eval=FALSE}
set.seed(1234)  # this will allow you to exactly duplicate your result
Ngroups = 50
NperGroup = 3
N = Ngroups*NperGroup
groups = factor(rep(1:Ngroups, each=NperGroup))
u = rnorm(Ngroups, sd=.5)
e = rnorm(N, sd=.25)
x = rnorm(N)
y = 2 + .5*x + u[groups] + e

d = data.frame(x, y, groups)
```

Which of the above represent the fixed and random effects? Now run the following.

```{r simMixed2, eval=FALSE}
model = lmer(y ~ x + (1|groups), data=d)
summary(model)
confint(model)



library(ggplot2)
ggplot(aes(x, y), data=d) +
  geom_point()
```

Do the results seem in keeping with what you expect?

In what follows we'll change various aspects of the data, then rerun the model after each change, then summarizing and getting confidence intervals as before.  For each note specifically at least one thing that changed in the results.

0. First calculate or simply eyeball the intraclass correlation coefficient $\frac{\textrm{random effect variance}}{\textrm{residual + random effect variance}}$.  In addition, create a density plot of the random effects as follows.

```{r simMixed3, eval=FALSE}
re = ranef(model)$groups
qplot(x=re, geom='density', xlim=c(-3,3))
```

1. Change the random effect variance/sd and/or the residual variance/sd and note your new estimate of the ICC, and plot the random effect as before.
2. Reset the values to the original.  Change <span class="objclass">Ngroups</span> to 10. What differences do you see in the confidence interval estimates? 
3. Set the Ngroups back to 50. Now change <span class="objclass">NperGroup</span> to 10, and note again the how the CI is different from the base condition.


[^richlypar]: I actually like [Richly Parameterized Linear Models](https://www.crcpress.com/Richly-Parameterized-Linear-Models-Additive-Time-Series-and-Spatial/Hodges/p/book/9781439866832), or [Structured Additive Regression Models](https://www.springer.com/us/book/9783642343322).  Both are a mouthful, but at least the latter reduces to [STARs](http://m-clark.github.io/workshops/stars/).

[^notation]: Note that I leave out the observation level subscript to keep things clean. I find that multilevel style notation quickly becomes unwieldy, and don't wish to reproduce it.  It also tends to add confusion to a lot of applied researchers starting out with mixed models.

[^vcmodel]: Actually, the simplest model would have no covariates at all, just <span class="emph">variance components</span>, with no correlations among the random effects.  Such a model can be interesting to look at while exploring your data, but would probably never suffice on its own to tell the story you desire to.

[^lmlmercoef]: This will not always be the case, e.g. with unbalanced data, but they should be fairly close.

[^mlevel]: This is why the multilevel depiction is subpar, and leads many to confusion at times.  You have a target variable and predictor variables based on theory.  Whether they are cluster level variables or if there are interactions doesn't have anything to do with the data structure as much as it does the theoretical implications.  However, if you depict the model in multilevel fashion, the final model must adhere to the 'plugged in' result.  So if, e.g. you posit a cluster level variable for a random slope, you *must* include the implied interaction of the cluster level and observation level covariates.

[^fuzzyp]: Note that many common modeling situations involve a fuzzy p setting, but especially penalized regression approaches such as mixed, additive, ridge regression models etc.  Rather than be a bad thing, this usually is a sign you're doing something interesting, or handling complexity in an appropriate way.

[^confint]: See `?confint.merMod` for details and options.


<!--chapter:end:01_random_intercepts.Rmd-->

# More Random Effects

Previously we've looked at random intercepts, but any observation level covariate effect could be allowed to vary by cluster as well.


## Application

Returning to the GPA data, recall the visualization from before.


```{r spaghetti2, echo=FALSE}
set.seed(1234)
gpa_lm = lm(gpa ~ occasion, data=gpa)
sample_students = gpa %>% filter(student %in% sample(1:200, 10))
occasion_sample = gpa$occasion[gpa$student %in% sample_students$student]
gpa_sample = gpa$gpa[gpa$student %in% sample_students$student]
gpa %>% 
  modelr::add_predictions(gpa_lm, var='all') %>% 
  group_by(student) %>% 
  plot_ly %>% 
  add_lines(x=~occasion, y=~gpa, opacity=.2, color=I(palettes$orange$complementary), showlegend=F) %>%
  add_paths(x=occasion_sample, y=gpa_sample, color=I(palettes$orange$orange), opacity=.5, showlegend=F) %>% 
  add_lines(x=~occasion, y=~all, color=I('#b2001d')) %>% 
  theme_plotly()
```

<br>
Let us now assume that the trend over time is allowed to vary by student.  Using <span class="pack">lme4</span>, this is quite straightforward.

```{r random_slope, eval=FALSE}
gpa_mixed =  lmer(gpa ~ occasion + (1 + occasion|student), data=gpa)
summary(gpa_mixed)
```

Pretty easy huh? Let's look at the results.


```{r random_slope_summary, echo=FALSE}
gpa_mixed =  lmer(gpa ~ occasion + (1 + occasion|student), data=gpa)

pander(tidy(gpa_mixed, 'fixed', conf.int=T) %>% mutate_if(is.numeric, arm::fround, digits=3))

tidy(VarCorr(gpa_mixed)) %>% 
  slice(-3) %>%  
  select(-var2) %>% 
  rename(variance=vcov, sd=sdcor, re=var1) %>%  
  mutate_if(is.numeric, arm::fround, digits=3) %>% 
  mutate_all(function(x) ifelse(is.na(x), '', x)) %>%
  data.frame %>% 
  pander()
```

Note that since we have 0 as our starting semester, the intercept tells us what the average GPA is in the first semester.  The associated intercept variance tells us how much that starting GPA bounces around from student to student.

The slope variance for occasion might not look like much in comparison, but slopes are on a notably different scale than the intercept.  Note that the mean slope for the semester to semester effect, our fixed effect, is `r round(fixef(gpa_mixed)[2], 2)`, but from student to student it bounces around half that.  Thus we could expect most students to fall somewhere between a flat effect of zero to more than double the population average[^sdslopes]. 

Yet another point of interest is the correlation of the intercepts and slopes. In this case it's `r tidy(VarCorr(gpa_mixed)) %>% slice(3) %>% select(sdcor) %>%  round(2)`. That's pretty small, but the interpretation is the same as with any correlation.  In this case specifically, it tells us that those with lower intercepts would be associated with increased time trajectories.   This makes intuitive sense in that people are improving in general, and those at the bottom would have more room to improve.  However, this is very slight, and practically speaking we might not put too much weight on it.


### Comparison to many regressions


Let's compare these results to the ones we would have gotten had we run a separate regression for each student.  In what follows we see the distribution of of the estimated intercept and slope coefficients for all the students.

```{r ranints_vs_separateints, echo=FALSE}
gint = data_frame(Mixed=coef(gpa_mixed)$student[,1], Separate=gpa_lm_by_group[,1]) %>% 
  gather(key=Model, value=Intercept) %>% 
  ggplot(aes(x=Intercept)) +
  geom_density(aes(color=Model, fill=Model), alpha=.25) +
  scale_fill_manual(values=c(palettes$orange$orange, palettes$orange$complementary)) +
  ggtitle('Intercepts') +
  labs(x='', y='') +
  xlim(c(1.5,4)) +
  theme_trueMinimal() +
  theme(
    legend.key.size=unit(2, 'mm'),
    legend.title=element_text(size=8),
    legend.text=element_text(size=8),
    legend.box.spacing=unit(0, 'in'),
    legend.position=c(.75,.75))
gslopes = data_frame(Mixed=coef(gpa_mixed)$student[,2], Separate=gpa_lm_by_group[,2]) %>% 
  gather(key=Model, value=Occasion) %>% 
  ggplot(aes(x=Occasion)) +
  geom_density(aes(color=Model, fill=Model), alpha=.25, show.legend=F) +
  scale_fill_manual(values=c(palettes$orange$orange, palettes$orange$complementary)) +
  ggtitle('Slopes for occasion') +
  labs(x='', y='') +
  xlim(c(-.2,.4)) +
  theme_trueMinimal() 


gridExtra::grid.arrange(gint, gslopes, ncol=2)
```

Here we can see that the mixed model intercepts are generally not as extreme, i.e. the tails of the distribution have been pulled toward the overall effect.  Same goes for the slopes. In both cases the mixed model shrinks what otherwise would have been the by-group estimate, which would overfit.  This <span class="emph">regularizing</span> effect is yet another bonus when using mixed models.


### Visual prediction

Let's compare our results visually. First there is the linear regression fit. We assume the same trend for everyone.  If we add the conditional predictions that include the subject specific effects from the mixed model, we now can also make subject specific predictions, greatly enhancing the practical use of the model.  As the code to create this plot (using <span class="pack">plotly</span>) was very easy, I go ahead and show it.

```{r visualize_mixed_fit, echo=-1, eval=-1}
going_down_now = factor(rep(coef(gpa_mixed)$student[,'occasion']<0, e=6), labels=c('Up', 'Down'))
gpa %>% 
  modelr::add_predictions(gpa_lm, var='lm') %>% 
  modelr::add_predictions(gpa_mixed, var='mixed') %>% 
  group_by(student) %>% 
  plot_ly %>% 
  add_lines(x=~occasion, y=~lm, opacity=1, color=I('#ff5500'), name='Standard\nRegression') %>%
  add_lines(x=~occasion, y=~mixed, opacity=.2, color=I('#03b3ff'), name='Mixed\nModel') %>%
  theme_plotly()
```

By contrast, the by-group approach is more noisy due to treating everyone independently.  Many more students are expected to have downward or flat trends relative to the mixed model (the mixed model only had `r sum(coef(gpa_mixed)$student[,'occasion']<0)` trends estimated to be negative).

```{r visualize_bygroup_fit, echo=FALSE}
gpa_lm_fits_by_group = gpa %>% 
  split(.$student) %>% 
  map(~lm(gpa ~ occasion, data=.x)) %>% 
  map(fitted) %>% 
  unlist
going_down_now = factor(rep(gpa_lm_by_group[,'occasion']<0, e=6), labels=c('Up', 'Down'))
gpa %>% 
  modelr::add_predictions(gpa_lm, var='lm') %>% 
  mutate(stufit=gpa_lm_fits_by_group) %>% 
  group_by(student) %>% 
  plot_ly %>% 
  add_lines(x=~occasion, y=~lm, opacity=1, color=I(palettes$orange$orange), name='Standard\nRegression') %>%
  add_lines(x=~occasion, y=~stufit, color=~going_down_now, opacity=.2, color=I(palettes$orange$complementary), name='By Group') %>%
  theme_plotly()
```


## Exercises

#### Sleep revisited

Run the sleep study model with random coefficient for the Days effect, and interpret the results.  What is the correlation between the intercept and Days random effects?  Use the <span class="func">ranef</span> and <span class="func">coef</span> functions on the model you've created. What do you see?

```{r sleepstudy2}
library(lme4)
data("sleepstudy")
```

In the following replace <span class="objclass">model</span> with the name of your model object. Run each line, inspecting the result of each as you go along. 

```{r, eval=FALSE}
re = ranef(model)$Subject
fe = fixef(model)
apply(re, 1, function(x) x + fe) %>% t
```

The above code adds the fixed effects to each row of the random effects (the <span class="func">t</span> just transposes the result). What is the result compared to what you saw before?



#### Simulation revisited

The following shows a simplified way to simulate some random slopes, but otherwise is the same as the simulation before.  Go ahead and run the code.

```{r simSlopes, eval=FALSE}
set.seed(1234)  # this will allow you to exactly duplicate your result
Ngroups = 50
NperGroup = 3
N = Ngroups*NperGroup
groups = factor(rep(1:Ngroups, each=NperGroup))
re_int = rnorm(Ngroups, sd=.75)
re_slope = rnorm(Ngroups, sd=.25)
e = rnorm(N, sd=.25)
x = rnorm(N)
y = (2 + re_int[groups]) + (.5 + re_slope[groups])*x +  + e

d = data.frame(x, y, groups)
```

This next bit of code shows a way to run a mixed model while specifying that there is no correlation between intercepts and slopes.  There is generally no reason to do this unless the study design warrants it[^nocorr], but you could do it as a step in the model-building process, such that you fit a model with no correlation, then one with it.

```{r simSlopes2, eval=FALSE}
model_base = lmer(y ~ x + (1|groups), data=d)
model = lmer(y ~ x + (1|groups) + (0 + x|groups), data=d)
summary(model)
confint(model)



library(ggplot2)
ggplot(aes(x, y), data=d) +
  geom_point()
```

Compare model fit using the <span class="func">AIC</span> function, e.g. `AIC(model)`.  The model with the lower AIC is the better model, so which would you choose?



[^sdslopes]: In case it's not clear, I'm using the fact that we assume a normal distribution for the random effect of occasion.  A quick rule of thumb for a normal distribution is that 95% fall between $\pm$ 2 standard deviations of the mean.

[^nocorr]: I personally have not come across a situation where I'd do this in practice.  Even if the simpler model with no correlation was a slightly better fit, there isn't much to be gained by it.

<!--chapter:end:02_random_slopes.Rmd-->

# Common Extensions



## Additional Grouping Structure


### Cross-classified models

Oftentimes there will be additional sources of variance beyond one grouping factor. Consider as an example, a visual perception experiment where there are multiple trials for each individual along with specific images displayed.  Such data might look like this.

```{r demodata_crossed, echo=FALSE}
crossing(Person=1:20, Image=letters[1:10]) %>% 
  mutate(score=sample(1:10, 200, replace=T)) %>% 
  DT::datatable(options=list(dom='tp', autoWidth=F, 
                             columnDefs=list(
                               list(width='10px', targets=0:2),
                               list(className = 'dt-center', targets = 0:2))), rownames = F, width=250)
```

<br>
<br>

In such a case we have observations clustered within both person and image, but person and image are not nested within one another. For example all participants see all 10 items.  Such a situation is typically referred to as one in which there are <span class="emph">crossed</span> random effects.  In such settings we have multiple sources variances to consider.


#### Example: Student achievement

For our own demonstration we'll look at achievement scores for students.  The sources of dependency are due to students having gone to the same primary or secondary schools.  However, in this example, going to a primary school doesn't necessarily mean you'll go to a specific secondary school.  Note also that there are no repeated measures, we see each student only one. Here's a quick look a the data, and for more detail, check the [appendix][Data].


```{r pupil_nurses_setup, echo=FALSE, eval=FALSE}
pupils = read_sav('data/raw_data/joop_hox_data2/9 CrossClass/pupcross.sav') %>% 
  as_factor() %>% 
  mutate(ACHIEV = as.numeric(as.character(ACHIEV)),
         PUPSEX = factor(PUPSEX, labels=c('male', 'female')) ) %>% 
  rename(achievement = ACHIEV,
         primary_school_id = PSCHOOL,
         secondary_school_id = SSCHOOL,
         sex = PUPSEX,
         ses = PUPSES,
         primary_denominational=PDENOM,
         secondary_denominational=SDENOM)  
save(pupils, file='data/pupils.RData')

nurses = read_sav('data/raw_data/joop_hox_data2/2 Basic Model/nurses.sav') %>% 
  as_factor() %>% 
  rename(experience = experien,
         treatment = expcon,
         sex = gender) %>% 
  mutate(treatment = factor(treatment, labels=c('Ctrl', 'Training')),
         age = as.numeric(as.character(age)),
         sex = factor(sex, labels=c('Male', 'Female'))) %>% 
  select(-starts_with('Z'), -starts_with('C'))
save(nurses, file='data/nurses.RData')
```


```{r examine_pupil_data, echo=1}
load('data/pupils.RData')
DT::datatable(pupils, 
              options=list(dom='tp', 
                           scrollX=T,  
                           autoWidth=T),
                           # columnDefs = list(list(width = '150px', targets = 1),
                           #                   list(width = '100px', targets = 3))), 
              rownames=F)
```
<br>
<br>

For our mixed model we'll look at the effects for `sex` and socioeconomic status, `ses`, a six level variable from low to high, on scholastic achievement.  The range of achievement scores is roughly `r round(min(pupils$achievement))` to `r round(max(pupils$achievement))`, with mean of `r round(mean(pupils$achievement), 1)` and standard deviation `r round(sd(pupils$achievement), 1)`.  We'll take into account the clustering at primary school and secondary school.  To incorporate the additional structure in <span class="pack">lme4</span> syntax is very easy, we just do as we did before for both grouping factors[^crossed_notation].

```{r cross_classified, eval=1}
pupils_crossed = lmer(achievement ~ sex + ses 
                      + (1|primary_school_id) + (1|secondary_school_id), 
                      data = pupils)
summary(pupils_crossed, correlation=F)
```


```{r cross_classified_fixed, echo=FALSE}
pander(tidy(pupils_crossed, 'fixed', conf.int=T) %>% mutate_if(is.numeric, arm::fround, digits=2))
```

The fixed effects tell us there is a positive effect of being female on achievement, and in general, relative to lowest SES category, being in the upper categories of SES also has a positive effect.

```{r  cross_classified_random, echo=FALSE}
# note tidy doesn't work with multiple random effects and conf.int
crossed_var_cor = tidy(VarCorr(pupils_crossed)) %>% 
  select(-var2) %>% 
  rename(variance=vcov, sd=sdcor)
crossed_var_cor %>%  
  mutate_if(is.numeric, arm::fround, digits=2) %>% 
  mutate_all(function(x) ifelse(is.na(x), '', x)) %>%
  pander()

```

When we look at the variance components we see that primary and secondary school contributes about `r round(sum(crossed_var_cor$variance[1:2])/sum(crossed_var_cor$variance)*100)`% of the total variance.  Most of the variance attributable to school comes from the primary school.

Note that we have the usual extensions here if desired.  As an example, we could also do random slopes for student level characteristics.



### Hierarchical Structure

Now that we have looked at cross-classified models, we can examine hierarchical cluster structuring.  In this situation we have clusters nested within other clusters, which may be nested within still other clusters.  A typical example might be cities within counties, and counties within states.

#### Example: Nurses and Stress

For our demonstration we'll use the nurses data set. Here we are interested in the effect of a training program (`treatment`) on stress levels (on a scale of 1-7) of nurses.  In this scenario, nurses are nested within wards, which themselves are nested within hospitals, so we will have random effects pertaining to ward (within hospital) and hospital. For more information see the [appendix][Data].

```{r nurses_data, echo=1}
load('data/nurses.RData')
DT::datatable(nurses, 
              options=list(dom='tp', 
                           scrollX=T,  
                           autoWidth=T),
                           # columnDefs = list(list(width = '150px', targets = 1),
                           #                   list(width = '100px', targets = 3))), 
              rownames=F)
```

<br>
<br>

For the model we examine effects of the treatment as well as several other covariates, at least one at each of the nurse, ward, and hospital levels. Again, when it comes to the fixed effects portion, you can simply think about that part as you would any standard regression, we just add covariates as theory/exploration would suggest.  To incorporate this type of random effects structure is not too different from the cross-classified approach, but does add a slight change.

```{r hierarchical, eval=1}
nurses_hierarchical = lmer(stress ~ age  + sex + experience 
                           + treatment + wardtype + hospsize 
                           + (1|hospital) + (1|hospital:wardid), data = nurses)
nurses_hierarchical = lmer(stress ~ age  + sex + experience 
                           + treatment + wardtype + hospsize 
                           + (1|hospital/wardid), data = nurses) # same thing!
summary(nurses_hierarchical, correlation=F)
```

```{r hierarchical_fixed, echo=FALSE}
tidy(nurses_hierarchical, 'fixed', conf.int=T) %>% 
         mutate_if(is.numeric, arm::fround, digits=2) %>% 
  pander(justify='lrrrrr')
```

As far as the fixed effects go, about the only thing that doesn't have a statistical effect is ward type[^signflip].  


```{r hierarchical_random, echo=FALSE}
# note tidy doesn't work with multiple random effects and conf.int
hierarch_var_cor = tidy(VarCorr(nurses_hierarchical)) %>% 
  select(-var2) %>% 
  rename(variance=vcov, sd=sdcor)
hierarch_var_cor %>%  
  mutate_if(is.numeric, arm::fround, digits=3) %>% 
  mutate_all(function(x) ifelse(is.na(x), '', x)) %>%
  pander(justify='lcrr')
```

Concerning the random effects, there appears to be quite a bit of variability from ward to ward especially, but also hospital.  Recall that stress is a 7 point scale, so from ward to ward we can expect scores to bounce around about half a  point on average, which is quite dramatic in my opinion. 

### Crossed vs. Nested

The following shows the difference in the results from treating ward as nested vs. crossed. Notice anything different?

```{r crossed_vs_nested, echo=1:2}
nurses_hierarchical = lmer(stress ~ age  + sex + experience 
                           + treatment + wardtype + hospsize 
                           + (1|hospital) + (1|hospital:wardid), data = nurses)
nurses_crossed = lmer(stress ~ age  + sex + experience 
                           + treatment + wardtype + hospsize 
                           + (1|hospital) + (1|wardid), data = nurses)
hierarch_var_cor %>%  
  mutate_if(is.numeric, arm::fround, digits=3) %>% 
  mutate_all(function(x) ifelse(is.na(x), '', x)) %>%
  pander(justify='lcrr')

crossed_var_cor = tidy(VarCorr(nurses_crossed)) %>% 
  select(-var2) %>%
  rename(variance=vcov, sd=sdcor)
crossed_var_cor %>%  
  mutate_if(is.numeric, arm::fround, digits=3) %>% 
  mutate_all(function(x) ifelse(is.na(x), '', x)) %>%
  pander(justify='lcrr')
```


No? Good, you're not crazy.  Here's a quote from the [lme4 text](http://lme4.r-forge.r-project.org/book/Ch2.pdf), section 2.2.1.1, which is definitely worth your time.

> The blurring of mixed-effects models with the concept of multiple,
hierarchical levels of variation results in an unwarranted emphasis on 'levels'
when defining a model and leads to considerable confusion. It is perfectly
legitimate to define models having random effects associated with non-nested
factors. The reasons for the emphasis on defining random effects with respect to
nested factors only are that such cases do occur frequently in practice, and that
some of the computational methods for estimating the parameters in the models
can only be easily applied to nested factors. 
> 
This is not the case for the methods used in the lme4 package. *Indeed there is
nothing special done for models with random effects for nested factors*. When
random effects are associated with multiple factors, exactly the same
computational methods are used whether the factors form a nested sequence or are
partially crossed or are completely crossed.

See this [discussion also](https://stats.stackexchange.com/questions/228800/crossed-vs-nested-random-effects-how-do-they-differ-and-how-are-they-specified), as well as [this from the FAQ](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#nested-or-crossed) from one of the <span class="pack">lme4</span> developers. 

So there you have it.  When it comes to <span class="pack">lme4</span>, crossed vs. nested is simply a state of mind (data)[^crossnest].


## Residual Structure

Sometimes we will want to estimate something regarding the residual correlation structure. This especially the case in the longitudinal setting, where we think that observations closer in time would be more strongly correlated than those further apart.  What does this model look like? Consider the following for an individual and three time points.

$$\boldsymbol{y} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$$

So we have three observations of $y$ that are multivariate normally distributed.  The mean $\mu$ is a function of covariates just like in standard regression.

$$\mu = b_0 + b_1\cdot \mathrm{time} + b_2\cdot x_1 ...$$

However, instead of just plopping an $\epsilon$ at the end, we want to go further in defining the entire residual variance/covariance structure for all three time points.

In the simplest setting we have constant variance and no covariance.

$$\Sigma = 
\left[
\begin{array}{ccc} 
\sigma^2 & 0   & 0   \\
0   & \sigma^2 & 0   \\
0   & 0   & \sigma^2 \\
\end{array}\right]$$


Now we actually want to get at the underlying correlation.  So now we have something like this, where $\rho$ represents the residual correlation.

$$\Sigma = 
\left[
\begin{array}{ccc} 
1 & \rho_1   & \rho_2   \\
\rho_1   & 1 & \rho_3   \\
\rho_2   & \rho_3   & 1 \\
\end{array}\right]$$



In this case we'd estimate a different correlation for all time point pairs.  This is typically described as an <span class="">unstructured</span> or simply 'symmetric' correlation structure. 

If you are familiar with repeated measures ANOVA, which is a [special case of a mixed model](https://m-clark.github.io/docs/mixedModels/anovamixed.html), you may recall that the usual assumption is a <span class="emph">sphericity</span>, a relaxed form of <span class="emph">compound symmetry</span>, where all the correlations have the same value, i.e. $\rho_1=\rho_2=\rho_3$, and all variances are equal.

Another very commonly used correlation structure is an <span class="emph">autocorrelation</span> structure, of lag order one, for the residuals.  What this means is that we assume the residuals at one time point apart correlate with some value $\rho$, observations at two time points apart correlate $\rho^2$, and so on.  As such we only need to estimate $\rho$, while the rest are then automatically determined.  Here's what it'd look like for four time points.

$$\Sigma = 
\left[
\begin{array}{cccc} 
1 & \rho     & \rho^2   & \rho^3   \\
\rho     & 1 & \rho     & \rho^2   \\
\rho^2   & \rho     & 1 & \rho     \\
\rho^3   & \rho^2   & \rho     & 1 \\
\end{array}\right]$$


If $\rho$ was estimated to be .5, it would look like the following.

$$\Sigma = 
\left[
\begin{array}{cccc} 
1 & .5       & .25      & .06   \\
.5       & 1 & .5       & .25  \\
.25      & .5       & 1 & .5    \\
.06      & .25      &  .5      & 1 \\
\end{array}\right]$$

Again, the main point is that points further apart in time are assumed to have less correlation. 

Know that there are many patterns and possibilities to potentially consider, and that they are not limited to the repeated measures scenario.  For example, the correlation could represent spatial structure.  We could also have variances that are different at each time point[^residstruct].

For reasons that defy my ability to parse, <span class="pack">lme4</span> does not provide the ability to model the residual correlation structure, though practically every other mixed model package does[^lmerho].  In fact, two packages that come with the basic R installation do so, <span class="pack">mgcv</span> and <span class="pack">nlme</span>.  We'll demonstrate with the latter. 

The following example shows the same model we did before, but with the autocorrelation structure we just described. In <span class="pack">nlme</span> we use the built in <span class="func">corAR1</span> function and `correlation` argument.  Note also the different random effect specification (though not *too* different).

```{r corr_residual, echo=1:3, eval=-3}
library(nlme)
corr_res = lme(gpa ~ occasion, 
               data = gpa,
               random = ~1|student, 
               correlation = corAR1(form = ~occasion))
summary(corr_res)
pander(corr_res, round=3)

vc = VarCorr(corr_res) # christ lme objects are the worst
cbind(rownames(vc), vc[,1:2]) %>% 
  data.frame(stringsAsFactors=F) %>% 
  mutate_at(vars(Variance, StdDev), function(x) round(as.numeric(x), 3)) %>% 
  rename(' ' = V1) %>% 
  pander
```
<br>

Notice first that the fixed effect for occasion is the same as [before][Mixed model]. The variance estimates have changed slightly along with the variances of the fixed effects (i.e. the standard errors). The main thing is that we have a new parameter called `Phi` in the <span class="pack">nlme</span> output,  that represents our autocorrelation, with value of `r round(coef(corr_res$model$corStruct, unconstrained = F), 3)`. This suggests at least some correlation exists among the residuals for observations next to each other in time, though it diminishes quickly as observations grow further apart.


## Generalized Linear Mixed Models

Just as generalized linear models extend the standard linear model, we can generalize (linear) mixed models to <span class="emph">generalized linear mixed models</span>.  Furthermore, there is nothing keeping us to only exponential family, as other packages would potentially allow for many other response distributions.

For this example we'll do a logistic regression in the mixed model setting. In this case, we'll use the speed dating data set. In the speed dating events, the experiment randomly assigned each participant to ten short dates (four minutes) with participants of the opposite sex. For each date, each person rated six attributes (attractive, sincere, intelligent, fun, ambitious, shared interests) of the other person on a 10-point scale and wrote down whether he or she would like to see the other person again.

Our target variable is whether the participant would be willing to date the person again (`decision`).  To keep things simple the predictors will be limited to the sex of the participant (`sex`), whether the partner was of the same race (`samerace`), and three of the attribute ratings the participant gave of their partner- attractiveness (`attractive`), sincerity (`sincere`), and intelligence (`intelligent`). The latter have been scaled to have zero mean and standard deviation of one (`_sc`).

```{r speed_dating, echo=FALSE, eval=FALSE}
speed_dating0 = readr::read_csv('data/raw_data/ARM_Data/Speed Dating Data.csv')
speed_dating = speed_dating0 %>% 
  select(1:17, attr, sinc, intel, fun, amb, shar, dec) %>% 
  rename(id_win_wave=id,
         sex = gender,
         partner_id=pid,
         n_met_in_wave=round,
         partner_age = age_o,
         partner_race = race_o,
         attractive = attr, 
         sincere = sinc, 
         intelligent = intel, 
         fun = fun, 
         ambitious = amb, 
         shared_interests = shar,
         decision=dec) %>% 
  mutate(decision = factor(decision, labels=c('No', 'Yes')),
         sex = factor(sex, labels=c('Female', 'Male')),
         samerace = factor(samerace, labels=c('No', 'Yes')),
         attractive_sc=scale(attractive)[,1],
         sincere_sc=scale(sincere)[,1],
         intelligent_sc=scale(intelligent)[,1],
         fun_sc=scale(fun)[,1],
         ambitious_sc=scale(ambitious)[,1],
         shared_interests_sc=scale(shared_interests)[,1]) %>% 
  group_by(iid) %>% 
  mutate(never_always = if_else(all(decision=='Yes') | all(decision=='No'), 1, 0)) %>% 
  ungroup() %>% 
  filter(never_always == 0) %>%  # as in Fahrmeier
  select(-never_always)
# describeAll(speed_dating)
save(speed_dating, file='data/speed_dating.RData')
```

```{r glmm_init, eval=FALSE, echo=FALSE}
# pretty much dupes fahrmeier although their table has a typo, and their would be 500, not 390 individuals after getting rid of constant
# sd_model = glmer(decision ~ sex*attractive_sc + sex*shared_interests_sc
#                  + (1|iid), data=speed_dating, family=binomial)   
load('data/speed_dating.RData')
sd_model = glmer(decision ~ sex + samerace + attractive_sc + sincere_sc
                 + intelligent_sc 
                 + (1|iid), data=speed_dating, family=binomial)
summary(sd_model, correlation=F)
glmm_var_cor = tidy(VarCorr(sd_model)) %>%   # because for some reason knitr can't find an object it just used in the previous chunk.
  select(-var2) %>% 
  rename(variance=vcov, sd=sdcor) %>%  
  mutate_if(is.numeric, arm::fround, digits=2) #%>% 
  # mutate_all(function(x) ifelse(is.na(x), '', x)) %>%
  # pander()
save(sd_model, glmm_var_cor, file='data/speed_dating_model.RData')
```

```{r glmm_speed_dating, eval=FALSE}
load('data/speed_dating.RData')
sd_model = glmer(decision ~ sex + samerace + attractive_sc + sincere_sc
                 + intelligent_sc 
                 + (1|iid), data=speed_dating, family=binomial)
summary(sd_model, correlation=F)
```


```{r glmm_fixed, echo=FALSE}
load('data/speed_dating_model.RData')
pander(tidy(sd_model, 'fixed', conf.int=T) %>% mutate_if(is.numeric, arm::fround, digits=2))
```
<br>

The fixed effects results are as expected for the attributes, with attractiveness being a very strong effect in particular.  In addition, having a partner of the same race had a positive effect while sex of the participant was statistically negligible.  You are free to exponentiate the coefficients to get the odds ratios if desired, just as you would with standard logistic regression.

<br>

```{r glmm_random, echo=FALSE, eval=TRUE}
pander(glmm_var_cor)
```

<br>

For the variance components, notice that there is no residual variance. This is because we are not modeling with the normal distribution for the response, thus there is no $\sigma$ to estimate.  However, the result suggests that there is quite a bit of variability from person to person.


## Exercises


### Sociometric data

In the following data, kids are put into different groups and rate each other in terms of how much they would like to share some activity with the others. We have identifying variables for the person doing the rating (sender), the person being rated (receiver), what group they are in, as well as age and sex for both sender and receiver, as well as group size.


```{r socio_setup, echo=FALSE, eval=FALSE}
soc = read_spss('data/raw_data/joop_hox_data2/9 CrossClass/SocsLong.sav')
glimpse(soc)


sociometric = soc %>% 
  mutate(sexsend = factor(sexsend, labels=c('Male', 'Female')),  # from text 0 male, 1 female
         sexrec = factor(sexrec, labels=c('Male', 'Female')))

save(sociometric, file='data/sociometric.RData')
```

In the following, load the sociometric data and run the following model. You will have three sources of structure to consider:

- senders (within group)
- receivers (within group)
- group


We will proceed with the following modeling steps. For each, make sure you are creating a separate model object for each one run.

- Model 1: No covariates, only sender and receiver random effects. Note that even though we don't add group yet, still use the nesting 
- Model 2: No covariates, add group random effect
- Model 3: Add all covariates
- Model 4: In order to examine sex match effects, do an interaction of the sex variables.
- Compare models with AIC


```{r socio, echo=1, eval=FALSE}
load('data/sociometric.RData')
model1 = lmer(rating ~ (1|group:sender) + (1|group:receiver), 
             data=sociometric)
summary(model1, correlation=F) 
model2 = lmer(rating ~ (1|group:sender) + (1|group:receiver) + (1|group), 
             data=sociometric)
summary(model2, correlation=F) 

model3 = lmer(rating ~ sexsend + sexrec + agesend + agerec + grsize + (1|group:sender) + (1|group:receiver) + (1|group), 
             data=sociometric)
summary(model3, correlation=F)
model4 = lmer(rating ~ sexsend*sexrec + agesend + agerec + grsize + (1|group:sender) + (1|group:receiver) + (1|group), 
             data=sociometric)
summary(model4, correlation=F)
c(AIC(model1), AIC(model2), AIC(model3), AIC(model4))
```




### Patents

Do a Poisson mixed effect model using the [patent data][Data].  Predict the number of citations (`ncit`) based on whether there was opposition (`opposition`) and if it was for the biotechnology/pharmaceutical industry (`biopharm`). Use year as a random effect to account for unspecified economic conditions.  

```{r patent_setup, echo=FALSE, eval=FALSE}
patents0 = readr::read_tsv('data/raw_data/patent.raw')
patents = patents0 %>% 
  rename(opposition = opp) 
save(patents, file='data/patents.RData')
glmer(ncit ~ opposition +  biopharm + (1|year), data=patents, family='poisson')
```




Interestingly, one can model overdispersion in a Poisson model by specifying an random intercept for each observation (`subject` in the data).  In other words, no clustering or grouped structure is necessary.

[^residstruct]: One reason to do so would be that you expect variability to decrease over time, e.g. due to experience.  You might also allow that variance to be different due to some other grouping factor entirely (e.g. due to treatment group membership).  I will likely add an example in the future.  But for the time being, in <span class="pack">nlme</span>, the relevant argument is `weights`.  See `?varIdent` as an example of what you would provide to the `weights` argument.

[^lmerho]: Don't get me wrong, while I'm extremely grateful to the work put forth by those involved with <span class="pack">lme4</span>, making it probably the best mixed model package out there, this feature request has been made by its users for over a decade at this point.

[^crossed_notation]: I don't show the formal model here as we did before, but this is why depicting mixed models solely as 'multilevel' becomes a bit problematic in my opinion. In the standard mixed model notation it's straightforward though, you just add an addition random effect term, just as we do in the actual model syntax.

[^signflip]: Setting aside our discussion to take a turn regarding regression modeling more generally, this is a good example of 'surprising' effects not being so surprising when you consider them more closely.  Take a look at the effect of experience. More experience means less stress, this is probably not surprising.  Now look at the age effect. It's positive! But wouldn't older nurses have more experience? What's going on here? When interpreting experience, it is with age *held constant*, thus more experience helps with lowering stress no matter what your age.  With age, we're holding experience constant.  If experience doesn't matter, being older is affiliated with more stress, which might be expected given the type of very busy and high pressure work often being done (the mean age is `r median(nurses$age)`).  A good way to better understand this specifically is to look at predicted values when age is young, middle, and older vs. experience levels at low, middle, and high experience, possibly explicitly including the interaction of the two in the model.  Also note that if you take experience out of the model, the age effect is negative, which is expected, as it captures experience also.

[^crossnest]: Note that it *does* matter if you label your data inappropriately.  For example, if in the nesting situation you start your id variable at 1 for each nested group, then you have to use the nested notation in <span class="pack">lme4</span>, otherwise, e.g. it won't know that id = 1 in group 1 is different from id 1 in group 2.  Again though, if you're not being silly, this wouldn't be an issue.

<!--chapter:end:03_extensions.Rmd-->

# Issues


## Alternative approaches

I have a [document](https://m-clark.github.io/docs/clustered/) that goes into more detail about many approaches to dealing with clustered data, but we can briefly talk about some here. Common alternatives used in clustered data situations include:

- Fixed effects models (also panel linear models with fixed, as opposed to random, effects)
- Using cluster-robust standard errors
- Generalized estimating equations (GEE)

The first two are commonly used by those trained with an econometrics perspective, while you might see GEE more with those of a biostatistics or other perspective. GEE are in fact a generalization of the cluster-robust approach, and extend the GLS to nonlinear/GLM settings.  The nature of fixed effects models allow you to control for, but not investigate any cluster level effects.  This makes them a non-starter for many investigations, as those are typically of prime theoretical interest.  GEE approaches allow one to take into account the dependency in the data, but ignore what might be very interesting, i.e. the random effects and associated variance. There are also few tools for GEE in more complicated correlational structures beyond a single clustering variable.


### Growth curve models

With longitudinal data, growth curve models are a latent variable approach that is commonly used in these situations. With appropriate setup, they will duplicate the results of a mixed model.  In my opinion, there are few reasons to use a growth curve approach over a mixed model, and many reasons not to, not least of which is that effects which would be simple to interpret in the mixed model approach are now a source of confusion to applied researchers in the growth curve model, even though it's the same thing.  Furthermore, indirect effects, growth mixture models and other extensions common in the latent variable approach are more easily implemented in the mixed model approach.  In short, only the most complicated models would perhaps require a growth curve model, but would also bring with it many other complications.  See more [here](https://m-clark.github.io/docs/sem/latent-growth-curves.html).



## Sample sizes

### Small number of clusters

Think about how many values of some variable you'd need before you felt comfortable with statistics based on it, especially standard deviation/variance.  That's at play with mixed models, in the sense you'd like to have enough groups to adequately assess the variance components. Mixed models will run with very small numbers, though the estimates will generally be biased.  I have a demo [here](https://m-clark.github.io/docs/mixedModels/growth_vs_mixed_sim.html) if interested.

One way to deal with this is to move to the Bayesian context, which will automatically induce some regularization  in parameter estimates.

### Small N within cluster

Mixed models work even with no more than two in each cluster and some singletons. Even in the simple case of pre-post design, mixed models are entirely applicable, though limited (you can't have random slopes).  So whenever you have clustering of some kind, you should consider mixed models.


### Balanced/Missing values

We've primarily been looking at <span class="emph">balanced</span> data, where each clusters have the same number of observations within them.  There is no requirement for this, and in many cases we wouldn't even expect it, e.g. people within geographical units.

However, if data is only missing on the outcome, or a mix of variables, we essentially have the same issue as with typical data situations, and will have the same considerations for dealing with missingness.  If you don't lose much data, the practical gain by ignoring missingness generally outweighs the complexities that can come with, for example, multiple imputation[^mi], even in the best of settings. By default, mixed models assume missing at random.  On the other hand, longitudinal data has special considerations, as there is increasing dropout over time.


## Model Comparison

Model comparison takes place in the usual way in the sense of potentially having statistical tests and information criteria.  Unfortunately, the typical likelihood ratio tests one might use in standard settings are not so straightforward here. For example, at a minimum you'd have to change the default estimation from REML to ML, and the models must have the same random effects structure, in order to compare models with different fixed effects for the resulting test p-value to be correct.  It works the other way to compare models with different random effects structure.

In my opinion, model selection involves considerations of theory, parsimony, and prediction, and those tests do not. I'm not partial to such tests even in the standard setting, and would use AIC here to potentially aid (not make) a model choice if I thought it was necessary, as I would there[^lrtest]. In general though, trying to determine a 'best' model with one set of data is a problematic endeavor at best, and at worst, completely misguided.  I think it's very useful to build models of increasing complexity, and select one to focus based on the available evidence.  Just don't get hung up on choosing one based solely on the outcome of a single statistic.  If you have a lot of data, you should consider some sort of explicit validation approach if you really want to compare competing models, but that is not without complication given the dependency in the data.



[^mi]: Multiple imputation is straightforward only in theory.  In practice it becomes a major pain to go very far beyond getting the parameter estimates for simple models.  Full information maximum likelihood is little implemented outside of SEM software/packages.

[^lrtest]: If you really want them see the <span class="pack">lmertest</span> package.  Note also that AIC does not come with a free lunch.  See the <span class="pack">cAIC4</span> package and references therein.

<!--chapter:end:04_issues.Rmd-->

# Going Further

This section covers topics that are generally beyond the scope of what would be covered in the workshop, but will be added to over time.

## Other distributions

As noted in the GLMM section, we are not held to use only GLM family distributions regarding the target variable.  However the tools you have available to do so will quickly diminish.  However, a couple packages could help in this regard with simpler random effects structures.  For example, the <span class="pack">mgcv</span> package allows one access to a variety of response distributions, such as $t$, negative binomial, beta, zero-inflated Poisson and more.  If you only have random intercepts you might try it.  If you're willing to go Bayesian, you'll have even more options with <span class="pack">rstanarm</span> and <span class="pack">brms</span>.  I've personally had success with ordinal, beta, truncated normal and more with <span class="pack">brms</span> in particular.


Note also that nothing says that the random effects must come from a normal distribution either.  You probably are going to need some notably strong theoretical reasons for trying something else though.  However, it does come up for some folks.  You'll almost certainly need to use a specialized approach, as most mixed model tools do not offer such functionality out of the box.


## Other contexts

Here is a list of some other contexts in which you can find random effects models, or extensions of mixed models into other situations.

##### Spatial models

It is often the case we want to take into account the geography of a situation. Spatial random effects allow one to do so in the continuous case, e.g. with latitude and longitude coordinates, as well as discrete, as with state. Typical random effects approaches, e.g. with a state random effect, would not correlate state effects.  One might capture geography incidentally, or via cluster level variables such as 'region' indicator.  However, if you're interested in a spatial random effect, use something that can account for it specifically.

##### Survival models

Random effects models in the survival context are typically referred to as frailty models.  As a starting point, the <span class="pack">survival</span> package that comes with R can do such models.

##### Item Response Theory

Some IRT models can be estimated as a mixed model, or otherwise thought of as incorporating random effects. See Boeck et al. (2011) The Estimation of Item Response Models with the lmer Function from the lme4 Package in R. I also have some brief demonstration [here](http://m-clark.github.io/docs/sem/item-response-theory.html). 

##### Multi-membership

Sometimes observations may belong to more than one cluster of some grouping variable. For example, in a longitudinal setting some individuals may move to other cities or schools, staying in one place longer than another.  Depending on the specifics of the modeling setting, you may need to take a multi-membership approach to deal with this.

##### Phylogenetic models

In biology, models make take observations that are of the same species. While one can use species as an additional source of variance, the species are not independent as they may come from the same phylogenetic tree/branch.  Bayesian packages are available to do such models (e.g. <span class="pack">MCMCglmm</span> and <span class="pack">brms</span>).

##### Adjacency structures

Similar to spatial and phylogenetic models, the dependency among the groups/clusters themselves can be described in terms of a markov random field/undirected graph.  In simpler terms, one may think of a situation  where a binary adjacency matrix would denote connections among the nodes/cluster groups.  For example, the clustering may be due to individuals, which themselves might be friends with one another.  One way to deal with such a situation would be similar to spatial models for discrete random units.

##### Gaussian Processes

Gaussian processes are another way to handle dependency in the data, especially over time or space. Some spatial models are in fact a special case. One can think of gaussian processes as adding a 'continuous category' random effect.  Consider the effect of age in many models, could that not also be a source of dependency regarding some outcomes? In *Statistical Rethinking*, McElreath has a nice chapter 'Adventures in Covariance' that gets into this a bit.  

##### Bayesian

I will eventually add a brief section on Bayesian mixed models to this document.  There are two good packages that allow one to use the <span class="pack">lme4</span> style even.  Suffice it to say, thinking about 'random effects' and distributions of parameters as one does in mixed models means you're halfway down the road to a Bayesian approach anyway.

##### Surveys & Mr. P

Clustering is often a result of sampling design.  Often one would use a survey design approach for proper inference in such situations.  However, <span class="emph">multi-level regression with post-stratification</span>, or Mr. P, is an alternative mixed model approach that can potentially lead to better results in the same setting. One might even be able to [generalize from a sample of Xbox players](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/04/forecasting-with-nonrepresentative-polls.pdf) to the national level!

##### Post-hoc comparisons and multiple testing 

This is not an issue I'm personally all that concerned with, but a lot of folks seem to be.  The 'problem' is that one has a lot of p-values for some model or across a set of models, and is worried about spurious claims of significance.  If one were truly worried about it, they'd be doing different models that would incorporate some sort of regularization, rather than attempting some p-value hack afterwards.  Didn't we talk about [regularization][Comparison to many regressions] somewhere?  Yep, you can use a mixed model approach instead. See [Gelman](http://www.stat.columbia.edu/~gelman/research/published/multiple2f.pdf) for details.

##### Growth mixture models

Often people will assume *latent* clusters of individuals within the data, with model effects differing by these latent groups also. Sometimes called <span class="emph">latent trajectory models</span>, these are conceptually adding a cluster analysis to the mixed model setting.  While common in structural equation modeling, packages like <span class="pack">flexmix</span> can keep you in the standard setting.


## Nonlinear Mixed Effects

Earlier we used the <span class="pack">nlme</span> package.  The acronym stands for nonlinear mixed effects models.  In this case, we are assuming a specific functional form for a predictor.  A common example is a logistic growth curve[^lgc], and one could use a function like <span class="func">SSlogis</span>.

In other cases we do not specify the functional form, and take a more non-parametric approach.  Here's where the powerful <span class="pack">mgcv</span> package comes in, and there are few if any that have its capabilities for <span class="emph">generalized additive models</span> combined with standard random effects approaches as well. Depending on the approach you take, you can even get <span class="pack">nlme</span> or <span class="pack">lme4</span> output along with the GAM results.  Highly recommended.

## Connections

The incorporation of spatial random effects, additive models, and mixed models altogether under one modeling roof is sometimes referred to as <span class="emph">structured additive regression</span> models, or STARs.  The <span class="pack">mgcv</span> package is at least one place where you can pull this off.  But the notion of a *random effect* is a broad one, and we might think of many such similar effects to add to a model.  

As mentioned previously, thinking of parameters as random, instead of fixed, essentially puts one in the Bayesian mindset.  Moving to that world for your modeling will open up many doors, including expanding your mixed model options.


[^lgc]: Not to be confused with latent growth curve models or logistic regression.

<!--chapter:end:05_further.Rmd-->

# Summary

The odds of you eventually coming across dependency in your data is very high.  Using standard methods while ignoring the situation can lead to problematic inference. More to the point however, you're missing out on a much richer story to tell with the data. One can estimate a variety of cluster-specific effects, incorporate multiple types of clustering, all while still be able to talk about global effects as well.  One can also extend such models to other types of 'random effects' as well.

With the right tools, even complicated mixed models can be fit relatively easily and quickly for even moderately large data sets.  It does take some getting used to, but in the end can be a highly satisfying modeling approach.  Use them the next time you encounter some dependency in your data.

<br><br>
<span class="" style="font-size:200%; font-variant:small-caps">Best of luck with your research!</span>

<!--chapter:end:06_summary.Rmd-->

# Appendix


## Data

Note that I have converted these from their original SPSS format to R data.frames saved within RData files. I also cleaned them up with better names/labels etc. For data sets used in that text, most of the description is taken from Joop Hox's text appendix ('Data Stories').

- <span class="emph">GPA</span>: The GPA data are a longitudinal data set, where 200 college students have been followed 6 consecutive semesters. In this data set, there are GPA measures on 6 consecutive occasions, with a JOB status variable (how many hours worked) for the same 6 occasions. There are two student-level explanatory variables: the gender (1= male, 2= female) and the high school GPA. There is also a dichotomous student-level outcome variable, which indicates whether a student has been admitted to the university of their choice. Since not every student applies to a university, this variable has many missing values. 

- <span class="emph">pupils</span>: Assume that we have data from 1000 pupils who have attended 100 different primary schools, and subsequently went on to 30 secondary schools. Similar to the situation where we have pupils within schools and neighborhoods, we have a cross-classified structure. Pupils are nested within primary and within secondary schools, with primary and secondary schools crossed. In other words: pupils are nested within the cross-classification of primary and secondary schools. In our example, we have a response variable achievement which is measured in secondary school. We have two explanatory variables at the pupil level: pupil gender (0 = male, 1 = female) and a six-point scale for pupil socioeconomic status, pupil ses. We have at the school level a dichotomous variable that indicates if the school is public (denom = 0) or denominational (denom = 1). Since we have both primary and secondary schools, we have two such variables (named pdenom for the primary school and sdenom for the secondary school).


- <span class="emph">nurses</span>: The data in this example are from a hypothetical study on stress in hospitals. The data are from nurses working in wards nested within hospitals. In each of 25 hospitals, four wards are selected and randomly assigned to an experimental and control condition. In the experimental condition, a training program is offered to all nurses to cope with job- related stress. After the program is completed, a sample of about 10 nurses from each ward is given a test that measures job-related stress. Additional variables are: nurse age (years), nurse experience (years), nurse gender (0 = male, 1 = female), type of ward (0 = general care, 1 = special care), and hospital size (0 = small, 1 = medium, 2 = large). This is an example of an experiment where the experimental intervention is carried out at the group level. In biomedical research this design is known as a cluster randomized trial. They are quite common also in educational and organizational research, where entire classes or schools are assigned to experimental and control conditions. Since the design variable Experimental versus Control group (ExpCon) is manipulated at the second (ward) level, we can study whether the experimental effect is different in different hospitals, by defining the regression coefficient for the ExpCon variable as random at the hospital level. In this example, the variable ExpCon is of main interest, and the other variables are covariates. Their function is to control for differences between the groups, which should be small given that randomization is used, and to explain variance in the outcome variable stress. To the extent that they are successful in explaining variance, the power of the test for the effect of ExpCon will be increased. Therefore, although logically we can test if explanatory variables at the first level have random coefficients at the second or third level, and if explanatory variables at the second level have random coefficients at the third level, these possibilities are not pursued. We do test a model with a random coefficient for ExpCon at the third level, where there turns out to be significant slope variation. This varying slope can be predicted by adding a cross-level interaction between the variables ExpCon and HospSize. In view of this interaction, the variables ExpCon and HospSize have been centered on their overall mean. 


- <span class="emph">sociometric</span>: The sociometric data are intended to demonstrate a data structure where the cross-classification is at the lowest level, with an added group structure because there are several groups. The story is that in small groups all members are asked to rate each other on a scale of 1-9, where higher numbers indicate a more positive view of the individual (i.e. how much they would like to share some activity with the rated person). Each record is defined by the senderreceiver pairs, with explanatory variables age and sex defined separately for the sender and the receiver. The group variable 'group size' is added to this file. There are 20 groups, with sizes ranging from 4 to 11.


- <span class="emph">speed dating</span>: Data involve a speed dating experiment on a sample of a few hundred students in graduate and professional schools at Columbia University. In the speed dating events, the experiment randomly assigned each participant to ten short dates (four minutes) with participants of the opposite sex. For each date, each person rated six attributes (attractive, sincere, intelligent, fun, ambitious, shared interests) of the other person on a 10-point scale and wrote down whether he or she would like to see the other person again. The data have been filtered to remove constant responders, i.e. those who always or never wanted to see  their partner again, those six attributes have scaled versions with mean 0 and standard deviation 1 (`_sc`), column names have been made useful, and only some of the variables have been kept for the demo. If you run the same model, you can get very similar estimates to that in the Fahrmeier et al. text, table 7.4 (though there is a typo for the Male effect).

- The data come from Gelman's website but are based on [this article](http://faculty.chicagobooth.edu/emir.kamenica/documents/genderDifferences.pdf).


- <span class="emph">patents</span>:
The European Patent Office is able to protect a patent from competition for a certain period of time. The Patent Office has the task to examine inventions and to declare patent if certain prerequisites are fulfilled. The most important requirement is that the invention is something truly new. Even though the office examines each patent carefully, in about 80% of cases competitors raise an objection against already assigned patents. In the economic literature the analysis of patent opposition plays an important role as it allows one to (indirectly) investigate a number of economic questions. For instance, the frequency of patent opposition can be used as an indicator for the intensity of the competition in different
market segments.

- The primary variables are: `opp`: Patent opposition (1=yes 0=no), `biopharm`: Patent from biotech/pharma sector, `ustwin`: US twin patent exists, `patus`: Patent holder from the USA, `patgsgr`: Patent holder from Germany, Switzerland, or Great Britain, `year`, `ncit`: Number of citations for the patent, `ncountry`: Number of designated states for the patent, and 
`nclaims`: Number of claims. Centered and other transformed variables are also present.

- In order to analyze objections against patents, a data set with 4,866 patents from the sectors biotechnology/pharmaceutics and semiconductor/computer was collected.  The goal of one analysis is to model the probability of patent opposition, while using a variety of explanatory variables for the binary response variable patent opposition (yes/no). This corresponds to a regression problem with a binary response. In other cases you might wish to analyze the number of citations.




## Programming languages

R has more mixed modeling capabilities than anything else out there.  Here are other options within R.

- <span class="pack">lme4</span>: generalized linear mixed models; extremely efficient
- <span class="pack">nlme</span>: (non-)linear mixed models but only for the gaussian case.
- <span class="pack">mgcv</span>: provides means to use both lme4 or nlme, extends distributional families, correlated residuals, and all that additive model stuff too
- <span class="pack">rstanarm</span>:  Bayesian with lme4 level options
- <span class="pack">brms</span>: Bayesian with possibly the most extensive mixed model capabilities out there
- <span class="pack">ordinal</span>: for various types of ordinal models



- Python: the [statsmodels](http://www.statsmodels.org/stable/mixed_linear.html) module has basic capabilities for mixed models but not too many frills (e.g. no crossed random effects).  CSCAR directory Kerby Shedden has been part of this specific development. You can see his notes from the Data Science Skills Series [here](https://github.com/kshedden/Statsmodels-MixedLM).
- Julia: one of the <span class="pack">lme4</span> authors develops a [MixedModels](https://github.com/dmbates/MixedModels.jl) package for Julia

### Proprietary

Among the standard stats packages, I can only recommend Stata for both ease of implementation and flexibility in modeling. SAS and SPSS both have non-intuitive and needlessly verbose syntax, and, while SAS is a fairly good tool for mixed models (from my understanding), I've seen SPSS consistently struggle with even simple models, and its GUI interface, the only reason to use it, is not even remotely intuitive.

Mplus has a lot of functionality both for standard mixed models and growth curve models, though between <span class="pack">lavaan</span>, <span class="pack">mediation</span>, and the rest of R, there is little need to use Mplus except for very complicated multilevel SEM, which requires very large samples and a lot of theoretical justification.

I will also say that there is zero reason to use mixed model specific software like HLM.  The days for such hijinks have long since passed.


## Reference texts and other stuff

An excellent modeling book can be found in [Data Analysis Using Regression and Multilevel/Hierarchical Models](http://www.stat.columbia.edu/~gelman/arm/).  You will learn a great deal about statistical modeling in general, as the mixed model stuff only comprises the second part.


On the applied side, CSCAR's own Brady West has a book- [Linear Mixed Models: A Practical Guide Using Statistical Software ](http://www-personal.umich.edu/~bwest/almmussp.html). It shows examples in a variety of programming formats while not glossing over the details you need to know.

Two texts come at mixed models from a very broad perspective, which I like. I find Fahrmeier et al. [Regression - Models, Methods and Applications](http://www.springer.com/us/book/9783642343322) to be very good.  It will also tie mixed models to spatial and nonparametric approaches.  [Richly Parameterized Linear Models - Additive, Time Series, and Spatial Models Using Random Effects](https://www.crcpress.com/Richly-Parameterized-Linear-Models-Additive-Time-Series-and-Spatial/Hodges/p/book/9781439866832), does as well, and provides some nice historical context and hits on practical issues you *will* encounter if you play with these models long enough.

There is literally [a book on mixed models with lme4](http://lme4.r-forge.r-project.org/lMMwR/lrgprt.pdf) by Douglas Bates, one of the developers. While dated with regard to <span class="pack">lme4</span>, it's definitely not in general, and worth having around.

[Mixed model FAQ](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html)

I have [several other docs](http://m-clark.github.io/documents/#mixed-models) on various aspects of mixed models.

<!--chapter:end:1000_appendix.Rmd-->

