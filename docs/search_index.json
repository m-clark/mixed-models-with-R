[["index.html", "Mixed Models with R Getting started with random effects", " Mixed Models with R Getting started with random effects Michael Clark m-clark.github.io "],["introduction.html", "Introduction Overview Workshop Key packages", " Introduction Overview Mixed models are an extremely useful modeling tool for situations in which there is some dependency among observations in the data, where the correlation typically arises from the observations being clustered in some way. For example, it is quite common to have data in which we have repeated measurements for the units of observation, or in which the units of observation are otherwise grouped together (e.g. students within school, cities within geographic region). While there are different ways to approach such a situation, mixed models are a very common and powerful tool to do so. In addition, they have ties to other statistical approaches that further expand their applicability. Goals This document also serves as the basis for a workshop. The goal of the workshop is primarily to provide a sense of when one would use mixed models and a variety of standard techniques to implement them. Exercises are available to practice your skills. Prerequisites The document is for the most part very applied in nature, and only assumes a basic understanding of standard regression models. Use of R for regression modeling is also assumed, though there will be some review. Demonstrations will be done almost entirely with the lme4 package. Note the following color coding used in this document: emphasis package function object/class link (with hover underline) Workshop If doing or interested in the workshop follow these steps: Download the zip file at GitHub. Be mindful of where you put it. Unzip it. Be mindful of where you put the resulting folder. Open RStudio. File/Open Project and click on the blue icon (mixed-models-with-r-workshop-2019.Rproj) in the folder you just created. File/Open Click on the ReadMe file and do what it says. Key packages To run the code in this document you’ll really only need the following: lme4 tidyverse (for data processing) merTools (optional) glmmTMB (optional) brms (optional) modelr (optional) nlme (part of base R, no need for install) I also use a custom package called mixedup that provides more usable and printable output for mixed models from lme4, brms, mgcv, etc. Much of the output you see will come from that. "],["random_intercepts.html", "Mixed Models Terminology Kinds of Clustering Random Intercepts Model Example: Student GPA The Standard Regression Model The Mixed Model Application Cluster Level Covariates Summary of Mixed Model Basics Exercises for Starting Out", " Mixed Models Mixed models have been around a long time in the statistical realm. For example, standard ANOVA methods can be seen as special cases of a mixed model. More recently, mixed models have a variety of applications and extensions, allowing them to encompass a diverse range of data situations. They can be seen as a first step in expanding one’s tool set beyond the generalized linear model. Terminology For the uninitiated, the terminology surrounding mixed models, especially across disciplines, can be a bit confusing. Some terms you might come across regarding these types of models include: Variance components Random intercepts and slopes Random effects Random coefficients Varying coefficients Intercepts- and/or slopes-as-outcomes Hierarchical linear models Multilevel models (implies multiple levels of hierarchically clustered data) Growth curve models (possibly Latent GCM) Mixed effects models All describe types of mixed models. Some terms might be more historical, others are more often seen in a specific discipline, others might refer to a certain data structure, and still others are special cases. Mixed effects, or simply mixed, models generally refer to a mixture of fixed and random effects. For the models in general, I prefer the terms ‘mixed models’ or ‘random effects models’ because they are simpler terms, no specific structure is implied, and the latter can also apply to extensions that many would not think of when other terms are used1. Regarding the mixed effects, fixed effects is perhaps a poor but nonetheless stubborn term for the typical main effects one would see in a linear regression model, i.e. the non-random part of a mixed model, and in some contexts they are referred to as the population average effect. Though you will hear many definitions, random effects are simply those specific to an observational unit, however defined. The approach outlined in this document largely pertains to the case where the observational unit is the level of some grouping factor, but this is only one possibility. Kinds of Clustering Data might have one or multiple sources of clustering, and that clustering may be hierarchical, such that clusters are nested within other clusters. An example would be scholastic aptitude tests given multiple times to students (repeated observations nested within students, students nested within schools, schools nested within districts). In other cases, there is no nesting structure. An example would be a reaction time experiment where participants perform the same set of tasks. While observations are nested within individual, observations are also clustered according to task type. Some use the terms nested and crossed to distinguish between these scenarios. In addition, clustering may be balanced or not. We might expect more balance in studies of an experimental nature, but definitely not in other cases, e.g. where the cluster is something like geographical unit and the observations are people. In what follows we’ll see mixed effect models in all these data situations. In general, our approach will be the same, as such clustering is really more a property of the data than the model. However, it’s important to get a sense of the flexibility of mixed models to handle a variety of data situations. Random Intercepts Model For the following we’ll demonstrate the simplest2 and most common case of a mixed model, that in which we have a single grouping/cluster structure for the random effect. For reasons that will hopefully become clear soon, this is commonly called a random intercepts model. Example: Student GPA For the following we’ll assess factors predicting college grade point average (GPA). Each of the 200 students is assessed for six occasions (each semester for the first three years), so we have observations clustered within students. We have other variables such as job status, sex, and high school GPA. Some will be in both labeled and numeric form. See the appendix for more detail. The Standard Regression Model Now for the underlying model. We can show it in a couple different ways. First we start with just a standard regression to get our bearings. \\[\\mathscr{gpa} = b_{\\mathrm{intercept}} + b_{\\mathrm{occ}}\\cdot \\mathscr{occasion} + \\epsilon\\] We have coefficients (\\(b\\)) for the intercept and the effect of time. The error (\\(\\epsilon\\)) is assumed to be normally distributed with mean 0 and some standard deviation \\(\\sigma\\). \\[\\epsilon \\sim \\mathscr{N}(0, \\sigma)\\] An alternate way to write the model which puts emphasis on the underlying data generating process for \\(\\mathrm{gpa}\\) can be shown as follows. \\[\\mathscr{gpa} \\sim \\mathscr{N}(\\mu, \\sigma)\\] \\[\\mu = b_{\\mathrm{intercept}} + b_{\\mathrm{occ}}\\cdot \\mathscr{occasion}\\] More technically, the GPA and \\(\\mu\\) variables have an implicit subscript to denote each observation, but you can also think of it as a model for a single individual at a single time point. The Mixed Model Initial depiction Now we show one way of depicting a mixed model that includes a unique effect for each student. Consider the following model for a single student3. This shows that the student-specific effect, i.e. the deviation in GPA just for that student being who they are, can be seen as an additional source of variance. \\[\\mathscr{gpa} = b_{\\mathrm{intercept}} + b_{\\mathrm{occ}}\\cdot \\mathscr{occasion} + (\\mathrm{effect}_{\\mathscr{student}} + \\epsilon)\\] We would (usually) assume the following for the student effects. \\[\\mathrm{effect}_{\\mathrm{student}} \\sim \\mathscr{N}(0, \\tau)\\] Thus the student effects are random, and specifically are normally distributed with mean of zero and some estimated standard deviation (\\(\\tau\\)). In other words, conceptually the only difference between this mixed model and a standard regression is the student effect, which on average is no effect, but typically varies from student to student by some amount that is on average \\(\\tau\\). If we rearrange it, we can instead focus on model coefficients, rather than as an additional source of error. \\[\\mathscr{gpa} = (b_{\\mathrm{intercept}} + \\mathrm{effect}_{\\mathscr{student}}) + b_{\\mathrm{occ}}\\cdot \\mathscr{occasion} + \\epsilon\\] Or more succinctly: \\[\\mathscr{gpa} = b_{\\mathrm{int\\_student}} + b_{\\mathrm{occ}}\\cdot \\mathscr{occasion} + \\epsilon\\] In this way, we’ll have student-specific intercepts, as each person will have their own unique effect added to the overall intercept, resulting in a different intercept for each person. \\[b_{\\mathrm{int\\_student}} \\sim \\mathscr{N}(b_{\\mathrm{intercept}}, \\tau)\\] Now we see the intercepts as normally distributed with a mean of the overall intercept and some standard deviation. As such this is often called a random intercepts model. As a multi-level model Another way of showing the mixed model is commonly seen in the multilevel modeling literature. It is shown more explicitly as a two part regression model, one at the observation level and one at the student level. \\[\\mathrm{gpa} = b_{\\mathrm{int\\_student}} + b_{\\mathrm{occ}}\\cdot \\mathrm{occasion} + \\epsilon\\] \\[b_{\\mathrm{int\\_student}} = b_{\\mathrm{intercept}} + \\mathrm{effect}_{\\mathrm{student}}\\] However, after ‘plugging in’ the second level part to the first, it is identical to the previous. Note how we don’t have a student-specific effect for occasion. In this context, occasion is said to be a fixed effect only, and there is no random component. This definitely does not have to be the case though, as we’ll see later. Application Initial visualization It always helps to look before we leap, so let’s do so. Here we plot GPA vs. occasion (i.e. semester) to get a sense of the variability in starting points and trends. All student paths are shown in faded paths, with a sample of 10 shown in bold. The overall trend, as estimated by the regression we’ll do later, is shown in red. Two things stand out. One is that students have a lot of variability in starting out. Secondly, while the general trend in GPA is upward over time as we’d expect, individual students may vary in that trajectory. Standard regression So let’s get started. First, we’ll look at the regression and only the time indicator as a covariate, which we’ll treat as numeric. Note that I present a cleaner version of the summarized objects for the purposes of this document. load(&#39;data/gpa.RData&#39;) gpa_lm = lm(gpa ~ occasion, data = gpa) ## summary(gpa_lm) Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.599 0.018 145.7 0 occasion 0.106 0.006 18.04 0 Fitting linear model: gpa ~ occasion Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 1200 0.3487 0.2136 0.2129 The above tells us that starting out, i.e. when occasion is zero, the average GPA, denoted by the intercept, is 2.6. In addition, as we move from semester to semester, we can expect GPA to increase by about 0.11 points. This would be fine except that we are ignoring the clustering. A side effect of doing so is that our standard errors are biased, and thus claims about statistical significance based on them would be off. More importantly however is that we simply don’t get to explore the student effect, which would be of interest by itself. Regression by cluster An alternative approach would be to run separate regressions for every student. However, there are many drawbacks to this- it’s not easily summarized when there are many groups, typically there would be very little data within each cluster to do so (as in this case), and the models are over-contextualized, meaning they ignore what students have in common. We’ll compare such an approach to the mixed model later. Running a mixed model Next we run a mixed model that will allow for a student specific effect. Such a model is easily conducted in R, specifically with the package lme4. In the following, the code will look just like what you used for regression with lm, but with an additional component specifying the group, i.e. student, effect. The (1|student) means that we are allowing the intercept, represented by 1, to vary by student. With the mixed model, we get the same results as the regression, but as we’ll see we’ll have more to talk about. library(lme4) gpa_mixed = lmer(gpa ~ occasion + (1 | student), data = gpa) ## summary(gpa_mixed) term value se t p_value lower_2.5 upper_97.5 Intercept 2.599 0.022 119.800 0 2.557 2.642 occasion 0.106 0.004 26.096 0 0.098 0.114 group effect variance sd student Intercept 0.064 0.252 Residual 0.058 0.241 First we see that the coefficients, i.e. or in this context they can be called the fixed effects, for the intercept and time are the same4 as we saw with the standard regression, as would be their interpretation. The standard errors, on the other hand are different here, though in the end our conclusion as far as statistical significance goes would be the same. Note specifically that the standard error for the intercept has increased. Conceptually you can think about allowing random intercepts per person allows us to gain information about the individual, while recognizing the uncertainty with regard to the overall average that we were underestimating before5. While we have coefficients and standard errors, you might have noticed that lme4 does not provide p-values! There are several reasons for this, namely that with mixed models we are essentially dealing with different sample sizes, the \\(N_c\\) within cluster, which may vary from cluster to cluster (and even be a single observation!), and N total observations, which puts us in kind of a fuzzy situation with regard to reference distributions, denominator degrees of freedom and how to approximate a ‘best’ solution. Other programs provide p-values automatically as if there is no issue, and without telling you which approach they use to calculate them (there are several). Furthermore, those approximations may be very poor in some scenarios, or make assumptions that may not be appropriate for the situation6. However, it’s more straightforward to get confidence intervals, and we can do so with lme4 as follows7. confint(gpa_mixed) group effect variance sd sd_2.5 sd_97.5 var_prop student Intercept 0.064 0.252 0.225 0.282 0.523 Residual 0.058 0.241 0.231 0.252 0.477 Variance components One thing that’s new compared to the standard regression output is the estimated variance/standard deviation of the student effect (\\(\\tau\\) in our formula depiction from before). This tells us how much, on average, GPA bounces around as we move from student to student. In other words, even after making a prediction based on time point, each student has their own unique deviation, and that value (in terms of the standard deviation) is the estimated average deviation across students. Note that scores move due to the student more than double what they move based on a semester change. Another way to interpret the variance output is to note percentage of the student variance out of the total, or 0.064 / 0.122 = 52%. This is also called the intraclass correlation, because it is also an estimate of the within cluster correlation, as we’ll see later. Estimates of the random effects After running the model, we can actually get estimates of the student effects8. I show two ways for the first five students, both as random effect and as random intercept (i.e. intercept + random effect). ranef(gpa_mixed)$student %&gt;% head(5) # showing mixedup::extract_random_effects(gpa_mixed) group_var effect group value se lower_2.5 upper_97.5 student Intercept 1 -0.071 0.092 -0.251 0.109 student Intercept 2 -0.216 0.092 -0.395 -0.036 student Intercept 3 0.088 0.092 -0.091 0.268 student Intercept 4 -0.187 0.092 -0.366 -0.007 student Intercept 5 0.030 0.092 -0.149 0.210 coef(gpa_mixed)$student %&gt;% head(5) group_var effect group value se lower_2.5 upper_97.5 student Intercept 1 2.528 0.095 2.343 2.713 student Intercept 2 2.383 0.095 2.198 2.568 student Intercept 3 2.687 0.095 2.502 2.872 student Intercept 4 2.412 0.095 2.227 2.597 student Intercept 5 2.629 0.095 2.444 2.814 Note that we did not allow occasion to vary, so it is a constant, i.e. fixed, effect for all students. Often, we are keenly interested in these effects, and want some sense of uncertainty regarding them. With lme4 this typically would be done via bootstrapping, specifically with the bootMer function within lme4. However, for some users this may be a bit of a more complex undertaking. The merTools package provides for an easy way to get this with the predictInterval function9. Or you can go straight to the plot of them. library(merTools) predictInterval(gpa_mixed) # for various model predictions, possibly with new data REsim(gpa_mixed) # mean, median and sd of the random effect estimates plotREsim(REsim(gpa_mixed)) # plot the interval estimates The following plot is of the estimated random effects for each student and their interval estimate (a modified version of the plot produced by that last line of code10). Recall that the random effects are normally distributed with a mean of zero, shown by the horizontal line. Intervals that do not include zero are in bold. Prediction Let’s now examine standard predictions vs. cluster-specific predictions. As with most R models, we can use the predict function on the model object. predict(gpa_mixed, re.form=NA) %&gt;% head() 1 2 3 4 5 6 2.599214 2.705529 2.811843 2.918157 3.024471 3.130786 In the above code we specified not to use the random effects re.form=NA, and as such, our predictions for the observations are pretty much what we’d get from the standard linear model. predict_no_re = predict(gpa_mixed, re.form=NA) predict_lm = predict(gpa_lm) But each person has their unique intercept, so let’s see how the predictions differ when we incorporate that information. predict_with_re = predict(gpa_mixed) Depending on the estimated student effect, students will start above or below the estimated intercept for all students. The following visualizes the unconditional prediction vs. the conditional prediction that incorporates the random intercept for the first two students. We can see that the predictions from the mixed model are shifted because of having a different intercept. For these students, the shift reflects their relatively poor start. Cluster Level Covariates Note our depiction of a mixed model as a multilevel model. \\[\\mathrm{gpa} = b_{\\mathrm{int\\_student}} + b_{\\mathrm{occ}}\\cdot \\mathrm{occasion} + \\epsilon\\] \\[b_{\\mathrm{int\\_student}} = b_{\\mathrm{intercept}} + \\mathrm{effect}_{\\mathrm{student}}\\] If we add student a student level covariate, e.g sex, to the model, we then have the following. \\[b_{\\mathrm{int\\_student}} = b_{\\mathrm{intercept}} + b_{sex}\\cdot \\mathrm{sex} + \\mathrm{effect}_{\\mathrm{student}}\\] Which, after plugging in, we still have the same model as before, just with an additional predictor. \\[\\mathrm{gpa} = b_{\\mathrm{intercept}} + b_{\\mathrm{occ}}\\cdot \\mathrm{occasion}+ b_{sex}\\cdot \\mathrm{sex} + (\\mathrm{effect}_{\\mathscr{student}} + \\epsilon)\\] Thus, adding cluster level covariates doesn’t have any unusual effect on how we think about the model11. We simply add them to our set of predictor variables. Note also, that we can create cluster level covariates as means or some other summary of the observation level variables. This is especially common when the clusters represent geographical units and observations are people. For example, we might have income as a person level covariate, and use the median to represent the overall wealth of the geographical region. Summary of Mixed Model Basics Mixed models allow for us to take into account clustering in the data. If this were all it was used for, we would have more accurate inference relative to what would be had if we ignored the structure in the data. However, we get much more. We better understand the sources of variability in the target variable. We also get group specific estimates of the parameters in the model, allowing us to understand exactly how the groups differ from one another. Furthermore, this in turn allows for group specific prediction, and thus much more accurate prediction, assuming there is appreciable variance due to the clustering. In short, there is much to be gained by mixed models, even in the simplest of settings. Exercises for Starting Out Sleep For this exercise, we’ll use the sleep study data from the lme4 package. The following describes it. The average reaction time per day for subjects in a sleep deprivation study. On day 0 the subjects had their normal amount of sleep. Starting that night they were restricted to 3 hours of sleep per night. The observations represent the average reaction time (in milliseconds) on a series of tests given each day to each subject. After loading the package, the data can be loaded as follows. I show the first few observations. library(lme4) data(&quot;sleepstudy&quot;) Reaction Days Subject 249.5600 0 308 258.7047 1 308 250.8006 2 308 321.4398 3 308 356.8519 4 308 414.6901 5 308 Run a regression with Reaction as the target variable and Days as the predictor. Run a mixed model with a random intercept for Subject. Interpret the variance components and fixed effects. Adding the cluster-level covariate Rerun the mixed model with the GPA data adding the cluster level covariate of sex, or high school GPA (highgpa), or both. Interpret all aspects of the results. What happened to the student variance after adding cluster level covariates to the model? Simulating a mixed model The following represents a simple way to simulate a random intercepts model. Note each object what each object is, and make sure the code make sense to you. Then run it. set.seed(1234) # this will allow you to exactly duplicate your result Ngroups = 100 NperGroup = 3 N = Ngroups * NperGroup groups = factor(rep(1:Ngroups, each = NperGroup)) u = rnorm(Ngroups, sd = .5) e = rnorm(N, sd = .25) x = rnorm(N) y = 2 + .5 * x + u[groups] + e d = data.frame(x, y, groups) Which of the above represent the fixed and random effects? Now run the following. model = lmer(y ~ x + (1|groups), data=d) summary(model) confint(model) library(ggplot2) ggplot(aes(x, y), data=d) + geom_point() Do the results seem in keeping with what you expect? In what follows we’ll change various aspects of the data, then rerun the model after each change, then summarize and get confidence intervals as before. For each note specifically at least one thing that changed in the results. First calculate or simply eyeball the intraclass correlation coefficient \\(\\frac{\\textrm{random effect variance}}{\\textrm{residual + random effect variance}}\\). In addition, create a density plot of the random effects as follows. re = ranef(model)$groups qplot(x = re, geom = &#39;density&#39;, xlim = c(-3, 3)) Change the random effect variance/sd and/or the residual variance/sd and note your new estimate of the ICC, and plot the random effect as before. Reset the values to the original. Change Ngroups to 50. What differences do you see in the confidence interval estimates? Set the Ngroups back to 100. Now change NperGroup to 10, and note again the how the CI is different from the base condition. I actually like Richly Parameterized Linear Models, or Structured Additive Regression Models. Both are a mouthful, but at least the latter reduces to STARs.↩︎ Actually, the simplest model would have no covariates at all, just variance components, with no correlations among the random effects. Such a model can be interesting to look at while exploring your data, but would probably never suffice on its own to tell the story you desire to.↩︎ Note that I leave out the observation level subscript to keep things clean. I find that multilevel style notation quickly becomes unwieldy, and don’t wish to reproduce it. It also tends to add confusion to a lot of applied researchers starting out with mixed models.↩︎ This will not always be the case, e.g. with unbalanced data, but they should be fairly close.↩︎ The standard error for our time covariate went down due to our estimate of \\(\\sigma\\) being lower for this model, and there being no additional variance due to cluster membership.↩︎ Note that many common modeling situations involve a fuzzy p setting, but especially penalized regression approaches such as mixed, additive, ridge regression models etc. Rather than be a bad thing, this usually is a sign you’re doing something interesting, or handling complexity in an appropriate way.↩︎ See ?confint.merMod for details and options. The output you see is based on my wrapper mixedup::extract_vc.↩︎ These are sometimes referred to as BLUPs or EBLUPs, which stands for (empirical) best linear unbiased prediction. However, they are only BLUP for linear mixed effects models. As such you will also see them referred to as conditional mode. Furthermore, in the Bayesian context, the effects are actually estimated as additional model parameters, rather than estimated/predicted after the fact.↩︎ Note that while predictionInterval does not quite incorporate all sources of uncertainty as does bootMer, it’s actually feasible for larger data sets, and on par with the Bayesian results (e.g. with rstanarm).↩︎ Note that the default plot from merTools is confusingly labeled for single random effect, because it unnecessarily adds a facet. You’ll understand it better by looking the plot in the discussion of crossed random effects later. However, the one displayed is from my own package, visibly.↩︎ This is why the multilevel depiction is sub-par, and leads many to confusion at times. You have a target variable and predictor variables based on theory. Whether they are cluster level variables or if there are interactions doesn’t have anything to do with the data structure as much as it does the theoretical motivations. However, if you choose to depict the model in multilevel fashion, the final model must adhere to the ‘plugged in’ result. So if, for example, you posit a cluster level variable for a random slope, you must include the implied interaction of the cluster level and observation level covariates.↩︎ "],["random_slopes.html", "More Random Effects Application Comparison to many regressions Visualization of effects Summary of Random Slopes Exercises for Random Slopes", " More Random Effects Previously we’ve looked at random intercepts, but any observation or lower level covariate effect could be allowed to vary by cluster as well. Application Returning to the GPA data, recall the visualization from before. I’ve animated it to highlight the differences among students. Let us now assume that the trend over time is allowed to vary by student in addition to the intercepts. Using lme4, this is quite straightforward. gpa_mixed = lmer(gpa ~ occasion + (1 + occasion | student), data = gpa) summary(gpa_mixed) Pretty easy huh? Within the parenthesis, to the left of that bar | we are just positing a model formula as we would do with most modeling functions12. Let’s look at the results. term value se lower_2.5 upper_97.5 Intercept 2.599 0.018 2.563 2.635 occasion 0.106 0.006 0.095 0.118 group effect variance sd var_prop student Intercept 0.045 0.213 0.491 student occasion 0.005 0.067 0.049 Residual 0.042 0.206 0.460 As before, since we have 0 as our starting semester, the intercept tells us what the average GPA is in the first semester. The coefficient for occasion still reflects a one semester change in GPA. As we have not changed the fixed effect portion of the model, the values are the same as before. The associated intercept variance tells us how much that starting GPA bounces around from student to student. The variance for the occasion effect might not look like much in comparison, but slopes are on a notably different scale than the intercept. Note that the mean slope for the semester to semester effect, our fixed effect, is 0.11, but from student to student it bounces around half that. Thus we could expect most students to fall somewhere between a flat effect of zero to more than double the population average13. Yet another point of interest is the correlation of the intercepts and slopes. In this case it’s -0.1. That’s pretty small, but the interpretation is the same as with any correlation. In this case specifically, it tells us that those with lower intercepts would be associated with increased time trajectories. This makes intuitive sense in that people are improving in general, and those at the bottom would have more room to improve. However, this is very slight, and practically speaking we might not put too much weight on it. Comparison to many regressions Let’s compare these results to the ones we would have gotten had we run a separate regression for each student. In what follows we see the distribution of of the estimated intercept and slope coefficients for all the students. Note that these are the same estimates one would have gotten with a fixed effects model with an occasion by student interaction14. Here we can see that the mixed model intercepts are generally not as extreme, i.e. the tails of the distribution have been pulled toward the overall effect. Same goes for the slopes. In both cases the mixed model shrinks what would have been the by-group estimate, which would otherwise overfit in this scenario. This regularizing effect is yet another bonus when using mixed models15. It comes into play largely when we have few observations per group and less estimated variance for the random effects. In other words, when there is little information in a group, or less group-level variance relative to the observation variance, then the mixed model will produce a group-specific effect that is closer to the overall population effect. In that sense, the mixed model group coefficients better reflect our ignorance. Conversely, with more pronounced group effects, our uncertainty about the overall effect increases. The following shows what would happen to similar data under a variety of settings with simulated data that is based on the results of the GPA model we had above. On the far left is the plot just shown, our current data setting. Then there are four settings to go along with the original results. The first shows what would happen had we taken many more measurements per student. In the next, we add to the intercept and slope variance, and decrease the residual variance, but keep the sample size the same as the original data. In both cases we have a less regularizing effect of the mixed model. The random coefficients are very similar to the separate regressions results. Then, we keep the data the same but where we only have 4 observations per student, which shows more variability in the per-student results, and thus relatively more shrinkage with the mixed model. Finally, we add to the number of occasions per student (10), but have dropout over time, and so have roughly the same amount of data, but which is imbalanced. For more on this see topic, see my post here. Visualization of effects Now let’s compare our predictions visually. First there is the linear regression fit. We assume the same starting point and trend for everyone. If we add the conditional predictions that include the subject specific effects from the mixed model, we now can also make subject specific predictions, greatly enhancing the practical use of the model. In contrast, the by-group approach is more noisy due to treating everyone independently. Many more students are expected to have downward or flat trends relative to the mixed model. The mixed model meanwhile only had 3 trends estimated to be negative. Summary of Random Slopes At this point it might be clearer why some would call these richly parameterized linear models. Relative to a standard regression we get extra variance parameters that add to our understanding of the sources of uncertainty in the model, we can get the subjects specific effects, and their correlation, and use that information to obtain far better predictions. What’s not to like? Exercises for Random Slopes Sleep revisited Run the sleep study model with random coefficient for the Days effect, and interpret the results. What is the correlation between the intercept and Days random effects? Use the ranef and coef functions on the model you’ve created to inspect the individual-specific effects. What do you see? library(lme4) data(&quot;sleepstudy&quot;) In the following, replace model with the name of your model object. Run each line, inspecting the result of each as you go along. re = ranef(model)$Subject fe = fixef(model) apply(re, 1, function(x) x + fe) %&gt;% t() The above code adds the fixed effects to each row of the random effects (the t just transposes the result). What is the result compared to what you saw before? Simulation revisited The following shows a simplified way to simulate some random slopes, but otherwise is the same as the simulation before. Go ahead and run the code. set.seed(1234) # this will allow you to exactly duplicate your result Ngroups = 50 NperGroup = 3 N = Ngroups * NperGroup groups = factor(rep(1:Ngroups, each = NperGroup)) re_int = rnorm(Ngroups, sd = .75) re_slope = rnorm(Ngroups, sd = .25) e = rnorm(N, sd = .25) x = rnorm(N) y = (2 + re_int[groups]) + (.5 + re_slope[groups]) * x + e d = data.frame(x, y, groups) This next bit of code shows a way to run a mixed model while specifying that there is no correlation between intercepts and slopes. There is generally no reason to do this unless the study design warrants it16, but you could do it as a step in the model-building process, such that you fit a model with no correlation, then one with it. model_ints_only = lmer(y ~ x + (1|groups), data = d) model_with_slopes = lmer(y ~ x + (1|groups) + (0 + x|groups), data = d) summary(model_with_slopes) confint(model_with_slopes) library(ggplot2) ggplot(aes(x, y), data = d) + geom_point() Compare model fit using the AIC function, e.g. AIC(model). The model with the lower AIC is the better model, so which would you choose? Technically the intercept is assumed but you should keep it for clarity.↩︎ In case it’s not clear, I’m using the fact that we assume a normal distribution for the random effect of occasion. A quick rule of thumb for a normal distribution is that 95% falls between \\(\\pm\\) 2 standard deviations of the mean.↩︎ This is just one issue with a fixed effects approach. You would have to estimate 400 parameters, but without anything (inherently) to guard against overfitting. The so-called fixed effects model from the econometrics perspective gets around this by demeaning variables that vary within groups, i.e. subtracting the per group mean. This is also equivalent to a model adding a dummy variable for the groups, though it’s a computationally more viable model to fit, as one no longer includes the grouping variable in the model (not really a concern with data FE models are actually applied to and it being what year it is). But while it controls for group level effects, we still cannot estimate them. Traditional approaches to fixed effects models also do not have any way to incorporate group-specific slopes, except perhaps by way of an interaction of a covariate with the cluster, which brings you back to square one of having to estimate a lot of parameters. For more about FE models and their issues, see my document on clustered data approaches, and Bell et al. (2016). Fixed and Random effects: making an informed choice.↩︎ This phenomenon is also sometimes referred to as partial pooling. This idea of pooling is as in ‘pooling resources’ or borrowing strength. You have complete pooling, which would be the standard regression model case of ignoring the clusters, i.e. all cluster effects are assumed to be the same. With no pooling, we assumes the clusters have nothing in common, i.e. the separate regressions approach. Partial pooling is seen in the mixed model scenario, where the similarity among the clusters is estimated in some fashion, and data for all observations informs the estimates for each cluster. I’ve never really liked the ‘pooling’ terminology, as regularization/shrinkage is a more broad concept that applies beyond mixed models, and I’d prefer to stick to that. In any case, if interested in more, see practically anything Andrew Gelman has written on it, and the pool-no-pool document here.↩︎ I personally have not come across a situation where I’d do this in practice. Even if the simpler model with no correlation was a slightly better fit, there isn’t much to be gained by it.↩︎ "],["extensions.html", "Common Extensions Additional Grouping Structure Residual Structure Generalized Linear Mixed Models Exercises for Extensions", " Common Extensions Additional Grouping Structure Cross-classified models Oftentimes there will be additional sources of variance beyond one grouping factor. Consider as an example, a visual perception experiment where there are multiple trials for each individual, along with specific images displayed. Such data might look like this. In these situations, we have observations clustered within both person and image, but person and image are not nested within one another- all participants see all 10 images. Such a situation is typically referred to as one in which there are crossed random effects, which just means non-nested. In the situations we’ll look at next, we will have multiple sources variances to consider. Example: Student achievement For our own demonstration we’ll look at achievement scores for students. The sources of dependency are due to students having gone to the same primary or secondary schools. However, in this example, going to a primary school doesn’t necessarily mean you’ll go to a specific secondary school. Note also that there are no repeated measures, we see each student only once. Here’s a quick look a the data, and for more detail, check the appendix. load(&#39;data/pupils.RData&#39;) For our mixed model we’ll look at the effects for sex and socioeconomic status, ses, a six level variable from low to high, on scholastic achievement. The range of achievement scores is roughly 4 to 10, with mean of 6.3 and standard deviation 0.9. We’ll take into account the clustering at primary school and secondary school. To incorporate the additional structure in lme4 syntax is very easy, we just do as we did before, though now for both grouping factors17. pupils_crossed = lmer( achievement ~ sex + ses + (1|primary_school_id) + (1|secondary_school_id), data = pupils ) ## ## summary(pupils_crossed, correlation = FALSE) term value se lower_2.5 upper_97.5 Intercept 5.924 0.123 5.684 6.164 sexfemale 0.261 0.046 0.171 0.350 ses2 0.132 0.118 -0.098 0.362 ses3 0.098 0.110 -0.118 0.314 ses4 0.298 0.105 0.093 0.503 ses5 0.354 0.101 0.156 0.551 seshighest 0.616 0.110 0.401 0.832 The fixed effects tell us there is a positive effect of being female on achievement, and in general, relative to lowest SES category, being in the upper categories of SES also has a positive effect. group effect variance sd var_prop primary_school_id Intercept 0.17 0.42 0.24 secondary_school_id Intercept 0.07 0.26 0.09 Residual 0.47 0.69 0.66 When we look at the variance components we see that primary and secondary school contributes about 34% of the total variance. Most of the variance attributable to school comes from the primary school. If we inspect the random effects, we can see that we now have two sets of effects- 50 for the primary schools, and 30 for the secondary. Both would be incorporated into any pupil-specific prediction. glimpse(ranef(pupils_crossed)) List of 2 $ primary_school_id :&#39;data.frame&#39;: 50 obs. of 1 variable: ..$ (Intercept): num [1:50] -0.327 0.183 0.52 0.474 0.253 ... ..- attr(*, &quot;postVar&quot;)= num [1, 1, 1:50] 0.0247 0.0225 0.0244 0.0215 0.0285 ... $ secondary_school_id:&#39;data.frame&#39;: 30 obs. of 1 variable: ..$ (Intercept): num [1:30] -0.41069 0.08247 -0.00589 -0.06162 0.08481 ... ..- attr(*, &quot;postVar&quot;)= num [1, 1, 1:30] 0.0155 0.014 0.0159 0.0131 0.0142 ... - attr(*, &quot;class&quot;)= chr &quot;ranef.mer&quot; Let’s look at them visually using merTools. Note that we have the usual extensions here if desired. As an example, we could also do random slopes for student level characteristics. Hierarchical structure Now that we have looked at cross-classified models, we can proceed to examine hierarchical cluster structuring. In this situation we have clusters nested within other clusters, which may be nested within still other clusters. A typical example might be cities within counties, and counties within states. Example: Nurses and stress For our demonstration we’ll use the nurses data set. Here we are interested in the effect of a training program (treatment) on stress levels (on a scale of 1-7) of nurses. In this scenario, nurses are nested within wards, which themselves are nested within hospitals, so we will have random effects pertaining to ward (within hospital) and hospital. For more information see the appendix. load(&#39;data/nurses.RData&#39;) For the model we examine effects of the treatment as well as several other covariates, at least one at each of the nurse, ward, and hospital levels. Again, when it comes to the fixed effects portion, you can simply think about that part as you would any standard regression, we just add covariates as theory/exploration would suggest. To incorporate this type of random effects structure is not too different from the cross-classified approach, but does have a slight change to the syntax. nurses_hierarchical = lmer( stress ~ age + sex + experience + treatment + wardtype + hospsize + (1 | hospital) + (1 | hospital:ward), data = nurses ) ## ## # same thing! ## nurses_hierarchical = lmer( ## stress ~ age + sex + experience + treatment + wardtype + hospsize ## + (1|hospital/ward), ## data = nurses ## ) ## ## summary(nurses_hierarchical, correlation = F) term value se lower_2.5 upper_97.5 Intercept 5.380 0.185 5.018 5.742 age 0.022 0.002 0.018 0.026 sexFemale -0.453 0.035 -0.522 -0.385 experience -0.062 0.004 -0.070 -0.053 treatmentTraining -0.700 0.120 -0.935 -0.465 wardtypespecial care 0.051 0.120 -0.184 0.286 hospsizemedium 0.489 0.202 0.094 0.884 hospsizelarge 0.902 0.275 0.363 1.440 As far as the fixed effects go, about the only thing that doesn’t have a statistical effect is ward type18. group effect variance sd var_prop hospital:ward Intercept 0.337 0.580 0.500 hospital Intercept 0.119 0.345 0.177 Residual 0.217 0.466 0.323 Concerning the random effects, there appears to be quite a bit of variability from ward to ward especially, but also hospital. Recall that stress is a 7 point scale, so from ward to ward we can expect scores to bounce around about half a point on average, which is quite dramatic in my opinion. Again we inspect it visually. Crossed vs. nested The following shows the difference in the results from treating ward as a nested (within hospital) vs. crossed random effect. What do you notice is different? nurses_hierarchical = lmer( stress ~ age + sex + experience + treatment + wardtype + hospsize + (1|hospital) + (1|hospital:wardid), data = nurses ) nurses_crossed = lmer( stress ~ age + sex + experience + treatment + wardtype + hospsize + (1|hospital) + (1|wardid), data = nurses ) group effect variance sd var_prop hospital:ward Intercept 0.337 0.580 0.500 hospital Intercept 0.119 0.345 0.177 Residual 0.217 0.466 0.323 group effect variance sd var_prop wardid Intercept 0.337 0.580 0.500 hospital Intercept 0.119 0.345 0.177 Residual 0.217 0.466 0.323 Nothing? Good, you’re not crazy. Here’s a quote from the lme4 text, section 2.2.1.1, which is definitely worth your time. The blurring of mixed-effects models with the concept of multiple, hierarchical levels of variation results in an unwarranted emphasis on ‘levels’ when defining a model and leads to considerable confusion. It is perfectly legitimate to define models having random effects associated with non-nested factors. The reasons for the emphasis on defining random effects with respect to nested factors only are that such cases do occur frequently in practice, and that some of the computational methods for estimating the parameters in the models can only be easily applied to nested factors. This is not the case for the methods used in the lme4 package. Indeed there is nothing special done for models with random effects for nested factors. When random effects are associated with multiple factors, exactly the same computational methods are used whether the factors form a nested sequence or are partially crossed or are completely crossed. You might have noticed that we were using wardid rather than the ward grouping variable as in our first example. Even though every ward is unique, the ward column labels them with an arbitrary sequence starting with 1. While this might seem natural, ward 1 in hospital 1 is not the same as ward 1 in hospital 2, so it’s probably not a good idea to give them the same label. The wardid column properly distinguishes the wards with unique values (e.g. 11, 12). What would have happened had we used that variable as a crossed random effect? nurses_crossed_bad_data = lmer( stress ~ age + sex + experience + treatment + wardtype + hospsize + (1|hospital) + (1|ward), data = nurses ) group effect variance sd var_prop hospital Intercept 0.196 0.442 0.297 ward Intercept 0.000 0.000 0.000 Residual 0.463 0.681 0.703 This is certainly not the result we want. The variance in ward is already captured by treatment and type. However, as demonstrated, this can be avoided with the proper syntax, or proper labeling in the data to allow unique clusters to have unique identifiers. See this discussion also, as well as this from the FAQ from one of the lme4 developers. Josh Errickson at CSCAR also has a nice write-up with visual depiction of the underlying matrices of interest, which served as inspiration for some of the visualization in the next section. So there you have it. When it comes to lme4, or mixed models more generally, crossed vs. nested is simply a state of mind (data)19. Residual Structure Sometimes we will want to obtain more specific estimates regarding the residual covariance/correlation structure. This is especially the case in the longitudinal setting, where we think that observations closer in time would be more strongly correlated than those further apart, or that the variance changes over time. What does this model look like? Let’s begin by thinking about the covariance/correlation matrix for the entire set of observations for our target variable, and how we want to represent the dependency in those observations. I’ll show a visualization of the first 5 people from our GPA data and modeling situation. Recall that each person has 6 observations. This display regards the results from our random intercepts (only) model for GPA. Each block represents the covariance matrix pertaining to observations within an individual. Within the person there are variances on the diagonal and covariances on the off-diagonal. When considering the whole data, we can see that observations from one person have no covariance with another person (gray). Furthermore, the covariance within a person is a constant value, the variance is also a constant value. Where did those values come from? group effect variance sd var_prop student Intercept 0.064 0.252 0.523 Residual 0.058 0.241 0.477 Remember that there are two sources of variance in this model, the residual observation level variance, and that pertaining to person. Combined they provide the total residual variance that we aren’t already capturing with our covariates. In this case, it’s about 0.12, the value displayed on our diagonal. The off-diagonal is the variance attributable to student, which we alternately interpreted as an intraclass correlation (dividing by the total variance converts it to the correlation metric). More generically, and referring to previous notation for our estimated variances, we can see the covariance matrix (for a cluster) as follows. \\[\\Sigma = \\left[ \\begin{array}{ccc} \\color{orange}{\\sigma^2 + \\tau^2} &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 \\\\ \\tau^2 &amp; \\color{orange}{\\sigma^2 + \\tau^2} &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 \\\\ \\tau^2 &amp; \\tau^2 &amp; \\color{orange}{\\sigma^2 + \\tau^2} &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 \\\\ \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\color{orange}{\\sigma^2 + \\tau^2} &amp; \\tau^2 &amp; \\tau^2\\\\ \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\color{orange}{\\sigma^2 + \\tau^2} &amp; \\tau^2 \\\\ \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\color{orange}{\\sigma^2 + \\tau^2} \\\\ \\end{array}\\right]\\] This represents a covariance structure of compound symmetry. It is the default in most mixed model settings, and the same as what is shown visually above. Now let’s start to think about other types of covariance structures. Consider the following model for an individual and just three time points to keep things simpler to show. \\[\\boldsymbol{y} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\] So we have three observations of \\(y\\) that are multivariate normally distributed. The mean \\(\\mu\\) is a function of covariates just like in standard regression. \\[\\mu = b_0 + b_1\\cdot \\mathrm{time} + b_2\\cdot x_1 ...\\] However, instead of just plopping an \\(\\epsilon\\) at the end, we want to go further in defining the entire residual variance/covariance structure for all three time points. In the simplest setting of a standard linear regression model, we have constant variance and no covariance. \\[\\Sigma = \\left[ \\begin{array}{ccc} \\sigma^2 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma^2 \\\\ \\end{array}\\right]\\] Next, we can relax the assumption of equal variances, and estimate each separately. In this case of heterogeneous variances, we might see more or less variance over time, for example. \\[\\Sigma = \\left[ \\begin{array}{ccc} \\sigma_1^2 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_2^2 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_3^2 \\\\ \\end{array}\\right]\\] Now let’s say we actually want to get at the underlying covariance/correlation. I’ll switch to the correlation representation, but you can still think of the variances as constant or separately estimated. So now we have something like this, where \\(\\rho\\) represents the residual correlation among observations. \\[\\Sigma = \\sigma^2 \\left[ \\begin{array}{ccc} 1 &amp; \\rho_1 &amp; \\rho_2 \\\\ \\rho_1 &amp; 1 &amp; \\rho_3 \\\\ \\rho_2 &amp; \\rho_3 &amp; 1 \\\\ \\end{array}\\right]\\] In this case we’d estimate a different correlation for all time point pairs (with constant variance). This is typically described as an unstructured, or simply ‘symmetric,’ correlation structure. If you are familiar with repeated measures ANOVA, which is a special case of a mixed model, you may recall that the usual assumption is a sphericity, a relaxed form of compound symmetry, where all the correlations have the same value, i.e. \\(\\rho_1=\\rho_2=\\rho_3\\), and all variances are equal. Another very commonly used correlation structure (for time-based settings) is an autocorrelation structure, of lag order one, for the residuals. What this means is that we assume the residuals at one time point apart correlate with some value \\(\\rho\\), observations at two time points apart correlate \\(\\rho^2\\), and so on. As such we only need to estimate \\(\\rho\\), while the rest are then automatically determined. Here’s what it’d look like for four time points. \\[\\Sigma = \\sigma^2 \\left[ \\begin{array}{cccc} 1 &amp; \\rho &amp; \\rho^2 &amp; \\rho^3 \\\\ \\rho &amp; 1 &amp; \\rho &amp; \\rho^2 \\\\ \\rho^2 &amp; \\rho &amp; 1 &amp; \\rho \\\\ \\rho^3 &amp; \\rho^2 &amp; \\rho &amp; 1 \\\\ \\end{array}\\right]\\] If \\(\\rho\\) was estimated to be .5, it would look like the following. \\[\\Sigma = \\sigma^2 \\left[ \\begin{array}{cccc} 1 &amp; .5 &amp; .25 &amp; .06 \\\\ .5 &amp; 1 &amp; .5 &amp; .25 \\\\ .25 &amp; .5 &amp; 1 &amp; .5 \\\\ .06 &amp; .25 &amp; .5 &amp; 1 \\\\ \\end{array}\\right]\\] Again, the main point is that points further apart in time are assumed to have less correlation. Know that there are many patterns and possibilities to potentially consider, and that they are not limited to the repeated measures scenario. For example, the correlation could represent spatial structure, where units closer together geographically would be more correlated. And as noted, we could also have variances that are different at each time point20. We’ll start with that for the next example. Heterogeneous variance Unfortunately, lme4 does not provide the ability to model the residual covariance structure, at least not in a straightforward fashion, though many other mixed model packages do21. In fact, two packages that come with the basic R installation do so, mgcv and nlme. We’ll demonstrate with the latter. The nlme package will have a different random effect specification, though not too different. In addition, to estimate heterogeneous variances, we’ll need to use an additional weights argument. The following will allow each time point of occasion to have a unique estimate. library(nlme) heterovar_res = lme( gpa ~ occasion, data = gpa, random = ~ 1 | student, weights = varIdent(form = ~ 1 | occasion) ) ## summary(heterovar_res) term value se z p_value lower_2.5 upper_97.5 Intercept 2.599 0.026 99.002 0 2.547 2.650 occasion 0.106 0.004 26.317 0 0.098 0.114 group effect variance sd var_prop student Intercept 0.094 0.306 0.404 Residual 0.138 0.372 0.596 At this point we’re getting the same stuff we’re used to. Now the not-so fun part. For the values we’re interested in for this example, i.e. the variances at each occasion, nlme does not make it easy on someone to understand initially, as the output regards the way things are for estimation, not for what one would usually have to report. The variances are scaled relative to the first variance estimate, which is actually the reported residual variance in the random effects part. Additionally the values are also on the standard deviation rather than variance scale. From the default output display, we can see that variance decreases over time in this case, but the actual values are not provided. summary(heterovar_res$modelStruct) Random effects: Formula: ~1 | student (Intercept) Residual StdDev: 0.8232544 1 Variance function: Structure: Different standard deviations per stratum Formula: ~1 | occasion Parameter estimates: 0 1 2 3 4 5 1.0000000 0.8261186 0.6272415 0.4311126 0.3484013 0.4324628 Relative values are fine, I guess, but what we’d want are the actual estimates. Here’s how you can get them using the residual standard deviation to scale those values, then square them to get on the variance scale. (c(1.0000000, coef(heterovar_res$modelStruct$varStruct, unconstrained=F))*heterovar_res$sigma)^2 1 2 3 4 5 0.13815037 0.09428374 0.05435276 0.02567636 0.01676917 0.02583744 Yeah. You’ll have to look this up every time you want to do it, or just make your own function that takes the model input. Here is how my function would extract the information. mixedup::extract_het_var(heterovar_res, scale = &#39;var&#39;) X0 X1 X2 X3 X4 X5 1 0.138 0.094 0.054 0.026 0.017 0.026 A newer alternative to keep in mind is glmmTMB. It would allow one to stay more in the lme4 style and output. library(glmmTMB) heterovar_res2 = glmmTMB( gpa ~ occasion + (1|student) + diag(0 + occas |student), data = gpa ) summary(heterovar_res2) Family: gaussian ( identity ) Formula: gpa ~ occasion + (1 | student) + diag(0 + occas | student) Data: gpa AIC BIC logLik deviance df.resid 261.1 312.0 -120.5 241.1 1190 Random effects: Conditional model: Groups Name Variance Std.Dev. Corr student (Intercept) 0.093123 0.30516 student.1 occasyear 1 semester 1 0.129833 0.36032 occasyear 1 semester 2 0.086087 0.29341 0.00 occasyear 2 semester 1 0.046240 0.21503 0.00 0.00 occasyear 2 semester 2 0.017615 0.13272 0.00 0.00 0.00 occasyear 3 semester 1 0.008709 0.09332 0.00 0.00 0.00 0.00 occasyear 3 semester 2 0.017730 0.13316 0.00 0.00 0.00 0.00 0.00 Residual 0.008065 0.08980 Number of obs: 1200, groups: student, 200 Dispersion estimate for gaussian family (sigma^2): 0.00806 Conditional model: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 2.598788 0.026201 99.19 &lt;2e-16 *** occasion 0.106141 0.004034 26.31 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that the variances displayed for each time point are not conflated with the residual variance. To compare with nlme, just add the residual variance to those estimates. As with every mixed model package (apparently), it still takes a bit to get the variances in a usable form from the VarCorr object. The following shows how though, and compares to the nlme result. vc_glmmtmb = VarCorr(heterovar_res2) vc_glmmtmb = attr(vc_glmmtmb$cond$student.1, &#39;stddev&#39;)^2 + sigma(heterovar_res2)^2 Here is the output from my function: mixedup::extract_het_var(heterovar_res2, scale = &#39;var&#39;) group occasyear.1.semester.1 occasyear.1.semester.2 occasyear.2.semester.1 occasyear.2.semester.2 occasyear.3.semester.1 occasyear.3.semester.2 1 student.1 0.138 0.094 0.054 0.026 0.017 0.026 In any case, putting these together shows we get the same result. year 1 sem 1 year 1 sem 2 year 2 sem 1 year 2 sem 2 year 3 sem 1 year 3 sem 2 glmmTMB 0.138 0.094 0.054 0.026 0.017 0.026 nlme 0.138 0.094 0.054 0.026 0.017 0.026 Autocorrelation The following example shows the same basic model, but with the autocorrelation structure we described previously. In nlme we use the built-in corAR1 function and correlation argument similar to how we did with the weights argument. library(nlme) corr_res = lme( gpa ~ occasion, data = gpa, random = ~ 1 | student, correlation = corAR1(form = ~ occasion) ) ## summary(corr_res) term value se z p_value lower_2.5 upper_97.5 Intercept 2.597 0.023 113.146 0 2.552 2.642 occasion 0.107 0.005 20.297 0 0.097 0.118 group effect variance sd var_prop student Intercept 0.046 0.215 0.381 Residual 0.075 0.273 0.619 Notice first that the fixed effect for occasion is the same as before. The variance estimates have changed slightly along with the variances of the fixed effects (i.e. the standard errors). The main thing is that we have a new parameter called Phi in the nlme output that represents our autocorrelation, Phi, with value of 0.418. This suggests at least some correlation exists among the residuals for observations next to each other in time, though it diminishes quickly as observations grow further apart. Note that glmmTMB will analyze such structure as well. Note that we need the factor form for occasion for this specification, and also note how it is part of model formula, like another random effect. More on this in the supplemental section. corr_res_tmb = glmmTMB( gpa ~ occasion + ar1(0 + occas | student) + (1 | student), data = gpa ) Generalized Linear Mixed Models Just as generalized linear models extend the standard linear model, we can generalize (linear) mixed models to generalized linear mixed models. Furthermore, there is nothing restricting us to only the exponential family, as other packages would potentially allow for many other response distributions. For this example we’ll do a logistic regression in the mixed model setting. In this case, we’ll use the speed dating data set. In the speed dating events, the experiment randomly assigned each participant to ten short dates (four minutes) with other participants. For each date, each person rated six attributes (attractive, sincere, intelligent, fun, ambitious, shared interests) of the other person on a 10-point scale and wrote down whether he or she would like to see the other person again. Our target variable is whether the participant would be willing to date the person again (decision). To keep things simple, the predictors will be limited to the sex of the participant (sex), whether the partner was of the same race (samerace), and three of the attribute ratings the participant gave of their partner- attractiveness (attractive), sincerity (sincere), and intelligence (intelligent). The latter have been scaled to have zero mean and standard deviation of .5, which puts them on a more even footing with the binary covariates (_sc)22. load(&#39;data/speed_dating.RData&#39;) sd_model = glmer( decision ~ sex + samerace + attractive_sc + sincere_sc + intelligent_sc + (1 | iid), data = speed_dating, family = binomial ) summary(sd_model, correlation = FALSE) term value se z p_value lower_2.5 upper_97.5 Intercept -0.743 0.121 -6.130 0.00 -0.981 -0.506 sexMale 0.156 0.164 0.954 0.34 -0.165 0.478 sameraceYes 0.314 0.075 4.192 0.00 0.167 0.460 attractive_sc 1.957 0.058 33.560 0.00 1.842 2.071 sincere_sc 0.311 0.054 5.747 0.00 0.205 0.417 intelligent_sc 0.444 0.054 8.232 0.00 0.338 0.550 The fixed effects results are as expected for the attributes, with attractiveness being a very strong effect in particular. In addition, having a partner of the same race had a positive effect, while sex of the participant was statistically negligible. You are free to exponentiate the coefficients to get the odds ratios if desired, just as you would with standard logistic regression. group effect variance sd iid Intercept 2.708 1.645 For the variance components, notice that there is no residual variance. This is because we are not modeling with the normal distribution for the response, thus there is no \\(\\sigma\\) to estimate. However, the result suggests that there is quite a bit of variability from person to person. Exercises for Extensions Sociometric data In the following data, kids are put into different groups and rate each other in terms of how much they would like to share some activity with the others. We have identifying variables for the person doing the rating (sender), the person being rated (receiver), what group they are in, as well as age and sex for both sender and receiver, as well as group size. To run a mixed model, we will have three sources of structure to consider: senders (within group) receivers (within group) group First, load the sociometric data. load(&#39;data/sociometric.RData&#39;) To run the model, we will proceed with the following modeling steps. For each, make sure you are creating a separate model object for each model run. Model 1: No covariates, only sender and receiver random effects. Note that even though we don’t add group yet, still use the nesting approach to specify the effects (e.g. 1|group:receiver) Model 2: No covariates, add group random effect Model 3: Add all covariates: agesend/rec, sexsend/rec, and grsize (group size) Model 4: In order to examine sex matching effects, add an interaction of the sex variables to the model sexsend:sexrec. Compare models with AIC (see the note about model comparison), e.g. AIC(model1). A lower value would indicate the model is preferred. Patents Do a Poisson mixed effect model using the patent data. Model the number of citations (ncit) based on whether there was opposition (opposition) and if it was for the biotechnology/pharmaceutical industry (biopharm). Use year as a random effect to account for unspecified economic conditions. load(&#39;data/patents.RData&#39;) Interestingly, one can model overdispersion in a Poisson model by specifying an random intercept for each observation (subject in the data). In other words, no specific clustering or grouped structure is necessary, but we can use the random effect approach to get at the extra variance. I don’t show the formal model here as we did before, but this is why depicting mixed models solely as ‘multilevel’ becomes a bit problematic in my opinion. In the standard mixed model notation it’s straightforward though, you just add an additional random effect term, just as we do in the actual model syntax.↩︎ Setting aside our discussion to take a turn regarding regression modeling more generally, this is a good example of ‘surprising’ effects not being so surprising when you consider them more closely. Take a look at the effect of experience. More experience means less stress, this is probably not surprising. Now look at the age effect. It’s positive! But wouldn’t older nurses have more experience? What’s going on here? When interpreting experience, it is with age held constant, thus more experience helps with lowering stress no matter what your age. With age, we’re holding experience constant. If experience doesn’t matter, being older is affiliated with more stress, which might be expected given the type of very busy and high pressure work often being done (the mean age is 43). A good way to better understand this specifically is to look at predicted values when age is young, middle, and older vs. experience levels at low, middle, and high experience, possibly explicitly including the interaction of the two in the model. Also note that if you take experience out of the model, the age effect is negative, which is expected, as it captures experience also.↩︎ Just a reminder, it does matter if you label your data in a less than optimal fashion. For example, if in the nesting situation you start your id variable at 1 for each nested group, then you have to use the nested notation in lme4, otherwise, e.g. it won’t know that id = 1 in group 1 is different from id 1 in group 2. In our hospital example, this would be akin to using ward instead of wardid as we did. Again though, this wouldn’t be an issue if one practices good data habits. Note also the : syntax. In other modeling contexts in R this denotes an interaction, and that is no different here. In some contexts, typically due to experimental designs, one would want to explore random effects of the sort 1|A, 1|B and 1|A:B. However, this is relatively rare.↩︎ One reason to do so would be that you expect variability to decrease over time, e.g. due to experience. You might also allow that variance to be different due to some other grouping factor entirely (e.g. due to treatment group membership).↩︎ This feature request has been made by its users for over a decade at this point- it’s not gonna happen. The issue is that the way lmer works by default employs a method that won’t allow it (this is why it is faster and better performing than other packages). Unfortunately the common response to this issue is ‘use nlme.’ However many other packages work with lme4 rather than nlme, and if you aren’t going to use lme4 for mixed models you might as well go Bayesian with rstanarm or brms instead of nlme. I would even prefer mgcv to nlme (though it can use nlme under the hood) because of the other capabilities it provides, and the objects created are easier to work with in my opinion.↩︎ Note that for a balanced binary variable, the mean p=.5 and standard deviation is sqrt(p*(1-p)) = .5↩︎ "],["issues.html", "Issues Variance Accounted For Common Alternatives to Mixed Models Sample Sizes Model Comparison Convergence", " Issues This section discusses common issues, conundrums, and other things that might come up when implementing mixed models. Variance Accounted For People really love the R-squared value that comes from standard regression. Never mind that it is inherently biased, nor does it matter that there is no way to state what would be a ‘good’ result for a given data situation, nor that many actually don’t know how to interpret it, nor does it even matter that many have no qualms about dropping it from a report at the first sign of trouble. Suffice it to say that when there are multiple sources of ‘variance,’ talking about variance accounted for is not straightforward. Still, many have tried. You might look at the r2glmm package and the references noted in the description, or the r2 function in performance. See also the GLMM FAQ. I would suggest not even bothering beyond the standard linear mixed model. This also regards the intraclass correlation, which has little meaning outside of a linear mixed model with no random slopes. Again, just because you can calculate something that looks like it, doesn’t mean that it actually means what you want it to mean. Common Alternatives to Mixed Models I have a document that goes into more detail about many approaches to dealing with clustered data, but we can briefly talk about some here. Common alternatives used in clustered data situations include: Fixed effects models (also panel linear models with fixed, as opposed to random, effects) Using cluster-robust standard errors Generalized estimating equations (GEE) The first two are commonly used by those trained with an econometrics perspective, while you might see GEE more with those of a biostatistics or other perspective. GEE are in fact a generalization of the cluster-robust approach, and extend generalized least squares (GLS) to nonlinear/GLM settings. GEE approaches allow one to take into account the dependency in the data, but not actually estimate what might be very interesting, i.e. the random effects and associated variance. There are also fewer tools for GEE in more complicated covariance structures beyond a single clustering variable. The nature of fixed effects models allow you to control for, but not actually explore, cluster level effects. This makes them a non-starter for many investigations, as those are typically of prime theoretical interest. In addition, the main concern econometricians have that leads them to prefer such models is easily accommodated in standard mixed models, so there is very little reason to employ them, as they are special cases of the more flexible mixed model. Growth curve models With longitudinal data, growth curve models are a latent variable approach that is commonly used in these situations. With appropriate setup, they will duplicate the results of a mixed model. In my opinion, there are few reasons to use a growth curve approach over a mixed model, and many reasons not to, not least of which is that effects which would be simple to interpret in the mixed model approach are now a source of confusion to applied researchers in the growth curve model, even though it’s the same thing. Furthermore, indirect effects, growth mixture models and other extensions common in the latent variable approach are more easily implemented in the mixed model approach. In short, only the most complicated models would perhaps require using structural equation modeling, but as such, would also bring with them many other issues. See more here and here. The supplemental section has a full demonstration of how to equate these models. Sample Sizes Small number of clusters Think about how many values of some variable you’d need before you felt comfortable with statistics based on it, especially standard deviation/variance. That’s at play with mixed models, in the sense you’d like to have enough groups to adequately assess the variance components. Mixed models will run with very small numbers, though the estimates will generally be biased. I have a demo here if interested. One way to deal with this is to move to the Bayesian context, which can be used to induce some regularization in parameter estimates, and better deal with possible variance estimates that are near zero. This also speaks to the issue some will have regarding whether they should treat something as a fixed vs. random effect. Historical definitions would unnecessarily restrict usage of random effects approaches. For example, random effects were defined to be a (random) sample from some population. If this were the case, some might take issue when your levels do not deal with a sample, but the whole population, as in the case where your cluster is state and you have all 50 states. This doesn’t matter. If you have enough levels to consider a mixed model approach, feel free to do so. Small number of observations within clusters Mixed models work even with no more than two in each cluster and some singletons. Even in the simple case of pre-post design, mixed models are entirely applicable, though limited (e.g. you can’t have random slopes with just pre-post). So whenever you have clustering of some kind, you should consider mixed models. Balanced/Missing values We’ve primarily been looking at balanced data, where each cluster has the same number of observations within them. There is no requirement for this, and in many cases we wouldn’t even expect it, e.g. people within geographical units. However, if data is only missing on the outcome, or a mix of variables, we essentially have the same issue as with typical data situations, and will have the same considerations for dealing with missingness. If you don’t lose much data, the practical gain by ignoring missingness generally outweighs the complexities that can come with, for example, multiple imputation23, even in the best of settings. By default, mixed models assume missing at random (MAR). On the other hand, longitudinal data has special considerations, as there is typically increasing dropout over time. Having dealt with missingness in a variety of contexts with different approaches (FIML, MI, Bayesian), the end result is usually that you spend vast amounts more time dealing with the missing data than you do understanding your primary models of interest, and yet don’t feel any better about the results. Unless the missingness would make you lose a large chunk of the data, and/or you actually know something about the underlying mechanism attributing to missing data, it’s probably best just to leave that to the limitations section of your report24. If you do deal with it, under less than ideal circumstances, I’d perhaps suggest an approach that is essentially a one-off imputation (e.g. with missForest) to be compared to the data that ignores the missingness, but still allows you to do everything you want. While you may not incorporate all sources of uncertainty in doing so, it seems to me a viable compromise. Big data Mixed model packages are often not so great with largish data, e.g. thousands, coupled with anything beyond random intercepts. However, I’ve used lme4 with millions and simple structure, and 100s of thousands with complicated structure, and it does very well (at least for the gaussian case). For truly big data you’re not going to have a lot of options though, but you’d need a lot. I did some testing of lme4, mgcv, and glmmTMB, for up to a million cases with two random effects, and generally even then you may only have to wait seconds to a couple minutes. In applied work with Medicare data of hundreds of thousands, and two random effects with thousands of levels each, those packages were still viable. Common techniques in machine learning have no special implementation for the inclusion of something like random effects. There has been some effort with trees/forests here (mebt specifically), here, and also vcrpart). However, most approaches in the ML world will simply throw the clustering variable in along with everything else, possibly even as a lower dimensional word embedding, or have enough data to do by-cluster approaches. While this may be fine predictively, it may not be theoretically interesting to those doing mixed models. On the plus side, if you’re willing to wait, tools like the Stan family will likely do just fine with bigger data as well, eventually. So while massive data may still be problematic, you may be fine with very large data. Model Comparison Model comparison takes place in the usual way in the sense of potentially having statistical tests and information criteria. Unfortunately, the typical likelihood ratio tests one might use in standard settings are not so straightforward here. For example, in order to compare models with different fixed effects, at a minimum you’d have to change the default estimation from REML to ML, and the models must have the same random effects structure, for the resulting test p-value to be correct. It works the other way to compare models with different random effects structure (with fixed effects the same across models), i.e. where you’d have to use REML. # to compare fixed effects refit with ML gpa_1 = lmer(gpa ~ occasion + (1 + occasion | student), data = gpa) gpa_2 = lmer(gpa ~ occasion + sex + (1 + occasion | student), data = gpa) gpa_3 = lmer(gpa ~ occasion + (1 | student), data = gpa) gpa_4 = lmer(gpa ~ occasion + (1 + occasion | student), data = gpa) anova(gpa_1, gpa_2, refit = TRUE) anova(gpa_3, gpa_4, refit = FALSE) npar AIC BIC logLik deviance Chisq Df p-value gpa_1 6 258.234 288.775 -123.117 246.234 gpa_2 7 250.068 285.699 -118.034 236.068 10.166 1 0.001 npar AIC BIC logLik deviance Chisq Df p-value gpa_3 4 416.893 437.253 -204.446 408.893 gpa_4 6 272.957 303.497 -130.478 260.957 147.936 2 0 One can see that the estimated log likelihood is not the same for gpa_1 (ML) and gpa_4 (REML), even though they are otherwise the same model. In my opinion, model selection involves considerations of theory, parsimony, and prediction, and those tests do not. I’m not a fan of such tests even in the standard setting, and would use AIC here to potentially aid (not make) a model choice if I thought it was necessary, as I would there25. We can just use the AIC function, but see also the cAIC4 package for a conditional AIC that works specifically for merMod objects. In the following, gpa_2 has the lowest (best) AIC value, and given that both it and gpa_1 are notably superior to gpa_3, one could conclude that having the random coefficient for occasion is useful. model AIC Δ AIC gpa_2 269.785 0.000 gpa_1 272.957 3.171 gpa_3 416.893 147.108 In general though, trying to determine a ‘best’ model with one set of data is a problematic endeavor at best, and at worst, completely misguided. I think it’s very useful to build models of increasing complexity, and select one to focus on based on the available evidence to simplify exposition. Just don’t get hung up on choosing one based solely on the outcome of a single statistic. If you have a lot of data, you should consider some sort of explicit validation approach if you really want to compare competing models, but that is not without complication given the dependency in the data. If your goal is more on the predictive side, consider model averaging rather than selecting a single model. Convergence Data is as data does. It is likely that you will eventually have issues in conducting mixed models, such as lack of convergence, estimates of zero for the random effects variance, warnings about scaling variables etc. These are not easy models to estimate (at least outside of the Bayesian context), so don’t be surprised if all doesn’t go smoothly. I have a more detailed overview, but we can talk briefly about typical problems and solutions. A few common issues26 I see are: Lack of centering/standardizing will often result in scaling warnings for lme4. It makes sense to do it anyway, so the same goes for mixed models as any other. A very common convergence problem in my experience results from time-based variables, such as incorporating a yearly trend. At the very least, it’s useful to have a meaningful zero value, for example, by starting the time at zero, rather than say, year 2008. You may need to standardize the trend variable also to get the model to converge, but can revert back to raw form for interpretation. The default optimizer options may need to be tweaked, for example, to allow for more iterations. If possible, you may need to change to a different optimizer. Zero estimates for the random effect variance, or \\(\\pm 1\\) estimates for correlation of intercepts and slopes, often can be attributed to not having enough data, not having enough clusters, or an overlooked data issue, along with possible scaling. You won’t have this issue in the Bayesian context, but in others, you may have to deal with the dependency in some other fashion (e.g. cluster-robust standard errors/GEE). Any complicated GLMM or similar model is likely to have problems, so be prepared. If you want to go beyond GLM, you’ll have fewer tools and likely more issues. There are packages like ordinal, mgcv, glmmTMB, and others that can potentially handle alternate distributions and other complexities, however I think one might be better off with a Bayesian approach (e.g. brms/rstan). In practice, I’ve found others to be prohibitively slow, unable to converge, or too limited in post-estimation options. If you’re using lme4 you have a couple resources and tools at your disposal. Start with ?convergence. This comes up so often that there is a help file just for convergence issues. Use the allFit function. If the results are very similar across fits you can feel better about them. Consult the troubleshooting section of the FAQ. The main point is that you’ll need to acknowledge the warnings and messages that the packages provide, and be prepared to take necessary steps to deal with these issues when they arise. Multiple imputation is straightforward only in theory. In practice it becomes a major pain to go very far beyond getting the parameter estimates for simple models. Full information maximum likelihood (FIML) is little implemented outside of SEM software/packages, and more problematic in its assumptions.↩︎ Just note that in some disciplines, reviewers, who will rarely do this themselves for the same reasons, nevertheless will make a big deal about the missing data because it’s an easy point for them to make. This is similar to econometrically trained reviewers who shout ‘endogeneity!’ at every turn, but won’t bother to tell you where to get the instrument in the first place, admit that IV analysis is problematic in its own right, or what specifically the approach should be in complex model settings such as mixed models with nonlinear, spatial effects, multiple levels of clustering etc. I’ve even seen a reviewer say that one ‘should conduct a test for whether data is missing at random vs. not, then proceed accordingly.’ That was the entirety of their suggestion. Aside from the fact that there technically is no such test, because it would require the very data one doesn’t have, declaring a type of missingness doesn’t tell you which of dozens of approaches one should take.↩︎ If you really go the statistical test route, see the lmertest package for additional functionality. Note also that AIC does not come with a free lunch, and as mentioned, see the cAIC4 package and references therein.↩︎ The Richly Parameterized Linear Models text discusses convergence and other issues probably more than any other text I’ve come across. Most don’t treat the subject at all.↩︎ "],["bayesian.html", "Bayesian Approaches Priors Demonstration Example Models Beyond the Model", " Bayesian Approaches With mixed models we’ve been thinking of coefficients as coming from a distribution (normal). While we have what we are calling ‘fixed’ effects, the distinguishing feature of the mixed model is the addition of this random component. Now consider a standard regression model, i.e. no clustering. You can actually do the same thing, i.e. assume the coefficients are not fixed, but random. In this sense, the goal is to understand that distribution, and focus on it, rather than just the summary of it, e.g. the mean. However, the mean (or other central tendency) of that distribution can be treated like you’ve been doing the fixed effects in your standard models. Thus you can use how you’ve been thinking about the random effects in mixed models as a natural segue to the Bayesian approach, where all parameters are random draws from a distribution. Using Bayesian versions of your favorite models takes no more syntactical effort than your standard models. The following is a standard linear regression and a mixed model in the brms package, but would likewise be the same for rstanarm. brms::brm(gpa ~ occasion, data = gpa) brms::brm(Reaction ~ Days + (1 + Days | Subject), data = sleepstudy) rstanarm::stan_lm(gpa ~ occasion, data = gpa) rstanarm::stan_lmer(Reaction ~ Days + (1 + Days | Subject), data = sleepstudy) So running the Bayesian models is not only as easy, the syntax is identical! Furthermore, just like mixed models allowed you to understand your data more deeply, the Bayesian models have the potential to do the same. Even the probabilities and intervals make more sense. With rstanarm and especially brms, you can do fairly complex models, taking you further than the standard mixed model packages, all without learning how to code the models explicitly in Stan, the probabilistic programming language that both are based on. However, when you get to that point, the modeling possibilities are only limited by your imagination. You will have to learn a new inferential framework, as well as some of the nuances of the Markov Chain Monte Carlo (MCMC) approach. But you may be surprised to find that the basics come more easily than you would anticipate. Using tools like brms and related make it easier than ever to dive into Bayesian data analysis, and you’ve already been in a similar mindset with mixed models, so try it out some time. I have an introduction to Baysian analysis with Stan, and a bit more on the Bayesian approach and mixed models in this document. Priors The following information about priors assumes some background knowledge of Bayesian analysis, particularly for regression models. The Stan development group offers recommendations here, so refer to it often. Note that Stan does not require conjugacy, in contrast to tools such as BUGS/JAGS. This frees one up to use other prior distributions as they see fit. Generally though, using some normal distribution for the fixed effects, and the package defaults for variance components, should suffice for the standard models we’ve been discussing. Fixed effects For fixed effect regression coefficients, normal and student t would be the most common prior distributions, but the default brms (and rstanarm) implementation does not specify any, and so defaults to a uniform/improper prior, which is a poor choice. You will want to set this for your models. Note that scaling numeric predictors benefits here just like it does with lme4, and makes specifying the prior easier as well. Variance components In Bayesian linear mixed models, the random effects are estimated parameters, just like the fixed effects (and thus are not BLUPs). The benefit to this is that getting interval estimates for them, or predictions using them, is as easy as anything else. Typically priors for variance components are half-t for the variances, as the values can only be positive, but beyond that, e.g. intercept and slope correlations, you can again just rely on the package defaults. To make this more explicit, let’s say we have a situation with random intercepts and slopes, with variances 1 and .1 respectively, with a .3 correlation. The random effects, say for 10 clusters, would come from a multivariate distribution as follows. re_cov = matrix(c(1, .3, .3, .1), ncol = 2) re_cov [,1] [,2] [1,] 1.0 0.3 [2,] 0.3 0.1 mvtnorm::rmvnorm(10, mean = c(0, 0), sigma = re_cov) [,1] [,2] [1,] -1.2221063 -0.19934740 [2,] 0.5511788 0.14630128 [3,] 1.0696280 0.51653368 [4,] -0.4822097 -0.11822515 [5,] 1.3431782 0.37536983 [6,] -0.2609433 -0.16101335 [7,] -0.8579271 -0.24116103 [8,] 0.3269719 0.05641921 [9,] -0.4339689 0.12164433 [10,] -0.7510034 -0.22716122 The priors in the model would regard the correlation matrix, and the estimated random effects would be added to the linear predictor, as we showed in the beginning. Demonstration Let’s return to our GPA model. I will add the priors for the fixed effects, and an option to speed computation by parallelizing the chains. library(brms) pr = prior(normal(0, 1), class = &#39;b&#39;) bayesian_mixed = brm( gpa ~ occasion + (1 + occasion | student), data = gpa, prior = pr, cores = 4 ) summary(bayesian_mixed) Family: gaussian Links: mu = identity; sigma = identity Formula: gpa ~ occasion + (1 + occasion | student) Data: gpa (Number of observations: 1200) Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; total post-warmup samples = 4000 Group-Level Effects: ~student (Number of levels: 200) Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sd(Intercept) 0.21 0.02 0.18 0.25 1.00 2289 3009 sd(occasion) 0.07 0.01 0.06 0.08 1.00 1249 1973 cor(Intercept,occasion) -0.09 0.11 -0.30 0.13 1.00 933 1360 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Intercept 2.60 0.02 2.56 2.63 1.00 2672 2743 occasion 0.11 0.01 0.09 0.12 1.00 2565 2969 Family Specific Parameters: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sigma 0.21 0.01 0.20 0.22 1.00 2634 2998 Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). Compare to our previous results. summary(gpa_mixed, cor= F) Linear mixed model fit by REML [&#39;lmerMod&#39;] Formula: gpa ~ occasion + (1 + occasion | student) Data: gpa REML criterion at convergence: 261 Scaled residuals: Min 1Q Median 3Q Max -3.2695 -0.5377 -0.0128 0.5326 3.1939 Random effects: Groups Name Variance Std.Dev. Corr student (Intercept) 0.045193 0.21259 occasion 0.004504 0.06711 -0.10 Residual 0.042388 0.20588 Number of obs: 1200, groups: student, 200 Fixed effects: Estimate Std. Error t value (Intercept) 2.599214 0.018357 141.59 occasion 0.106314 0.005885 18.07 Aside from additional diagnostic information, the Bayesian results are essentially the same, but now we can continue to explore the model. The brms package tries to use the same function names as lme4 where possible, so ranef, fixef, VarCorr, etc. are still in play. However, you can still use my functions for standard models, which will return tidy data frames. # examine random effects with the usual functions, not too tidy # ranef(bayesian_mixed) mixedup::extract_random_effects(bayesian_mixed) # A tibble: 400 x 7 group_var effect group value se lower_2.5 upper_97.5 &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 student Intercept 1 -0.2 0.115 -0.424 0.026 2 student Intercept 2 -0.21 0.114 -0.437 0.012 3 student Intercept 3 -0.007 0.112 -0.227 0.214 4 student Intercept 4 -0.094 0.113 -0.312 0.132 5 student Intercept 5 0.086 0.113 -0.129 0.31 6 student Intercept 6 -0.208 0.111 -0.427 0.01 7 student Intercept 7 -0.152 0.112 -0.369 0.066 8 student Intercept 8 0.149 0.115 -0.073 0.37 9 student Intercept 9 0.038 0.114 -0.188 0.263 10 student Intercept 10 0.097 0.114 -0.123 0.315 # … with 390 more rows However, we also have some nice plotting functions. Here I plot the occasion effect, as well as the estimated predictions from the model vs. our observed GPA values. conditional_effects(bayesian_mixed) pp_check(bayesian_mixed) There is a lot more modeling we can do here as we’ll see shortly, but it’s important to know you can do the basics easily. Example Models In the following I attempt to show a wide variety of (mixed) models one could do with brms. Typically is shown the modeling function brm, where the syntax is lme4-like. Elsewhere I use the specific bf function, which allows one to build a potentially complicated formula as a separate object to be used in the eventual modeling function. For example. brm(y ~ x, data = mydata, family = gaussian) f = bf(y ~ x) brm(f, ...) Standard mixed models Random intercept. brm(y ~ x + z + (1 | g)) Random intercept and random coefficient for x. brm(y ~ x + z + (1 + x | g)) Multiple grouping structure/random effects. brm(y ~ x + z + (1 | g1) + (1 | g2)) brm(y ~ x + z + (1 | g1 + g2)) # same thing brm(y ~ x + z + (1 | g1) + (1 | g1:g2)) Other distributional families Multiple types of ordinal models including ‘generalized’ or ‘varying coefficients’ models that include category specific effects. brm(y ~ x + z + (1 | g), family = cumulative) # x has category specific effects brm(y ~ cs(x) + z + (1 | g), family = acat) # for ordered predictors, see the mo() function. Multinomial. Uses the categorical distribution for a standard multicategory target. brm(y ~ x + z + (1 | g), family = categorical) Zero-inflated and hurdle models. brm( y ~ x + z + (1 | g), zi ~ x + z, family = zero_inflated_negbinomial(link = &#39;log&#39;) ) brm(y ~ x + z + (1 | g), family = hurdle_lognormal) Many more including weibull, student t, beta, skew normal, von mises, and more. Residual structure and heterogeous variances Various functions exist to model temporal, spatial and other residual structure. brm(y ~ time + (1 + time | g) + ar(time, person, p = 2)) We can model the variance just like anything else. brm(y ~ x + z + (1 | g), sigma ~ x + (1 | g)) We can allow the variance components themselves to vary by some group. In the following we’d have separate variances for male and female. brm(count ~ Sex + (1|gr(g, by = Sex))) Multi-membership models, where individuals may belong to more than one cluster can also be used. In the following, g1 and g2 are identical conceptually, but may take on different values for some observations. brm(y ~ 1 + (1 | mm(g1, g2))) Multivariate mixed models For multiple outcomes we can allow random effects to be correlated. In the following, ID1 is an arbitrary label that serves to connect/correlate the modeled random effects across multiple outcomes y1 and y2. In SEM literature this would be akin to a parallel process model if we add a random slope for a time indicator variable. bf( y1 ~ x + z + (1 | ID1 |g), y2 ~ x + z + (1 | ID1 |g) ) Such an approach would also make sense for zero-inflated models for example, where we want random effects for the same clustering to be correlated for both the count model and zero-inflated model. bf(y ~ x * z + (1 + x | ID1 | g), zi ~ x + (1 | ID1 | g)) Additive mixed models Much of the basic functionality of mgcv is incorporated, and works with the same syntax. brm(y ~ s(x) + z + (1 | g)) Nonlinear mixed models We can model similar situations where the functional form is known, as with nlme. bf( y ~ a1 - a2^x, a1 ~ 1, a2 ~ x + (x | g), nl = TRUE ) Censored and truncated targets For censored data, just supply the censoring variables as you would typically note in a survival/event-history model. bf(y | cens(censor_variable) ~ x + z + (1 | g), family = lognormal) # frailty # see also stan_jm in the rstanarm package for joint models For truncated models, specify the lower bound, upper bound, or both. brm(count | trunc(ub = 10) ~ x * z + (1 | g), family = poisson) Measurment error There may be cases where we know one variable is assumed to be measured with error, such as the mean of several trials, or latent variables estimated by some other means. In the following, sdx is the known standard deviation for x, which may be constant or vary by observation. brm(y ~ me(x, sdx) + z + (1 | g)) Mixture models Two clusters specified by multiple families along with mixture. So I guess this is technically a mixture mixed model. brm(y ~ x + z + (1 | g), family = mixture(gaussian, gaussian)) A ‘growth mixture model.’ brm(y ~ time + z + (1 + time | g), family = mixture(gaussian, gaussian)) Missing Values We can construct the model formula for missing values as follows, including using a mixed model as the imputation model (for x). f = bf(y ~ mi(x) + z + (1 | g)) + bf(x | mi() ~ z + (1 | g)) + set_rescor(FALSE) Beyond the Model The development of Stan and packages like rstanarm and brms is rapid, and with the combined powers of those involved, there are a lot of useful tools for exploring the model results. Even if one found a specialty package for a specific type of mixed model, it is doubtful you would have as many tools for model exploration such as posterior predictive checks, marginal effects, model comparison, basic model diagnostics and more. That said, the Stan ecosystem of R packages is notable at this point, and so use what works for your situation. "],["further.html", "Going Further Other Distributions Other Contexts Nonlinear Mixed Effects Models Connections", " Going Further This section covers topics that are generally beyond the scope of what would be covered in this introductory document, but may be given their own section over time. Other Distributions As noted in the GLMM section, we are not held to use only GLM family distributions regarding the target variable. Unfortunately, the tools you have available to do so will quickly diminish. However, a couple packages could help in this regard with simpler random effects structures. For example, the mgcv and glmmTMB packages allow one access to a variety of response distributions, such as student t, negative binomial, beta, zero-inflated Poisson and more. If you’re willing to go Bayesian, you’ll have even more options with rstanarm and brms. I’ve personally had success with ordinal, beta, truncated normal and more with brms in particular. Note also that nothing says that the random effects must come from a normal distribution either. You probably are going to need some notably strong theoretical reasons for trying something else, but it does come up for some folks. You’ll almost certainly need to use a specialized approach, as most mixed model tools do not offer such functionality out of the box. Other Contexts Here is a list of some other contexts in which you can find random effects models, or extensions of mixed models into other situations. Spatial models It is often the case we want to take into account the geography of a situation. Spatial random effects allow one to do so in the continuous case, e.g. with latitude and longitude coordinates, as well as discrete, as with political district. Typical random effects approaches, e.g. with a state random effect, would not correlate state effects. One might capture geography incidentally, or via cluster level variables such as ‘region’ indicator. However, if you’re interested in a spatial random effect, use something that can account for it specifically. Survival models Random effects models in the survival context are typically referred to as frailty models. As a starting point, the survival package that comes with base R can do such models. Item response theory Item response theory models are often used with scholastic and other testing data, but far more general than that. Some IRT models can be estimated as a mixed model, or otherwise thought of as incorporating random effects. See Boeck et al. (2011) The Estimation of Item Response Models with the lmer Function from the lme4 Package in R. I also have some brief demonstration here. Multi-membership models Sometimes observations may belong to more than one cluster of some grouping variable. For example, in a longitudinal setting some individuals may move to other cities or schools, staying in one place longer than another. Depending on the specifics of the modeling setting, you may need to take a multi-membership approach to deal with this. Phylogenetic models In biology, models make take observations that are of the same species. While one can use species as an additional source of variance as we have done, the species are not independent as they may come from the same phylogenetic tree/branch. Bayesian packages are available to do such models (e.g. MCMCglmm and brms). Adjacency structures Similar to spatial and phylogenetic models, the dependency among the groups/clusters themselves can be described in terms of a markov random field/undirected graph. In simpler terms, one may think of a situation where a binary adjacency matrix would denote connections among the nodes/cluster levels. For example, the clustering may be due to individuals, which themselves might be friends with one another. One way to deal with such a situation would be similar to spatial models for discrete random units. Gaussian processes Gaussian processes are another way to handle dependency in the data, especially over time or space. Some spatial models are in fact a special case of these. One can think of gaussian processes as adding a ‘continuous category’ random effect. Consider the effect of age in many models, could that not also be a source of dependency regarding some outcomes? In Statistical Rethinking, McElreath has a nice chapter ‘Adventures in Covariance’ that gets into this a bit. Surveys &amp; Mr. P Clustering is often a result of sampling design. Often one would use a survey design approach for proper inference in such situations, and you can use mixed models with survey weights. However, multi-level regression with post-stratification, or Mr. P, is an alternative mixed model approach that can potentially lead to better results in the same setting without weighting. One might even be able to generalize from a sample of Xbox players to the national level! Post-hoc comparisons and multiple testing This is not an issue I’m personally all that concerned with, but a lot of folks seem to be. The ‘problem’ is that one has a lot of p-values for some model or across a set of models, and is worried about spurious claims of significance. If one were truly worried about it, they’d be doing different models that would incorporate some sort of regularization, rather than attempting some p-value hack afterwards. Didn’t we talk about regularization somewhere? Yep, you can use a mixed model approach instead. See Gelman for details. Growth mixture models Often people will assume latent clusters of individuals within the data, with model effects differing by these latent groups also. Sometimes called latent trajectory models, these are conceptually adding a cluster analysis to the mixed model setting. While common in structural equation modeling, packages like flexmix can keep you in the standard model setting, which might be preferable. Nonlinear Mixed Effects Models Earlier we used the nlme package. The acronym stands for nonlinear mixed effects models. In this case, we are assuming a specific functional form for a predictor. A common example is a logistic growth curve27, and one could use a function like SSlogis. In other cases we do not specify the functional form, and take a more non-parametric approach. Here’s where the powerful mgcv package comes in, and there are few if any that have its capabilities for generalized additive models combined with standard random effects approaches. Depending on the approach you take, you can even get nlme or lme4 output along with the GAM results. Highly recommended. I would also recommend brms, which has specific functionality for nonlinear models in general, including IRT, as well as additive models in the vein of mgcv, as it uses the same constructor functions that come that package. It might be your best bet whether you have a specific nonlinear functional form or not. Connections The incorporation of spatial random effects, additive models, and mixed models altogether under one modeling roof is sometimes referred to as structured additive regression models, or STARs. The mgcv package is at least one place where you can pull this off. But the notion of a random effect is a broad one, and we might think of many such similar effects to add to a model. As mentioned previously, thinking of parameters as random, instead of fixed, essentially puts one in the Bayesian mindset. Moving to that world for your modeling will open up many doors, including expanding your mixed model options. Not to be confused with latent growth curve models or logistic regression.↩︎ "],["summary.html", "Summary", " Summary The odds of you eventually coming across dependency in your data is very high. Using standard methods while ignoring the situation can lead to problematic inference. More to the point however, you’re missing out on a much richer story to tell with the data. One can estimate a variety of cluster-specific effects, incorporate multiple types of clustering, all while still be able to talk about global effects as well. One can also extend such models to other types of ‘random effects’ as well. With the right tools, even complicated mixed models can be fit relatively easily and quickly for even moderately large data sets. It does take some getting used to, but in the end can be a highly satisfying modeling approach. Use them the next time you encounter some dependency in your data. let the rhythm hit ’em "],["supplemental.html", "Supplemental A Comparison to Latent Growth Curve Models Correlation Structure Revisited", " Supplemental A Comparison to Latent Growth Curve Models It is common in Structural Equation Modeling (SEM) to deal with longitudinal data via a Latent Growth Curve (LGC) model. It turns out that LGC are in a sense, just a different form of the very commonly used mixed model framework. In some ways they are more flexible, mostly in the standard structural equation modeling framework that allows for indirect, and other complex covariate relationships. In other ways, they are less flexible, e.g. with missing data, estimating nonlinear relationships, incorporating with many time points, dealing with time-varying covariates. With appropriate tools there is little one can’t do with the normal mixed model approach relative to the SEM approach, and one would likely have easier interpretation. As such I’d recommend sticking with the standard mixed model framework unless you really need to, but it is useful to have both tools. To best understand a growth curve model, I still think it’s instructive to see it from the mixed model perspective, where things are mostly interpretable from what you know from a standard linear model. We will use our GPA example from before, and one can refer to the appendix for more detail. Random effects as latent variables As before we assume the following for the GPA model. As a simple starting point we merely model a trend of time (occasion- 6 semesters) and have random effects due to student for both intercept and occasion. In this setting we are treating time as numeric, but one could treat the occasion variable as categorical28. \\[\\mathcal{GPA} = (b_{\\mathrm{intercept}} + \\mathrm{re}_{\\mathrm{intercept}}) + (b_{\\mathrm{occ}} + \\mathrm{re}_{\\mathrm{occasion}})\\cdot \\mathrm{occasion} + \\epsilon\\] \\[\\mathrm{re}_{\\mathrm{intercept}} \\sim \\mathcal{N}(0, \\tau)\\] \\[\\mathrm{re}_{\\mathrm{occasion}} \\sim \\mathcal{N}(0, \\varphi)\\] \\[\\epsilon \\sim \\mathcal{N}(0, \\sigma)\\] Thus the student effects for the intercept and slope are random, and specifically are normally distributed with mean of zero and some estimated standard deviation (\\(\\tau\\), \\(\\varphi\\) respectively)29. We consider these effects as coming from unspecified, or latent, causes due to student. In addition, we have the usual residual error \\(\\epsilon\\), which can also be thought of as a per-observation random effect due to all other unknown causes. The ‘multilevel model’ version might look like the following, but it is identical. \\[\\mathcal{GPA} = b_{\\mathrm{int\\_student}} + b_{\\mathrm{occ\\_student}}\\cdot \\mathrm{occasion} + \\epsilon\\] \\[b_{\\mathrm{int\\_student}} = b_{\\mathrm{intercept}} + \\mathrm{re}_{\\mathrm{intercept}}\\] \\[b_{\\mathrm{occ\\_student}} = b_{\\mathrm{occ}} + \\mathrm{re}_{\\mathrm{occasion}}\\] The corresponding model may be run using lme4 as follows. load(&#39;data/gpa.RData&#39;) # if you haven&#39;t downloaded the workshop RStudio project # load(url(&#39;https://github.com/m-clark/mixed-models-with-R/raw/master/data/gpa.RData?raw=true&#39;)) library(lme4) mixed_init = lmer(gpa ~ occasion + (1 + occasion|student), data = gpa) # summary(mixed_init) I show a simplified output below, so make sure you can match the results to the summary printout. The fixed (population-average) effects are the \\(b_{\\mathrm{intercept}}\\) and \\(b_{\\mathrm{occ}}\\) in the previous model depiction. The standard deviations of the random effects are the \\(\\tau\\), \\(\\varphi\\) and \\(\\epsilon\\). term value se lower_2.5 upper_97.5 Intercept 2.60 0.02 2.56 2.63 occasion 0.11 0.01 0.10 0.12 We can also get estimates of the student level effects. These are the \\(re_{intercept}\\) and \\(re_{occasion}\\) from before. Table 1: Per-student random effects (sample) group_var effect group value se lower_2.5 upper_97.5 student Intercept 1 -0.202 0.113 -0.424 0.019 student Intercept 2 -0.211 0.113 -0.432 0.011 student Intercept 3 -0.007 0.113 -0.228 0.215 student Intercept 4 -0.093 0.113 -0.315 0.128 student Intercept 5 0.087 0.113 -0.134 0.309 student Intercept 6 -0.206 0.113 -0.427 0.016 Random effects in SEM In SEM, we specify the latent linear, or common factor, model as follows. \\[Y = b_{\\mathrm{intercept}} + \\lambda F + \\epsilon\\] \\[F \\sim \\mathcal{N}(0, \\tau)\\] \\[\\epsilon \\sim \\mathcal{N}(0, \\sigma)\\] In the above, \\(Y\\) is our observed variable, \\(b_{intercept}\\) is the intercept as in a standard linear regression model, \\(\\lambda\\) is the coefficient (loading in factor analysis/SEM terminology) regarding the effect of the latent variable, represented as \\(F\\). The latent variable is assumed normally distributed, with zero mean, and some estimated variance, just like the random effects in mixed models. Note that if \\(\\lambda = 1\\), we then have the right hand side as \\(b_{intercept} + F\\), and this is indistinguishable from the random intercept portion of the mixed model (\\(b_{\\mathrm{intercept}} + \\mathrm{re}_{\\mathrm{intercept}}\\)). Through this that we can maybe start to get a sense of random effects as latent variables (or vice versa). Indeed, mixed models have ties to many other kinds of models (e.g. spatial, additive), because those models also add a ‘random’ component to the model in some fashion. Running a growth curve model The graphical model for the standard LGC model resembles that of confirmatory factor analysis (CFA) with two latent variables/factors. The observed, or manifest, measures are the dependent variable values at each respective time point. However, for those familiar with structural equation modeling (SEM), growth curve models will actually look a bit different compared with typical SEM, because we have to fix the factor loadings to specific values in order to make it work for the LGC. As we will see, this also leads to non-standard output relative to other SEM models, as there is nothing to estimate for the many fixed parameters. More specifically, we’ll have a latent variable representing the random intercepts, as well as one representing the random slopes for the longitudinal trend (time), which in the GPA data is the semester indicator. All loadings for the intercept factor are 1. The loadings for the effect of time are arbitrary, but should accurately reflect the time spacing, and typically it is good to start at zero, so that the zero has a meaningful interpretation. Wide data Given the above visualization, for the LGC our data needs to be in wide format, where each row represents the unit of interest, and we have separate columns for each time point of the target variable, as well as any other variable that varies over time. This is contrasted with the long format we use for the mixed model, where rows represent observations at a given time point. We can use the spread function from tidyr to help with that. We end up with a data frame of two-hundred observations and columns for each semester gpa (0 through 5 for six semesters) denoted by gpa_*. gpa_wide = gpa %&gt;% select(student, sex, highgpa, occasion, gpa) %&gt;% pivot_wider(names_from = occasion, values_from = gpa) %&gt;% rename_at(vars(`0`,`1`,`2`,`3`,`4`,`5`), function(x) glue::glue(&#39;gpa_{x}&#39;)) %&gt;% mutate(female = as.numeric(sex)-1) # convert to binary 0 = male 1 = female to be used later We’ll use lavaan for our excursion into LGC. The syntax will require its own modeling code, but lavaan tries to keep to R regression model style. The names of intercept and occasion are arbitrary, and correspond to the intercepts and slopes factors of the previous visualization. The =~ is just denoting that the left-hand side is the latent variable, and the right-hand side are the observed/manifest variables. We use the standard fixed loadings for an LGC model. lgc_init_model = &#39; intercept =~ 1*gpa_0 + 1*gpa_1 + 1*gpa_2 + 1*gpa_3 + 1*gpa_4 + 1*gpa_5 occasion =~ 0*gpa_0 + 1*gpa_1 + 2*gpa_2 + 3*gpa_3 + 4*gpa_4 + 5*gpa_5 &#39; Now we’re ready to run the model. Note that lavaan has a specific function, growth, to use for these models. It doesn’t spare us any effort for the model syntax, but does make it unnecessary to set various arguments for the more generic sem and lavaan functions. library(lavaan) lgc_init = growth(lgc_init_model, data = gpa_wide) summary(lgc_init) lavaan 0.6-7 ended normally after 73 iterations Estimator ML Optimization method NLMINB Number of free parameters 11 Number of observations 200 Model Test User Model: Test statistic 43.945 Degrees of freedom 16 P-value (Chi-square) 0.000 Parameter Estimates: Standard errors Standard Information Expected Information saturated (h1) model Structured Latent Variables: Estimate Std.Err z-value P(&gt;|z|) intercept =~ gpa_0 1.000 gpa_1 1.000 gpa_2 1.000 gpa_3 1.000 gpa_4 1.000 gpa_5 1.000 occasion =~ gpa_0 0.000 gpa_1 1.000 gpa_2 2.000 gpa_3 3.000 gpa_4 4.000 gpa_5 5.000 Covariances: Estimate Std.Err z-value P(&gt;|z|) intercept ~~ occasion 0.002 0.002 1.629 0.103 Intercepts: Estimate Std.Err z-value P(&gt;|z|) .gpa_0 0.000 .gpa_1 0.000 .gpa_2 0.000 .gpa_3 0.000 .gpa_4 0.000 .gpa_5 0.000 intercept 2.598 0.018 141.956 0.000 occasion 0.106 0.005 20.338 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) .gpa_0 0.080 0.010 8.136 0.000 .gpa_1 0.071 0.008 8.799 0.000 .gpa_2 0.054 0.006 9.039 0.000 .gpa_3 0.029 0.003 8.523 0.000 .gpa_4 0.015 0.002 5.986 0.000 .gpa_5 0.016 0.003 4.617 0.000 intercept 0.035 0.007 4.947 0.000 occasion 0.003 0.001 5.645 0.000 Fixed effects Most of the output is blank, which is needless clutter, but we do get the same five parameter values we are interested in though. We’ll start with the ‘intercepts’: Intercepts: Estimate Std.Err z-value P(&gt;|z|) intercept 2.598 0.018 141.956 0.000 occasion 0.106 0.005 20.338 0.000 It might be odd to call your fixed effects ‘intercepts,’ but it makes sense if we are thinking of it as a multilevel model as depicted previously, where we actually broke out the random effects as a separate model. These are the population average of the random intercepts and slopes for occasion. The estimates here are pretty much spot on with our mixed model estimates. To make the estimation approach as similar as possible, I’ve switched to standard maximum likelihood via REML = FALSE. library(lme4) gpa_mixed = lmer( gpa ~ occasion + (1 + occasion | student), data = gpa, REML = FALSE ) summary(gpa_mixed, cor=F) Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] Formula: gpa ~ occasion + (1 + occasion | student) Data: gpa AIC BIC logLik deviance df.resid 258.2 288.8 -123.1 246.2 1194 Scaled residuals: Min 1Q Median 3Q Max -3.2747 -0.5381 -0.0128 0.5327 3.1945 Random effects: Groups Name Variance Std.Dev. Corr student (Intercept) 0.044859 0.21180 occasion 0.004469 0.06685 -0.10 Residual 0.042387 0.20588 Number of obs: 1200, groups: student, 200 Fixed effects: Estimate Std. Error t value (Intercept) 2.59921 0.01831 141.94 occasion 0.10631 0.00587 18.11 Random effects Now let’s look at the variance estimates, where we see some differences between the LGC and mixed model approach. LGC by default assumes heterogeneous variance for each time point. Mixed models by default assume the same variance for each time point, but can allow them to be estimated separately in most modeling packages. Likewise, we could fix the LGC variances to be identical here. Just know that’s why the results are not identical (to go along with their respective estimation approaches, which are also different by default). Covariances: Estimate Std.Err z-value P(&gt;|z|) intercept ~~ occasion 0.002 0.002 1.629 0.103 Variances: Estimate Std.Err z-value P(&gt;|z|) .gpa_0 0.080 0.010 8.136 0.000 .gpa_1 0.071 0.008 8.799 0.000 .gpa_2 0.054 0.006 9.039 0.000 .gpa_3 0.029 0.003 8.523 0.000 .gpa_4 0.015 0.002 5.986 0.000 .gpa_5 0.016 0.003 4.617 0.000 intercept 0.035 0.007 4.947 0.000 occasion 0.003 0.001 5.645 0.000 print(VarCorr(gpa_mixed), comp = &#39;Var&#39;) # using print to show variance Groups Name Variance Corr student (Intercept) 0.0448593 occasion 0.0044694 -0.096 Residual 0.0423874 Random intercepts How can we put these models on the same footing? Let’s take a step back and do a model with only random intercepts. In this case, time is an observed measure, and has no person-specific variability. Our graphical model now looks like the following. Time, or time point (i.e. semester in our example), is now represented with a square to denote it is no longer affiliated with a latent variable. We can do this by fixing the slope ‘factor’ to have zero variance. However, note also that in the LGC, at each time point of the gpa outcome, we have a unique (residual) variance associated with it. Conversely, this is constant in the mixed model setting, i.e. we only have one estimate for the residual variance that does not vary by occasion. We deal with this in the LGC by giving the parameter an arbitrary name, resid, and then applying it to each time point. lgc_ran_int_model = &#39; intercept =~ 1*gpa_0 + 1*gpa_1 + 1*gpa_2 + 1*gpa_3 + 1*gpa_4 + 1*gpa_5 slope =~ 0*gpa_0 + 1*gpa_1 + 2*gpa_2 + 3*gpa_3 + 4*gpa_4 + 5*gpa_5 slope ~~ 0*slope # slope variance is zero intercept ~~ 0*slope # no covariance with intercept factor gpa_0 ~~ resid*gpa_0 # same residual variance for each time point gpa_1 ~~ resid*gpa_1 gpa_2 ~~ resid*gpa_2 gpa_3 ~~ resid*gpa_3 gpa_4 ~~ resid*gpa_4 gpa_5 ~~ resid*gpa_5 &#39; Now each time point will have one variance estimate. Let’s run the LGC. lgc_ran_int = growth(lgc_ran_int_model, data = gpa_wide) # increase the number of digits shown, remove some output unnecessary to demo summary(lgc_ran_int, nd = 4, header = FALSE) Parameter Estimates: Standard errors Standard Information Expected Information saturated (h1) model Structured Latent Variables: Estimate Std.Err z-value P(&gt;|z|) intercept =~ gpa_0 1.0000 gpa_1 1.0000 gpa_2 1.0000 gpa_3 1.0000 gpa_4 1.0000 gpa_5 1.0000 slope =~ gpa_0 0.0000 gpa_1 1.0000 gpa_2 2.0000 gpa_3 3.0000 gpa_4 4.0000 gpa_5 5.0000 Covariances: Estimate Std.Err z-value P(&gt;|z|) intercept ~~ slope 0.0000 Intercepts: Estimate Std.Err z-value P(&gt;|z|) .gpa_0 0.0000 .gpa_1 0.0000 .gpa_2 0.0000 .gpa_3 0.0000 .gpa_4 0.0000 .gpa_5 0.0000 intercept 2.5992 0.0217 120.0471 0.0000 slope 0.1063 0.0041 26.1094 0.0000 Variances: Estimate Std.Err z-value P(&gt;|z|) slope 0.0000 .gpa_0 (resd) 0.0580 0.0026 22.3607 0.0000 .gpa_1 (resd) 0.0580 0.0026 22.3607 0.0000 .gpa_2 (resd) 0.0580 0.0026 22.3607 0.0000 .gpa_3 (resd) 0.0580 0.0026 22.3607 0.0000 .gpa_4 (resd) 0.0580 0.0026 22.3607 0.0000 .gpa_5 (resd) 0.0580 0.0026 22.3607 0.0000 intrcpt 0.0634 0.0073 8.6605 0.0000 Compare it to the corresponding mixed model. mixed_ran_int = lmer(gpa ~ occasion + (1 | student), data = gpa, REML = FALSE) summary(mixed_ran_int, cor = FALSE) Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] Formula: gpa ~ occasion + (1 | student) Data: gpa AIC BIC logLik deviance df.resid 401.6 422.0 -196.8 393.6 1196 Scaled residuals: Min 1Q Median 3Q Max -3.6188 -0.6370 -0.0002 0.6366 2.8330 Random effects: Groups Name Variance Std.Dev. student (Intercept) 0.06336 0.2517 Residual 0.05803 0.2409 Number of obs: 1200, groups: student, 200 Fixed effects: Estimate Std. Error t value (Intercept) 2.599214 0.021652 120.05 occasion 0.106314 0.004072 26.11 Now we have essentially identical results to mixed_ran_int. The default estimation process is different for the two, resulting in some differences starting several decimal places out, but these are not meaningful differences. We can actually use the same estimator, but the results will still differ slightly due to the data differences. Random intercepts and slopes Now let’s let the slope for occasion vary. We can just delete or comment out the syntax related to the (co-) variance. By default slopes and intercepts are allowed to correlate as in the mixed model. We will continue to keep the variance constant. lgc_ran_int_ran_slope_model = &#39; intercept =~ 1*gpa_0 + 1*gpa_1 + 1*gpa_2 + 1*gpa_3 + 1*gpa_4 + 1*gpa_5 slope =~ 0*gpa_0 + 1*gpa_1 + 2*gpa_2 + 3*gpa_3 + 4*gpa_4 + 5*gpa_5 # slope ~~ 0*slope # slope variance is zero # intercept ~~ 0*slope # no covariance gpa_0 ~~ resid*gpa_0 # same residual variance for each time point gpa_1 ~~ resid*gpa_1 gpa_2 ~~ resid*gpa_2 gpa_3 ~~ resid*gpa_3 gpa_4 ~~ resid*gpa_4 gpa_5 ~~ resid*gpa_5 &#39; lgc_ran_int_ran_slope = growth(lgc_ran_int_ran_slope_model, data = gpa_wide) summary(lgc_ran_int_ran_slope, nd = 4, header = FALSE) Parameter Estimates: Standard errors Standard Information Expected Information saturated (h1) model Structured Latent Variables: Estimate Std.Err z-value P(&gt;|z|) intercept =~ gpa_0 1.0000 gpa_1 1.0000 gpa_2 1.0000 gpa_3 1.0000 gpa_4 1.0000 gpa_5 1.0000 slope =~ gpa_0 0.0000 gpa_1 1.0000 gpa_2 2.0000 gpa_3 3.0000 gpa_4 4.0000 gpa_5 5.0000 Covariances: Estimate Std.Err z-value P(&gt;|z|) intercept ~~ slope -0.0014 0.0016 -0.8337 0.4045 Intercepts: Estimate Std.Err z-value P(&gt;|z|) .gpa_0 0.0000 .gpa_1 0.0000 .gpa_2 0.0000 .gpa_3 0.0000 .gpa_4 0.0000 .gpa_5 0.0000 intercept 2.5992 0.0183 141.9471 0.0000 slope 0.1063 0.0059 18.1113 0.0000 Variances: Estimate Std.Err z-value P(&gt;|z|) .gpa_0 (resd) 0.0424 0.0021 20.0000 0.0000 .gpa_1 (resd) 0.0424 0.0021 20.0000 0.0000 .gpa_2 (resd) 0.0424 0.0021 20.0000 0.0000 .gpa_3 (resd) 0.0424 0.0021 20.0000 0.0000 .gpa_4 (resd) 0.0424 0.0021 20.0000 0.0000 .gpa_5 (resd) 0.0424 0.0021 20.0000 0.0000 intrcpt 0.0449 0.0068 6.5992 0.0000 slope 0.0045 0.0007 6.3874 0.0000 Again, we compare the mixed model to show identical output. mixed_ran_int_ran_slope = lmer(gpa ~ occasion + (1 + occasion|student), data = gpa) summary(mixed_ran_int_ran_slope, cor = FALSE) Linear mixed model fit by REML [&#39;lmerMod&#39;] Formula: gpa ~ occasion + (1 + occasion | student) Data: gpa REML criterion at convergence: 261 Scaled residuals: Min 1Q Median 3Q Max -3.2695 -0.5377 -0.0128 0.5326 3.1939 Random effects: Groups Name Variance Std.Dev. Corr student (Intercept) 0.045193 0.21259 occasion 0.004504 0.06711 -0.10 Residual 0.042388 0.20588 Number of obs: 1200, groups: student, 200 Fixed effects: Estimate Std. Error t value (Intercept) 2.599214 0.018357 141.59 occasion 0.106314 0.005885 18.07 In addition, the estimated random coefficients estimates from the mixed model perfectly correlate with those of the latent variables. student Int_mixed Slope_mixed Int_LGC Slope_LGC 1 2.397 0.166 2.398 0.166 2 2.389 0.105 2.389 0.104 3 2.593 0.149 2.593 0.149 4 2.506 0.064 2.506 0.064 5 2.686 0.080 2.686 0.081 6 2.394 0.063 2.394 0.063 Note that the intercept-slope relationship in the LGC is expressed as a covariance. If we want correlation, we just ask for standardized output. I show only the line output of interest. summary(lgc_ran_int_ran_slope, nd = 4, std = TRUE, header = FALSE) Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all intercept ~~ slope -0.0014 0.0016 -0.8337 0.4045 -0.0963 -0.0963 The std.all is what we typically will look at. Random effects with heterogeneous variances We have demonstrated heterogeneous variances previously. But to revisit here, lme4 does not provide an easy way to have separate variance at each time point, sacrificing various model complexities for computational advantages. However, nlme provides an easy, though not straightforward way to get at these estimates. See the previous section for details. library(nlme) mixed_ran_int_ran_slope_hetero_var = lme( gpa ~ occasion, random = ~ 1 + occasion | student, data = gpa, weights = varIdent(form = ~1|occasion) ) mixedup::summarise_model(mixed_ran_int_ran_slope_hetero_var) Variance Components: Group Effect Variance SD SD_2.5 SD_97.5 Var_prop student Intercept 0.04 0.19 0.15 0.23 0.30 student occasion 0.00 0.06 0.05 0.07 0.03 Residual 0.08 0.28 0.24 0.33 0.67 Fixed Effects: Term Value SE Z P_value Lower_2.5 Upper_97.5 Intercept 2.60 0.02 141.60 0.00 2.56 2.63 occasion 0.11 0.01 20.29 0.00 0.10 0.12 Table 2: Residual variance at each time point Semester Variance 0 0.080 1 0.071 2 0.054 3 0.029 4 0.015 5 0.016 Compare to the LGC (our lgc_init model). Variances: Estimate Std.Err z-value P(&gt;|z|) .gpa_0 0.080 0.010 8.136 0.000 .gpa_1 0.071 0.008 8.799 0.000 .gpa_2 0.054 0.006 9.039 0.000 .gpa_3 0.029 0.003 8.523 0.000 .gpa_4 0.015 0.002 5.986 0.000 .gpa_5 0.016 0.003 4.617 0.000 Other covariates Within these models we can have cluster level covariates which are constant over time, or covariates that vary over time. We will examine each in turn. Cluster level covariates Mixed model To add a cluster-level covariate, for a mixed model, it looks something like this (ignoring lowest level subscript, \\(b_0\\) = intercept): standard random intercept \\[\\mathcal{GPA} = b_{\\mathrm{int\\_student}} + b_{occ}\\cdot\\mathrm{time} + \\epsilon \\] \\[b_{\\mathrm{int\\_student}} = b_{\\mathrm{intercept}} + \\mathrm{re}_{\\mathrm{intercept}}\\] Plugging in becomes: \\[\\mathcal{GPA} = b_{\\mathrm{intercept}} + b_{occ}\\cdot\\mathrm{occasion} + \\mathrm{re}_{\\mathrm{intercept}} + \\epsilon \\] subject level covariate added \\[b_{\\mathrm{int\\_student}} = b_{\\mathrm{intercept}} + b_{sex}\\cdot\\mathrm{sex} + \\mathrm{re}_{\\mathrm{intercept}}\\] But if we plug that into our level 1 model, it just becomes: \\[\\mathcal{GPA} = b_{\\mathrm{intercept}} + b_{sex}\\cdot\\mathrm{sex} + b_{occ}\\cdot\\mathrm{occasion} + \\mathrm{re}_{\\mathrm{intercept}} + \\epsilon \\] In our previous modeling syntax it would look like this: gpa_mixed = lmer(gpa ~ sex + occasion + (1|student), data = gpa) We’d have a fixed effect for sex and interpret it just like in the standard regression setting. LGC With LGC, there is a tendency to interpret the model as an SEM, with the language of effects on latent variables, and certainly one can. For example, we can talk about the (implicitly causal) effect of sex on the intercepts factor, which represents GPA at the first semester. However, adding additional covariates typically causes confusion for those not familiar with mixed models. We literally do have to regress the intercept and slope latent variables on cluster level covariates as follows. \\[\\mathcal{GPA} = b_{\\mathrm{int\\_student}} + b_{\\mathrm{occ\\_student}}\\cdot \\mathrm{occasion} + \\epsilon\\] \\[b_{\\mathrm{int\\_student}} = b_{\\mathrm{intercept}} + b_{sex}\\cdot\\mathrm{sex} + \\mathrm{re}_{\\mathrm{intercept}}\\] Furthermore, people almost automatically put in an effect for the cluster level covariate on the slope factor also. In the mixed model this would result in the following: subject level covariate added added for slopes \\[b_{\\mathrm{occ\\_student}} = b_{\\mathrm{occ}} + \\gamma\\cdot\\mathrm{sex} + \\mathrm{re}_{\\mathrm{occasion}}\\] And after plugging in: \\[\\mathcal{GPA} = \\color{#b2001d}{b_{\\mathrm{intercept}} + b_{sex}\\cdot\\mathrm{sex} + b_{occ}\\cdot\\mathrm{occasion} + \\mathbf{\\gamma\\cdot\\mathrm{sex}\\cdot\\mathrm{occasion}}} + \\color{#001eb2}{\\mathrm{re}_{\\mathrm{intercept}} + \\mathrm{re}_{\\mathrm{occasion}}\\cdot\\mathrm{occasion}} + e\\] The fixed effects are in red, while the random effects are in blue. Focusing on the fixed effects, we can see that this warrants an interaction between sex and occasion. This is not required, but one should add it if they actually are interested in the interaction. Our graphical model looks like the following using the above notation. We are now ready to run the LGC for comparison. lgc_cluster_level_model &lt;- &#39; intercept =~ 1*gpa_0 + 1*gpa_1 + 1*gpa_2 + 1*gpa_3 + 1*gpa_4 + 1*gpa_5 occasion =~ 0*gpa_0 + 1*gpa_1 + 2*gpa_2 + 3*gpa_3 + 4*gpa_4 + 5*gpa_5 # regressions intercept ~ female occasion ~ female gpa_0 ~~ resid*gpa_0 # same residual variance for each time point gpa_1 ~~ resid*gpa_1 gpa_2 ~~ resid*gpa_2 gpa_3 ~~ resid*gpa_3 gpa_4 ~~ resid*gpa_4 gpa_5 ~~ resid*gpa_5 &#39; lgc_cluster_level = growth(lgc_cluster_level_model, data = gpa_wide) summary(lgc_cluster_level, std = TRUE, header = FALSE) Parameter Estimates: Standard errors Standard Information Expected Information saturated (h1) model Structured Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all intercept =~ gpa_0 1.000 0.212 0.717 gpa_1 1.000 0.212 0.710 gpa_2 1.000 0.212 0.671 gpa_3 1.000 0.212 0.613 gpa_4 1.000 0.212 0.551 gpa_5 1.000 0.212 0.492 occasion =~ gpa_0 0.000 0.000 0.000 gpa_1 1.000 0.067 0.224 gpa_2 2.000 0.134 0.424 gpa_3 3.000 0.201 0.581 gpa_4 4.000 0.267 0.695 gpa_5 5.000 0.334 0.776 Regressions: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all intercept ~ female 0.076 0.036 2.083 0.037 0.357 0.178 occasion ~ female 0.029 0.012 2.499 0.012 0.433 0.216 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .intercept ~~ .occasion -0.002 0.002 -1.184 0.237 -0.140 -0.140 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .gpa_0 0.000 0.000 0.000 .gpa_1 0.000 0.000 0.000 .gpa_2 0.000 0.000 0.000 .gpa_3 0.000 0.000 0.000 .gpa_4 0.000 0.000 0.000 .gpa_5 0.000 0.000 0.000 .intercept 2.560 0.026 97.376 0.000 12.085 12.085 .occasion 0.091 0.008 10.865 0.000 1.363 1.363 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .gpa_0 (resd) 0.042 0.002 20.000 0.000 0.042 0.486 .gpa_1 (resd) 0.042 0.002 20.000 0.000 0.042 0.476 .gpa_2 (resd) 0.042 0.002 20.000 0.000 0.042 0.425 .gpa_3 (resd) 0.042 0.002 20.000 0.000 0.042 0.355 .gpa_4 (resd) 0.042 0.002 20.000 0.000 0.042 0.287 .gpa_5 (resd) 0.042 0.002 20.000 0.000 0.042 0.229 .intrcpt 0.043 0.007 6.525 0.000 0.968 0.968 .occasin 0.004 0.001 6.273 0.000 0.953 0.953 Applied researchers commonly have difficulty interpreting the model due to past experience with SEM. While these are latent variables, they aren’t typical latent variables that represent underlying theoretical constructs. It doesn’t help that the output can be confusing, because now one has an ‘intercept for your intercepts’ and an ‘intercept for your slopes.’ In the multilevel context it makes sense, but there you know ‘intercept’ is just ‘fixed effect.’ This is the corresponding mixed model for comparison: mixed_cluster_level_cov = lmer( gpa ~ sex + occasion + sex:occasion + (1 + occasion|student), data = gpa ) summary(mixed_cluster_level_cov, cor = FALSE) Linear mixed model fit by REML [&#39;lmerMod&#39;] Formula: gpa ~ sex + occasion + sex:occasion + (1 + occasion | student) Data: gpa REML criterion at convergence: 256.7 Scaled residuals: Min 1Q Median 3Q Max -3.2556 -0.5409 -0.0142 0.5407 3.2263 Random effects: Groups Name Variance Std.Dev. Corr student (Intercept) 0.044096 0.20999 occasion 0.004328 0.06579 -0.14 Residual 0.042388 0.20588 Number of obs: 1200, groups: student, 200 Fixed effects: Estimate Std. Error t value (Intercept) 2.559549 0.026418 96.888 sexfemale 0.075553 0.036460 2.072 occasion 0.091128 0.008429 10.811 sexfemale:occasion 0.028927 0.011634 2.486 Time-varying covariates Mixed model If we had a time varying covariate, it’d look like the following. The gpa data doesn’t really come with a useful time-varying covariate, so I’ve added one just for demonstration, average weekly hours spent in the library (lib_hours). summary(gpa_mixed_tvc, cor = FALSE) Linear mixed model fit by REML [&#39;lmerMod&#39;] Formula: gpa ~ occasion + lib_hours + (1 + occasion | student) Data: gpa REML criterion at convergence: 48.5 Scaled residuals: Min 1Q Median 3Q Max -3.4105 -0.5185 -0.0023 0.5202 2.9575 Random effects: Groups Name Variance Std.Dev. Corr student (Intercept) 0.033134 0.18203 occasion 0.002817 0.05307 -0.13 Residual 0.037591 0.19388 Number of obs: 1200, groups: student, 200 Fixed effects: Estimate Std. Error t value (Intercept) 2.385494 0.021079 113.17 occasion 0.082838 0.005196 15.94 lib_hours 0.032216 0.002024 15.92 Note that we could have a random slope for library hours if we wanted. The fixed effect for the covariate is still as it would be for standard regression interpretation. LGC With time varying covariates, the syntax starts to get tedious for the LGC. Here we add lib_hours to the model, but we need to convert it to wide format and add it to our previous data. Thus lib_hours_* represent the average weekly hours spent in the library for each each student at each semester. lgc_tvc &lt;- growth(lgc_tvc_model, data = gpa_wide) summary(lgc_tvc, header = FALSE) Parameter Estimates: Standard errors Standard Information Expected Information saturated (h1) model Structured Latent Variables: Estimate Std.Err z-value P(&gt;|z|) intercept =~ gpa_0 1.000 gpa_1 1.000 gpa_2 1.000 gpa_3 1.000 gpa_4 1.000 gpa_5 1.000 occasion =~ gpa_0 0.000 gpa_1 1.000 gpa_2 2.000 gpa_3 3.000 gpa_4 4.000 gpa_5 5.000 Regressions: Estimate Std.Err z-value P(&gt;|z|) gpa_0 ~ lib_hours_0 0.045 0.004 10.701 0.000 gpa_1 ~ lib_hours_1 0.039 0.003 13.514 0.000 gpa_2 ~ lib_hours_2 0.033 0.002 13.752 0.000 gpa_3 ~ lib_hours_3 0.028 0.003 11.271 0.000 gpa_4 ~ lib_hours_4 0.024 0.003 8.527 0.000 gpa_5 ~ lib_hours_5 0.022 0.003 6.348 0.000 Covariances: Estimate Std.Err z-value P(&gt;|z|) intercept ~~ occasion -0.001 0.001 -0.656 0.512 Intercepts: Estimate Std.Err z-value P(&gt;|z|) .gpa_0 0.000 .gpa_1 0.000 .gpa_2 0.000 .gpa_3 0.000 .gpa_4 0.000 .gpa_5 0.000 intercept 2.300 0.030 76.682 0.000 occasion 0.122 0.011 11.123 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) .gpa_0 (resd) 0.036 0.002 20.000 0.000 .gpa_1 (resd) 0.036 0.002 20.000 0.000 .gpa_2 (resd) 0.036 0.002 20.000 0.000 .gpa_3 (resd) 0.036 0.002 20.000 0.000 .gpa_4 (resd) 0.036 0.002 20.000 0.000 .gpa_5 (resd) 0.036 0.002 20.000 0.000 intrcpt 0.030 0.005 6.019 0.000 occasin 0.003 0.001 5.975 0.000 However, this result is not the same as our mixed model. Here is the corresponding graphical model. The * represents a coefficient that is freely estimated. The problem here is similar to that seen with the residual variances. Unless we fix the coefficient to be constant, this is akin to having an interaction of the time-varying covariate with a categorical form of time. So in the same model, we flip from considering time as a numeric and linear effect on the outcome, to one that is categorical. This is rarely done in typical mixed or other regression models, though for some reason is the standard for the LGC setting. The following will get us back to the comparable mixed model. lgc_tvc_model &lt;- &#39; intercept =~ 1*gpa_0 + 1*gpa_1 + 1*gpa_2 + 1*gpa_3 + 1*gpa_4 + 1*gpa_5 occasion =~ 0*gpa_0 + 1*gpa_1 + 2*gpa_2 + 3*gpa_3 + 4*gpa_4 + 5*gpa_5 # time-varying covariates gpa_0 ~ lh_coef*lib_hours_0 gpa_1 ~ lh_coef*lib_hours_1 gpa_2 ~ lh_coef*lib_hours_2 gpa_3 ~ lh_coef*lib_hours_3 gpa_4 ~ lh_coef*lib_hours_4 gpa_5 ~ lh_coef*lib_hours_5 gpa_0 ~~ resid*gpa_0 # same residual variance for each time point gpa_1 ~~ resid*gpa_1 gpa_2 ~~ resid*gpa_2 gpa_3 ~~ resid*gpa_3 gpa_4 ~~ resid*gpa_4 gpa_5 ~~ resid*gpa_5 &#39; lgc_tvc &lt;- growth(lgc_tvc_model, data=gpa_wide) summary(lgc_tvc, header = FALSE) Parameter Estimates: Standard errors Standard Information Expected Information saturated (h1) model Structured Latent Variables: Estimate Std.Err z-value P(&gt;|z|) intercept =~ gpa_0 1.000 gpa_1 1.000 gpa_2 1.000 gpa_3 1.000 gpa_4 1.000 gpa_5 1.000 occasion =~ gpa_0 0.000 gpa_1 1.000 gpa_2 2.000 gpa_3 3.000 gpa_4 4.000 gpa_5 5.000 Regressions: Estimate Std.Err z-value P(&gt;|z|) gpa_0 ~ lb_hr_0 (lh_c) 0.032 0.002 15.951 0.000 gpa_1 ~ lb_hr_1 (lh_c) 0.032 0.002 15.951 0.000 gpa_2 ~ lb_hr_2 (lh_c) 0.032 0.002 15.951 0.000 gpa_3 ~ lb_hr_3 (lh_c) 0.032 0.002 15.951 0.000 gpa_4 ~ lb_hr_4 (lh_c) 0.032 0.002 15.951 0.000 gpa_5 ~ lb_hr_5 (lh_c) 0.032 0.002 15.951 0.000 Covariances: Estimate Std.Err z-value P(&gt;|z|) intercept ~~ occasion -0.001 0.001 -0.973 0.331 Intercepts: Estimate Std.Err z-value P(&gt;|z|) .gpa_0 0.000 .gpa_1 0.000 .gpa_2 0.000 .gpa_3 0.000 .gpa_4 0.000 .gpa_5 0.000 intercept 2.385 0.021 113.389 0.000 occasion 0.083 0.005 15.983 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) .gpa_0 (resd) 0.038 0.002 20.000 0.000 .gpa_1 (resd) 0.038 0.002 20.000 0.000 .gpa_2 (resd) 0.038 0.002 20.000 0.000 .gpa_3 (resd) 0.038 0.002 20.000 0.000 .gpa_4 (resd) 0.038 0.002 20.000 0.000 .gpa_5 (resd) 0.038 0.002 20.000 0.000 intrcpt 0.033 0.005 6.148 0.000 occasin 0.003 0.001 5.523 0.000 Compare again to the mixed model result. summary(gpa_mixed_tvc, cor = FALSE) Linear mixed model fit by REML [&#39;lmerMod&#39;] Formula: gpa ~ occasion + lib_hours + (1 + occasion | student) Data: gpa REML criterion at convergence: 48.5 Scaled residuals: Min 1Q Median 3Q Max -3.4105 -0.5185 -0.0023 0.5202 2.9575 Random effects: Groups Name Variance Std.Dev. Corr student (Intercept) 0.033134 0.18203 occasion 0.002817 0.05307 -0.13 Residual 0.037591 0.19388 Number of obs: 1200, groups: student, 200 Fixed effects: Estimate Std. Error t value (Intercept) 2.385494 0.021079 113.17 occasion 0.082838 0.005196 15.94 lib_hours 0.032216 0.002024 15.92 Now imagine having just a few of those kinds of variables as would be common in most longitudinal settings. In the mixed model framework one would add them in as any covariate in a regression model, and each covariate would be associated with a single fixed effect. In the LGC framework, one has to regress each time point for the target variable on its corresponding predictor time point. It might take a few paragraphs to explain the coefficients for just a handful of covariates. If you fix them to a single value, you would duplicate the mixed model, but the syntax requires even more tedium30. Some differences between mixed models and growth curves Wide vs. long The SEM framework is inherently multivariate, i.e. assuming multiple outcomes, so your data will need to be in wide format. In the R world, this is ‘untidy’ data, and makes other data processing and visualization more tedious. Random slopes One difference seen in comparing LGC models vs. mixed models is that in the former, random slopes are always assumed, whereas in the latter, one would typically see if it’s worth adding random slopes in the first place, or simply not assume them. Other random effects Just about any LGC you come across in the wild has only one clustering level of interest. However, it’s very common to have multiple and non-hierarchical random effects structure or additional random effects beyond the time covariate. In our example, these might include school, district, or other complicated structure, or the library hours from the time-varying covariate example. Tools like lme4 handle random effects and complicated structure easily. SEM tools do not do this easily, and resort to the multilevel (long-format) approach, which more or less defeats the purpose of using them, as they merely mimic the standard mixed model approach, albeit with yet another and different type of syntax31. However, if you have other latent variables or complicated indirect effects, this may be the way to go. Sample size SEM is inherently a large sample technique. The growth curve model does not require as much for standard approaches, but may require a lot more depending on the model one tries to estimate. In my own simulations, I haven’t seen too much difference compared to mixed models even for notably small sample sizes, but those were for very simple models. Number of time points A basic growth curve model requires four time points to incorporate the flexibility that would make it worthwhile. Mixed models don’t have the restriction (outside of the obvious need of two). In addition, mixed models can handle any number of time points without changing the syntax at all, while LGC is rarely applied to more than a handful of time points. Even then, when you have many time-varying covariates, which is common, the model syntax is tedious, and you end up having the number of parameters to estimate climb rapidly, as the default model assumes interactions with time as a categorical variable. Balance Mixed models can run even if some clusters have a single value. SEM requires balanced data and so one will always have to estimate missing values or drop them. Whether this missingness can be ignored in the standard mixed model framework is a matter of some debate. Most disciplines typically ignore the missingness, which for mixed models means assuming the observations are missing at random (MAR). With the LGC, the default is simply to drop any observation with missing, and so the assumption there is missing completely at random (MCAR), a stronger assumption. Numbering the time points Numbering your time from zero makes sense in both worlds. This leads to the natural interpretation that the intercept is the mean of the outcome for your first time point. In other cases having a centered value would make sense, or numbering from 0 to a final value of 1, which would mean the slope coefficient represents the change over the whole time span. Recommended packages that can do growth curve models Between lme4 and nlme or glmmTMB, you can do any standard LGC. Besides that, various packages provide functionality that some might think is only done with SEM software. One package I highly recommend is brms, as it builds on many other packages that incorporate a mixed model approach in the Bayesian framework. The others are ones that come to mind off the top of my head, so in some cases should be seen as a starting point only. Standard LGC, including alternative distributions (e.g. robust t, beta, count, zero-inflated): nlme, lavaan, lme4, mgcv, brms, many others Multilevel SEM: lavaan Flexible nonlinear relationships: nlme, mgcv, brms Missing data: lavaan, brms, mice applied to other packages Multivariate/Parallel Process/Correlated random effects: lavaan, brms Mediation: lavaan, mediation, brms Growth Mixture Models: brms, flexmix In short, you’d need a very complicated growth model to require moving from the mixed model framework to SEM-specific software or beyond R, in other words, one that combines potentially already complex modeling situations. Note also, unless you are incorporating latent variables, e.g. from scale measurements, there is little need to use something like lavaan or Mplus for standard mixed/multilevel modeling (i.e. in the long data framework), though they have such functionality. The Mplus manual also lumps survival and standard time series in the chapter on longitudinal and related models. However, I personally can’t see a scenario where I would use Mplus for survival or time series given the multitude of easier to use, more flexible, and more powerful options in R. Summary of LGC Latent Growth Curve modeling is an alternative way to do what is very commonly accomplished through mixed models, and allow for more complex models than typically seen for standard mixed models. One’s default should probably be to use the far more commonly used, and probably more flexible (in most situations), mixed modeling tools, where there are packages in R that could handle nonlinear effects, mediation and multivariate outcomes for mixed models. I have other documents regarding mixed models on my website and code at GitHub, including a document that does more comparison to growth curve models. However, the latent variable approach may provide what you need in some circumstances, and at the very least gives you a fresh take on the standard mixed model perspective. Correlation Structure Revisited Let’s revisit the notion of autocorrelation/autoregressive (AR) residual structure. We’ll start by recalling the AR structure we noted before, with \\(\\rho\\) our estimate of the covariance/correlation. For the following depiction, we have three observations per cluster, and observations are correlated in time. However the residual correlation decreases the further in time the observations are apart from one another. \\[\\Sigma = \\sigma^2 \\left[ \\begin{array}{cccc} 1 &amp; \\rho &amp; \\rho^2 &amp; \\rho^3 \\\\ \\rho &amp; 1 &amp; \\rho &amp; \\rho^2 \\\\ \\rho^2 &amp; \\rho &amp; 1 &amp; \\rho \\\\ \\rho^3 &amp; \\rho^2 &amp; \\rho &amp; 1 \\\\ \\end{array}\\right]\\] How does this get into our model? We can find out via simulation. The next bit of code follows lme4 developer Ben Bolker’s example. Here we create a variable which is a multivariate draw with the specified correlational structure. In addition, we’ll have a single covariate, call it x. The linear predictor is based on an intercept value of 5 and a coefficient for x of .5. simGroup &lt;- function(g, n = 5, rho = 0.7, sigma = .5) { # create time points and group id times &lt;- factor(1:n) group &lt;- factor(rep(g, n)) # create the ar structure cor_struct &lt;- rho^as.matrix(dist(1:n)) # if you want to play around with the estimated variance of glmmTMB you can, # but this will change what the expected correlation should be; use cov2cor on # cor_struct after adding the diagonal to see that value # diag(cor_struct) = 2.5 # Simulate the process x_ar &lt;- MASS::mvrnorm(mu = rep(0, n), Sigma = cor_struct) # add another covariate x &lt;- rnorm(n) # linear predictor mu &lt;- 5 + .5*x # Add measurement noise and create target variable y &lt;- mu + x_ar + rnorm(n, sd = sigma) data.frame(y, times, x, group) } simGroup(1) y times x group 1 4.239340 1 -0.5119390 1 2 4.927270 2 -0.6460704 1 3 4.328641 3 -0.9695196 1 4 4.126718 4 -0.8317013 1 5 5.389997 5 1.0799571 1 Now let’s do this for 500 groups or clusters, each with 10 observations, sigma equal to 1.5 and \\(\\rho\\) set at 0.8. set.seed(1234) test_df = map_df(1:500, simGroup, n = 10, rho = .8, sigma = 1.5) head(test_df) y times x group 1...1 6.924564 1 -0.47719270 1 2...2 5.349769 2 -0.99838644 1 3...3 4.673456 3 -0.77625389 1 4...4 5.626127 4 0.06445882 1 5...5 4.831887 5 0.95949406 1 6...6 3.813962 6 -0.11028549 1 Now we run and summarize the model with glmmTMB. library(glmmTMB) model_tmb = glmmTMB(y ~ x + ar1(times + 0 | group), data = test_df) summary(model_tmb) Family: gaussian ( identity ) Formula: y ~ x + ar1(times + 0 | group) Data: test_df AIC BIC logLik deviance df.resid 19586.5 19619.1 -9788.3 19576.5 4995 Random effects: Conditional model: Groups Name Variance Std.Dev. Corr group times1 1.007 1.003 0.79 (ar1) Residual 2.180 1.477 Number of obs: 5000, groups: group, 500 Dispersion estimate for gaussian family (sigma^2): 2.18 Conditional model: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 5.04663 0.03842 131.35 &lt;2e-16 *** x 0.54392 0.02393 22.73 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We recover our estimates. The ar1 value is very close to the true value of 0.8 we specified, while the residual standard deviation is close to 1.5, and our fixed effects are also as expected. We have an additional value approaching 1 for the times1 variance, which is the diagonal of the AR correlation matrix, which in our code is separate from what we draw for the observation level residual variance. Likewise, brms has similar syntax in that we add the AR component to the formula. library(brms) model_brm = brm( y ~ x + ar(times, group), data = test_df, cores = 4, seed = 1234 ) summary(model_brm) Family: gaussian Links: mu = identity; sigma = identity Formula: y ~ x + ar(times, group) Data: test_df (Number of observations: 5000) Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; total post-warmup samples = 4000 Correlation Structures: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ar[1] 0.25 0.01 0.22 0.28 1.00 4595 3082 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Intercept 5.05 0.03 4.98 5.11 1.00 5213 3582 x 0.55 0.02 0.50 0.60 1.00 4457 2895 Family Specific Parameters: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sigma 1.74 0.02 1.70 1.77 1.00 4237 3105 Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). While the focus here is on the other packages, we could also use the gls function in nlme, which gives the same result as brms. gls( y ~ x, data = test_df, correlation = corAR1(form = ~as.numeric(times)|group) ) Generalized least squares fit by REML Model: y ~ x Data: test_df Log-restricted-likelihood: -9853.791 Coefficients: (Intercept) x 5.0443811 0.5514778 Correlation Structure: AR(1) Formula: ~as.numeric(times) | group Parameter estimate(s): Phi 0.2453209 Degrees of freedom: 5000 total; 4998 residual Residual standard error: 1.784043 It may seem at first blush that glmmTMB and brms have come to different conclusions about the correlation and variance estimates. However, close inspection reveals they are in fact providing the same information from different viewpoints. The simulation code shows how we start with our linear predictor, which includes the fixed effects, then adds the random effect with autoregressive structure, and finally adds our residual variance. But the way brms (and nlme) is estimating it is how we’ve shown it in the matrix formulation above, as a single residual/random effect. \\[\\Sigma_{brms} = \\sigma^2_{brms} \\left[ \\begin{array}{cccc} 1 &amp; \\rho_{brms} &amp; \\rho_{brms}^2 &amp; \\rho_{brms}^3 \\\\ \\rho_{brms} &amp; 1 &amp; \\rho_{brms} &amp; \\rho_{brms}^2 \\\\ \\rho_{brms}^2 &amp; \\rho_{brms} &amp; 1 &amp; \\rho_{brms} \\\\ \\rho_{brms}^3 &amp; \\rho_{brms}^2 &amp; \\rho_{brms} &amp; 1 \\\\ \\end{array}\\right]\\] \\[\\Sigma_{tmb} = \\left[ \\begin{array}{cccc} \\sigma^2_{ar} &amp; \\rho_{tmb} &amp; \\rho_{tmb}^2 &amp; \\rho_{tmb}^3 \\\\ \\rho_{tmb} &amp; \\sigma^2_{ar} &amp; \\rho_{tmb} &amp; \\rho_{tmb}^2 \\\\ \\rho_{tmb}^2 &amp; \\rho_{tmb} &amp; \\sigma^2_{ar} &amp; \\rho_{tmb} \\\\ \\rho_{tmb}^3 &amp; \\rho_{tmb}^2 &amp; \\rho_{tmb} &amp; \\sigma^2_{ar} \\\\ \\end{array}\\right] + \\left[ \\begin{array}{cccc} \\sigma^2_{tmb} &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma^2_{tmb} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma^2_{tmb} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma^2_{tmb} \\\\ \\end{array}\\right]\\] So if we take the glmmTMB ar1 estimate and variance estimates we can recover the brms estimate. ar_tmb * ar_var_tmb/(ar_var_tmb + res_var_tmb) [1] 0.2494005 ar_brm [1] 0.2469 # if we assume ar var = 1 ar_brm*res_var_brms [1] 0.7432924 ar_tmb [1] 0.7895 # if we knew the ar variance, we could use brms to get to tmb&#39;s estimate ar_brm*(ar_var_tmb + res_var_tmb)/ar_var_tmb [1] 0.7815843 The glmmTMB approach is interesting in how it explicitly separates out the ar component from the rest of the residual component. This seems non-standard, as I don’t recall papers reporting the AR standard deviation for example, and every depiction I come across in the mixed model literature is the one that underlies brms. However, it seems like it might be useful and/or interesting from some settings, or maybe even preferable as an additional interpretation for a random effect, similar to the ones we commonly use. We can change our function to force glmmTMB to come to the same conclusion as brms by not distinguishing the variance components. In this case, glmmTMB will move all residual variance to the AR estimate, and the estimated correlation is the same as what brms reports. simGroup_alt &lt;- function(g, n = 5, rho = 0.7, sigma = .5) { # create time points and group id times &lt;- factor(1:n) group &lt;- factor(rep(g, n)) # create the ar structure cor_struct &lt;- rho^as.matrix(dist(1:n)) # combine the residual variance resid_struct &lt;- cor_struct*sigma^2 # Simulate the process; note the difference x_ar &lt;- MASS::mvrnorm(mu = rep(0, n), Sigma = resid_struct) # add another covariate x &lt;- rnorm(n) # linear predictor mu &lt;- 5 + .5*x # Create target variable; residual already incorporated into x_ar y &lt;- mu + x_ar data.frame(y, times, x, group) } set.seed(1234) test_df_alt = map_df(1:500, simGroup_alt, n = 10, rho = ar_brm, sigma = 1.5) model_tmb_alt = glmmTMB(y ~ x + ar1(times + 0 | group), data = test_df_alt) summary(model_tmb_alt) Family: gaussian ( identity ) Formula: y ~ x + ar1(times + 0 | group) Data: test_df_alt AIC BIC logLik deviance df.resid 17776.4 17809.0 -8883.2 17766.4 4995 Random effects: Conditional model: Groups Name Variance Std.Dev. Corr group times1 2.189e+00 1.479380 0.27 (ar1) Residual 2.277e-06 0.001509 Number of obs: 5000, groups: group, 500 Dispersion estimate for gaussian family (sigma^2): 2.28e-06 Conditional model: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 4.97996 0.02662 187.10 &lt;2e-16 *** x 0.47472 0.01975 24.04 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Summary of residual correlation structure What both of these syntactical approaches make clear is that, in specifying a specific correlational structure, we can think of it as adding a latent variable, i.e. a random effect, to our standard model, just as we have done with other random effects. This particular random effect has correlated observations within a group as specified by the structure. The same thing can apply in the case of heterogenous variances, just that no specific correlation is assumed in that case. As such, it may make more sense to think of it as an additional component to the model structure/formula, as opposed to an function argument separate from your theoretical focus. I’m omitting the observation level subscript, so this can work for the single observation or entire data set.↩︎ Usually we would draw both random effects from a multivariate normal distribution with some covariance.↩︎ To be fair, Mplus (and presumably lavaan at some point in the future) has shortcuts to make the syntax easier, but it also can make for more esoteric and less understandable syntax.↩︎ Honestly, for the same types of models I find the multilevel syntax of Mplus ridiculously complex relative to R packages.↩︎ "],["appendix.html", "Appendix Data Programming languages Reference texts and other stuff", " Appendix Data Note that I have converted these from their original SPSS format to R data.frames saved within RData files. I also cleaned them up with better names/labels etc. For data sets used in that text, most of the description is taken from Joop Hox’s text appendix (‘Data Stories’). GPA: The GPA data are a longitudinal data set, where 200 college students have been followed 6 consecutive semesters. In this data set, there are GPA measures on 6 consecutive occasions, with a JOB status variable (how many hours worked) for the same 6 occasions. There are two student-level explanatory variables: the sex (1= male, 2= female) and the high school GPA. There is also a dichotomous student-level outcome variable, which indicates whether a student has been admitted to the university of their choice. Since not every student applies to a university, this variable has many missing values. pupils: Assume that we have data from 1000 pupils who have attended 100 different primary schools, and subsequently went on to 30 secondary schools. Similar to the situation where we have pupils within schools and neighborhoods, we have a cross-classified structure. Pupils are nested within primary and within secondary schools, with primary and secondary schools crossed. In other words: pupils are nested within the cross-classification of primary and secondary schools. In our example, we have a response variable achievement which is measured in secondary school. We have two explanatory variables at the pupil level: pupil sex (0 = male, 1 = female) and a six-point scale for pupil socioeconomic status, pupil SES. We have at the school level a dichotomous variable that indicates if the school is public (denom = 0) or denominational (denom = 1). Since we have both primary and secondary schools, we have two such variables (named pdenom for the primary school and sdenom for the secondary school). nurses: The data in this example are from a hypothetical study on stress in hospitals. The data are from nurses working in wards nested within hospitals. In each of 25 hospitals, four wards are selected and randomly assigned to an experimental and control condition. In the experimental condition, a training program is offered to all nurses to cope with job- related stress. After the program is completed, a sample of about 10 nurses from each ward is given a test that measures job-related stress. Additional variables are: nurse age (years), nurse experience (years), nurse sex (0 = male, 1 = female), type of ward (0 = general care, 1 = special care), and hospital size (0 = small, 1 = medium, 2 = large). This is an example of an experiment where the experimental intervention is carried out at the group level. In biomedical research this design is known as a cluster randomized trial. They are quite common also in educational and organizational research, where entire classes or schools are assigned to experimental and control conditions. Since the design variable Experimental versus Control group (ExpCon) is manipulated at the second (ward) level, we can study whether the experimental effect is different in different hospitals, by defining the regression coefficient for the ExpCon variable as random at the hospital level. In this example, the variable ExpCon is of main interest, and the other variables are covariates. Their function is to control for differences between the groups, which should be small given that randomization is used, and to explain variance in the outcome variable stress. To the extent that they are successful in explaining variance, the power of the test for the effect of ExpCon will be increased. Therefore, although logically we can test if explanatory variables at the first level have random coefficients at the second or third level, and if explanatory variables at the second level have random coefficients at the third level, these possibilities are not pursued. We do test a model with a random coefficient for ExpCon at the third level, where there turns out to be significant slope variation. This varying slope can be predicted by adding a cross-level interaction between the variables ExpCon and HospSize. In view of this interaction, the variables ExpCon and HospSize have been centered on their overall mean. sociometric: The sociometric data are intended to demonstrate a data structure where the cross-classification is at the lowest level, with an added group structure because there are several groups. The story is that in small groups all members are asked to rate each other on a scale of 1-9, where higher numbers indicate a more positive view of the individual (i.e. how much they would like to share some activity with the rated person). Each record is defined by the sender–receiver pairs, with explanatory variables age and sex defined separately for the sender and the receiver. The group variable ‘group size’ is added to this file. There are 20 groups, with sizes ranging from 4 to 11. speed dating: Data involve a speed dating experiment on a sample of a few hundred students in graduate and professional schools at Columbia University. In the speed dating events, the experiment randomly assigned each participant to ten short dates (four minutes) with participants of the opposite sex. For each date, each person rated six attributes (attractive, sincere, intelligent, fun, ambitious, shared interests) of the other person on a 10-point scale and wrote down whether he or she would like to see the other person again. The data have been filtered to remove constant responders, i.e. those who always or never wanted to see their partner again, those six attributes have scaled versions with mean 0 and standard deviation 1 (_sc), column names have been made useful, and only some of the variables have been kept for the demo. If you run the same model, you can get very similar estimates to that in the Fahrmeier et al. text, table 7.4 (though there is a typo for the Male effect). The data come from Gelman’s website but are based on this article. patents: The European Patent Office is able to protect a patent from competition for a certain period of time. The Patent Office has the task to examine inventions and to declare patent if certain prerequisites are fulfilled. The most important requirement is that the invention is something truly new. Even though the office examines each patent carefully, in about 80% of cases competitors raise an objection against already assigned patents. In the economic literature the analysis of patent opposition plays an important role as it allows one to (indirectly) investigate a number of economic questions. For instance, the frequency of patent opposition can be used as an indicator for the intensity of the competition in different market segments. The primary variables are: opp: Patent opposition (1=yes 0=no), biopharm: Patent from biotech/pharma sector, ustwin: US twin patent exists, patus: Patent holder from the USA, patgsgr: Patent holder from Germany, Switzerland, or Great Britain, year, ncit: Number of citations for the patent, ncountry: Number of designated states for the patent, and nclaims: Number of claims. Centered and other transformed variables are also present. In order to analyze objections against patents, a data set with 4,866 patents from the sectors biotechnology/pharmaceutics and semiconductor/computer was collected. The goal of one analysis is to model the probability of patent opposition, while using a variety of explanatory variables for the binary response variable patent opposition (yes/no). This corresponds to a regression problem with a binary response. In other cases you might wish to analyze the number of citations. Programming languages R R has more mixed modeling capabilities than anything else out there. Here are just a few options within R. lme4: generalized linear mixed models; extremely efficient nlme: (non-)linear mixed models but only for the gaussian case. mgcv: provides means to use both lme4 or nlme, extends distributional families, correlated residuals, and all that additive model stuff too glmmTMB: newer package that similarly extends beyond lme4 ordinal: for various types of ordinal models rstanarm: Bayesian with lme4 level options brms: Bayesian with possibly the most extensive mixed model capabilities out there Python In Python, the statsmodels module has basic capabilities for mixed models, but not too many frills relatively speaking. CSCAR director Kerby Shedden has been part of this specific development. You can see his notes from the Data Science Skills Series here. The following provides an example. pycheck = ifelse(grepl(&#39;Windows&#39;, sessionInfo()$running), &#39;C:/ProgramData/Anaconda3&#39;, &#39;/Users/micl/opt/anaconda3/bin/python&#39;) import pandas as pd import statsmodels.api as sm import statsmodels.formula.api as smf gpa = pd.read_csv(&#39;data/gpa.csv&#39;) gpa_py_mixed = smf.mixedlm(&quot;gpa ~ occasion&quot;, gpa, groups=gpa[&quot;student&quot;]).fit() print(gpa_py_mixed.summary()) Mixed Linear Model Regression Results ======================================================= Model: MixedLM Dependent Variable: gpa No. Observations: 1200 Method: REML No. Groups: 200 Scale: 0.0581 Min. group size: 6 Log-Likelihood: -204.4464 Max. group size: 6 Converged: Yes Mean group size: 6.0 ------------------------------------------------------- Coef. Std.Err. z P&gt;|z| [0.025 0.975] ------------------------------------------------------- Intercept 2.599 0.022 119.801 0.000 2.557 2.642 occasion 0.106 0.004 26.096 0.000 0.098 0.114 Group Var 0.064 0.033 ======================================================= Julia One of the lme4 authors develops a MixedModels package for Julia. Doug Bates has some notebooks on implementation of mixed models in Julia, though they are a bit dated at this point, so consult the documentation on the most recent version of the package. Proprietary Among the standard stats packages, I can only recommend Stata for both ease of implementation and flexibility in modeling. SAS and SPSS both have non-intuitive and needlessly verbose syntax, and, while SAS is in fact a fairly good tool for mixed models (from my understanding), I’ve actually seen SPSS consistently struggle with even simple models, and its GUI interface, the only reason to use SPSS in the first place, is not even remotely intuitive. Mplus has a lot of functionality both for standard mixed models and growth curve models, though between brms, lavaan, mediation, and the rest of R, there is little need to use Mplus except for very complicated multilevel SEM, which requires very large samples and a lot of theoretical justification, which most of the people doing SEM are typically lacking in one or the other. I will also say that there is no reason to use mixed model specific software like HLM. The days for such hijinks have long since passed. Reference texts and other stuff An excellent modeling book can be found in Data Analysis Using Regression and Multilevel/Hierarchical Models. You will learn a great deal about statistical modeling in general, as the mixed model stuff only comprises the second part. Gelman has since updated this, or at least the first part, but I can’t speak to the content yet. On the applied side, former CSCAR consultant Brady West has a book- Linear Mixed Models: A Practical Guide Using Statistical Software. It shows examples in a variety of programming formats while not glossing over the details you need to know. Two texts come at mixed models from a very broad perspective, which I like. I find Fahrmeier et al. Regression - Models, Methods and Applications to be very good. It will also tie mixed models to spatial and nonparametric approaches. Richly Parameterized Linear Models - Additive, Time Series, and Spatial Models Using Random Effects, does as well, and provides some nice historical context and hits on practical issues you will encounter if you play with these models long enough. There is literally a book on mixed models with lme4 by Doug Bates, one of the developers. While dated with regard to lme4, it’s definitely not with regard to mixed models in general, and worth having around. Check out the Mixed Model FAQ maintained by Ben Bolker, lme4 developer I have several other docs on various aspects of mixed models, and post about different aspects about them regularly. There is a lot of good information on stackoverflow and cross validated Q &amp; A sites, where lme4 developer Ben Bolker has been quite active in answering people’s questions. "]]
